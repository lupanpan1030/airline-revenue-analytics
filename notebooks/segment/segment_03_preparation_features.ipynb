{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "style"
    ]
   },
   "outputs": [],
   "source": [
    "from airline_revenue_analytics.viz.charts import apply_style, PLOT_COLORS\n",
    "apply_style()\n",
    "PASS_COLOR = \"#D9F2E6\"\n",
    "FAIL_COLOR = \"#FCE4E4\"\n",
    "NEG_BG_COLOR = FAIL_COLOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2011cba5e4b2aabe6b48323dc18f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:43.706382Z",
     "iopub.status.busy": "2026-01-11T10:13:43.706285Z",
     "iopub.status.idle": "2026-01-11T10:13:43.715757Z",
     "shell.execute_reply": "2026-01-11T10:13:43.715302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook setup (segment pipeline) / Notebook \u521d\u59cb\u5316\uff08segment \u7ba1\u7ebf\uff09\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from airline_revenue_analytics.config import get_paths\n",
    "except ModuleNotFoundError as exc:\n",
    "    raise ModuleNotFoundError(\"Install the package first: pip install -e .\") from exc\n",
    "\n",
    "# Resolve repo paths and DB location / \u89e3\u6790\u4ed3\u5e93\u8def\u5f84\u4e0e\u6570\u636e\u5e93\u4f4d\u7f6e\n",
    "paths = get_paths(\"segment\")\n",
    "REPO_ROOT = paths.repo_root\n",
    "DATA_DIR = paths.data_raw\n",
    "OUT_DIR = paths.outputs_root\n",
    "FIG_DIR = paths.figures\n",
    "TAB_DIR = paths.tables\n",
    "DB_PATH = paths.db_path\n",
    "db_path = DB_PATH\n",
    "\n",
    "# SQLite connection (shared across cells) / SQLite \u8fde\u63a5\uff08\u5168\u5c40\u590d\u7528\uff09\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "def _rel(p: Path) -> str:\n",
    "    \"\"\"Render repo-relative paths for display / \u5c06\u8def\u5f84\u663e\u793a\u4e3a\u4ed3\u5e93\u76f8\u5bf9\u8def\u5f84.\"\"\"\n",
    "    try:\n",
    "        return str(Path(p).resolve().relative_to(REPO_ROOT))\n",
    "    except Exception:\n",
    "        return str(p)\n",
    "\n",
    "def find_path(filename: str) -> Path:\n",
    "    \"\"\"Locate a file under outputs/ or data/raw / \u5728 outputs/ \u6216 data/raw \u4e2d\u5b9a\u4f4d\u6587\u4ef6.\"\"\"\n",
    "    for p in (OUT_DIR / filename, DATA_DIR / filename, REPO_ROOT / filename):\n",
    "        if p.exists():\n",
    "            return p\n",
    "    for root, _, files in os.walk(OUT_DIR):\n",
    "        if filename in files:\n",
    "            return Path(root) / filename\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find {filename}. Put it under data/raw or outputs/segment.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aac9f3-e9d4-4e5d-a9c0-51971d894e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:43.717078Z",
     "iopub.status.busy": "2026-01-11T10:13:43.716955Z",
     "iopub.status.idle": "2026-01-11T10:13:44.450273Z",
     "shell.execute_reply": "2026-01-11T10:13:44.450017Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "before = {'flights': 33121, 'ticket_flights': 1045726, 'tickets': 366732, 'bookings': 262788}\n",
    "after = {'flights': 32734, 'ticket_flights': 1033331, 'tickets': 362470, 'bookings': 261888}\n",
    "\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_axis_off()\n",
    "\n",
    "def box(x,y,w,h,text):\n",
    "    rect = FancyBboxPatch((x,y), w,h, boxstyle=\"round,pad=0.02,rounding_size=0.02\", linewidth=1)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x+w/2, y+h/2, text, ha='center', va='center')\n",
    "    return (x,y,w,h)\n",
    "\n",
    "# -- Use the before/after numbers calculated in the previous section / -- \u4f7f\u7528\u4e0a\u4e00\u8282\u8ba1\u7b97\u7684\u524d\u540e\u5bf9\u6bd4\u6570\u503c\n",
    "f_raw, tf_raw, t_raw, b_raw = before['flights'], before['ticket_flights'], before['tickets'], before['bookings']\n",
    "f_sel, tf_sel, t_sel, b_sel = after['flights'], after['ticket_flights'], after['tickets'], after['bookings']\n",
    "\n",
    "# -- Positioning / -- \u5e03\u5c40\u5b9a\u4f4d\n",
    "raw_f  = box(0.08, 0.76, 0.36, 0.14, f\"Flights (raw)\\n{f_raw:,}\")\n",
    "raw_tf = box(0.56, 0.76, 0.36, 0.14, f\"Ticket_flights (raw)\\n{tf_raw:,}\")\n",
    "raw_t  = box(0.08, 0.56, 0.36, 0.14, f\"Tickets (raw)\\n{t_raw:,}\")\n",
    "raw_b  = box(0.56, 0.56, 0.36, 0.14, f\"Bookings (raw)\\n{b_raw:,}\")\n",
    "\n",
    "step1  = box(0.08, 0.36, 0.36, 0.14, f\"Filter flights\\nstatus='ARRIVED'\\n\u2192 {f_sel:,}\")\n",
    "step2  = box(0.56, 0.36, 0.36, 0.14, f\"Join: tf \u00d7 flights\\n(retain segments)\\n\u2192 {tf_sel:,}\")\n",
    "step3  = box(0.08, 0.18, 0.36, 0.14, f\"Keep tickets linked\\n\u2192 {t_sel:,}\")\n",
    "step4  = box(0.56, 0.18, 0.36, 0.14, f\"Keep bookings linked\\n\u2192 {b_sel:,}\")\n",
    "final  = box(0.32, 0.02, 0.36, 0.14, f\"Selected dataset\\n(segments level)\\nrows = {tf_sel:,}\")\n",
    "\n",
    "def connect(p1, p2, rad=0.0):\n",
    "    x1,y1,w1,h1 = p1\n",
    "    x2,y2,w2,h2 = p2\n",
    "    start = (x1+w1/2, y1)      # bottom center\n",
    "    end   = (x2+w2/2, y2+h2)   # top center\n",
    "    ax.add_patch(FancyArrowPatch(start, end, arrowstyle='->',\n",
    "                                 mutation_scale=14, linewidth=1.2,\n",
    "                                 connectionstyle=f\"arc3,rad={rad}\"))\n",
    "\n",
    "# Direct vertical connections / \u76f4\u63a5\u5782\u76f4\u8fde\u63a5\n",
    "connect(raw_f, step1, 0.0)\n",
    "connect(raw_tf, step2, 0.0)\n",
    "connect(raw_t, step3, 0.0)\n",
    "connect(raw_b, step4, 0.0)\n",
    "# Use arcs for cross-connections to avoid overlap / \u4ea4\u53c9\u8fde\u63a5\u7528\u5f27\u7ebf\u907f\u514d\u91cd\u53e0\n",
    "connect(step1, step2, rad=-0.25)\n",
    "connect(step2, final, rad=0.0)\n",
    "connect(step3, final, rad=0.20)\n",
    "connect(step4, final, rad=-0.20)\n",
    "\n",
    "plt.title(\"Figure 3.1.1 Data Selection Flowchart\", pad=18)\n",
    "out = FIG_DIR/\"figure_3_1_1_selection_flow.png\"\n",
    "fig.savefig(out, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved PNG:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31020a-ff9a-4149-b702-48a7a10df0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:44.451802Z",
     "iopub.status.busy": "2026-01-11T10:13:44.451635Z",
     "iopub.status.idle": "2026-01-11T10:13:45.037199Z",
     "shell.execute_reply": "2026-01-11T10:13:45.036968Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure segment-level feature table is available for downstream cells / \u786e\u4fdd\u822a\u6bb5\u7ea7\u7279\u5f81\u8868\u53ef\u7528\u4e8e\u4e0b\u6e38\u5355\u5143\n",
    "if \"seg\" not in globals():\n",
    "    X_path = find_path(\"X_features.parquet\")\n",
    "    y_path = find_path(\"y_target.parquet\")\n",
    "    seg = pd.read_parquet(X_path)\n",
    "    y_seg = pd.read_parquet(y_path).squeeze()\n",
    "    seg[\"amount\"] = y_seg.values\n",
    "\n",
    "# Defaults if not defined earlier / \u82e5\u672a\u63d0\u524d\u5b9a\u4e49\u5219\u4f7f\u7528\u9ed8\u8ba4\u503c\n",
    "SEAT_THRESHOLD = globals().get(\"SEAT_THRESHOLD\", 240)\n",
    "WIDEBODY_KEYWORDS = globals().get(\"WIDEBODY_KEYWORDS\", [\"777\",\"787\",\"747\",\"767\",\"a330\",\"a340\",\"a350\",\"a380\",\"il-96\"])\n",
    "\n",
    "\n",
    "def show_table(df: pd.DataFrame, title: str, csv_filename: str,\n",
    "               thousands_cols=None, percent_cols=None, hide_index=True):\n",
    "    thousands_cols = thousands_cols or []\n",
    "    percent_cols   = percent_cols or []\n",
    "    t = df.copy()\n",
    "    for c in thousands_cols:\n",
    "        if c in t.columns:\n",
    "            t[c] = t[c].map(lambda x: f\"{int(x):,}\" if pd.notna(x) else \"\")\n",
    "    for c in percent_cols:\n",
    "        if c in t.columns:\n",
    "            t[c] = t[c].map(lambda v: f\"{v:.1f}%\" if pd.notna(v) else \"\")\n",
    "    out_csv = TAB_DIR/csv_filename\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved CSV -> {out_csv}\")\n",
    "    try:\n",
    "        sty = (t.style\n",
    "               .set_table_styles([{'selector':'th','props':'font-weight:bold; text-align:center;'},\n",
    "                                  {'selector':'td','props':'text-align:center;'}])\n",
    "               .set_properties(**{'text-align':'center'}))\n",
    "        if hasattr(sty, \"hide_index\"):\n",
    "            sty = sty.hide_index()\n",
    "        else:\n",
    "            sty = sty.hide(axis=\"index\")\n",
    "        print(title)\n",
    "        display(sty)\n",
    "    except Exception:\n",
    "        display(t)\n",
    "\n",
    "feature_dict = [\n",
    " # Name, definition/rule, type, granularity, business motivation / \u540d\u79f0, \u5b9a\u4e49/\u89c4\u5219, \u7c7b\u578b, \u5206\u6790\u7c92\u5ea6, \u4e1a\u52a1\u52a8\u673a\n",
    " [\"Route_Code\",\n",
    "  \"Concatenate departure_airport and arrival_airport (e.g., DME-KHV).\",\n",
    "  \"string\",\"segment (ticket_flight \u00d7 flight)\",\n",
    "  \"Capture route-level effects for pricing and revenue.\"],\n",
    "\n",
    " [\"Sched_Flight_Duration_Minutes\",\n",
    "  \"UTC(scheduled_arrival) \u2212 UTC(scheduled_departure) in minutes; negatives set to NaN.\",\n",
    "  \"float\",\"segment\",\n",
    "  \"Proxy for distance/service level; strong price driver.\"],\n",
    "\n",
    " [\"Booking_Lead_Time_Days\",\n",
    "  \"UTC(dep) \u2212 UTC(book_date) in days; negatives set to NaN per 3.2 policy.\",\n",
    "  \"float\",\"segment\",\n",
    "  \"Captures demand timing and the early-bird premium phenomenon.\"],\n",
    "\n",
    " [\"Departure_DOW\",\"Day of week of UTC(dep) [0=Mon].\",\"int\",\"segment\",\n",
    "  \"Weekly patterns without requiring long-span seasonality.\"],\n",
    "\n",
    " [\"Departure_Hour\",\"Hour of day of UTC(dep) [0\u201323].\",\"int\",\"segment\",\n",
    "  \"Intra-day pricing and operational patterns.\"],\n",
    "\n",
    " [\"Is_Weekend\",\"1 if DOW \u2208 {Sat,Sun}, else 0.\",\"int (0/1)\",\"segment\",\n",
    "  \"Captures leisure/business mix by calendar.\"],\n",
    "\n",
    " [\"Fare_Class\",\"Cleaned fare_conditions \u2208 {Economy, Comfort, Business}.\",\n",
    "  \"category\",\"segment\",\"Primary driver of per-segment price.\"],\n",
    "\n",
    " [\"Fare_Class_Ordinal\",\"Map Economy=0, Comfort=1, Business=2.\",\"int\",\"segment\",\n",
    "  \"Order-aware representation for linear models.\"],\n",
    "\n",
    " [\"Is_Premium_Cabin\",\"1 if Fare_Class \u2208 {Comfort,Business}, else 0.\",\n",
    "  \"int (0/1)\",\"segment\",\"Binary simplification for rule cards and trees.\"],\n",
    "\n",
    " [\"Aircraft_Model_EN\",\"English label parsed from JSON model.\",\"string\",\"segment\",\n",
    "  \"Human-readable aircraft feature for interpretation.\"],\n",
    "\n",
    " [\"Seats_Per_Aircraft\",\"Count seats per aircraft_code from seats table.\",\n",
    "  \"int\",\"segment\",\"Capacity proxy; helps explain cabin mix & price.\"],\n",
    "\n",
    " [\"Is_Widebody\",\n",
    "  f\"1 if Seats_Per_Aircraft \u2265 {SEAT_THRESHOLD} OR Aircraft_Model_EN contains any of {WIDEBODY_KEYWORDS}.\",\n",
    "  \"int (0/1)\",\"segment\",\"Flag widebody operations affecting price & perception.\"],\n",
    "\n",
    " [\"Is_LongHaul\",\"1 if Sched_Flight_Duration_Minutes \u2265 240, else 0.\",\n",
    "  \"int (0/1)\",\"segment\",\"Separates long vs short haul for pattern search.\"]\n",
    "]\n",
    "\n",
    "tbl_331 = pd.DataFrame(feature_dict, columns=[\n",
    "    \"Feature Name\",\"Definition / Rule\",\"Data Type\",\"Analytical Grain\",\"Rationale\"\n",
    "])\n",
    "show_table(tbl_331, \"Table 3.3.1 \u2013 Feature Dictionary\",\n",
    "           \"table_3_3_1_feature_dictionary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c9c34-c3e5-48ab-b697-76dac6b235c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:45.038660Z",
     "iopub.status.busy": "2026-01-11T10:13:45.038515Z",
     "iopub.status.idle": "2026-01-11T10:13:45.067351Z",
     "shell.execute_reply": "2026-01-11T10:13:45.067097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Type checks / \u7c7b\u578b\u68c0\u67e5\n",
    "print(\"Type Check:\",\n",
    "      isinstance(seg['Sched_Flight_Duration_Minutes'].dropna().iloc[0], (int,float,np.floating)),\n",
    "      isinstance(seg['Booking_Lead_Time_Days'].dropna().iloc[0], (int,float,np.floating)),\n",
    "      seg['Is_Premium_Cabin'].dropna().isin([0,1]).all(),\n",
    "      seg['Is_Widebody'].dropna().isin([0,1]).all())\n",
    "\n",
    "# Range checks / \u53d6\u503c\u8303\u56f4\u68c0\u67e5\n",
    "neg_dur = int((seg['Sched_Flight_Duration_Minutes'] < 0).sum(skipna=True))\n",
    "neg_lead = int((seg['Booking_Lead_Time_Days'] < 0).sum(skipna=True))\n",
    "print(f\"Range Check: negatives \u2013 duration={neg_dur}, lead_time={neg_lead} (both expected 0 after cleaning)\")\n",
    "\n",
    "# Logic check: premium cabin \u6bd4\u4f8b \u2248 Business/Comfort \u5360\u6bd4\n",
    "p1 = seg['Is_Premium_Cabin'].mean()\n",
    "p2 = seg['Fare_Class'].isin(['Business','Comfort']).mean()\n",
    "print(f\"Logic Check: Is_Premium_Cabin share = {p1:.4f} | \"\n",
    "      f\"share(Business/Comfort) = {p2:.4f} | \u0394={abs(p1-p2):.6f} (expect ~0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a97bb2-0334-4229-9261-c0285a978e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:45.068667Z",
     "iopub.status.busy": "2026-01-11T10:13:45.068580Z",
     "iopub.status.idle": "2026-01-11T10:13:46.408285Z",
     "shell.execute_reply": "2026-01-11T10:13:46.407889Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "preview_cols = ['ticket_no','Route_Code','Fare_Class','Is_Premium_Cabin',\n",
    "                'Sched_Flight_Duration_Minutes','Booking_Lead_Time_Days',\n",
    "                'Departure_DOW','Departure_Hour','Is_Weekend',\n",
    "                'Aircraft_Model_EN','Seats_Per_Aircraft','Is_Widebody','amount']\n",
    "preview_df = seg[preview_cols].head(12).copy()\n",
    "\n",
    "fig_h = 1.2 + 0.35*len(preview_df)\n",
    "fig, ax = plt.subplots(figsize=(16, fig_h))\n",
    "ax.axis('off')\n",
    "tbl = ax.table(cellText=preview_df.values,\n",
    "               colLabels=preview_df.columns, loc='center')\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(8)\n",
    "tbl.scale(1, 1.2)\n",
    "plt.title(\"Figure 3.3.1 Preview of Constructed Features\", pad=10)\n",
    "plt.tight_layout()\n",
    "png1 = FIG_DIR/\"figure_3_3_1_feature_preview.png\"\n",
    "plt.savefig(png1, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", png1)\n",
    "\n",
    "# A) Flight duration (minutes) / A\uff09\u98de\u884c\u65f6\u957f\uff08\u5206\u949f\uff09\n",
    "dur = seg['Sched_Flight_Duration_Minutes'].astype(float)\n",
    "q99 = np.nanpercentile(dur, 99) if dur.notna().any() else 0\n",
    "dur_clip = dur.clip(upper=q99)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(dur_clip.dropna(), bins=40)\n",
    "plt.title(\"Figure 3.3.2a Sched_Flight_Duration_Minutes (clipped at 99th pct)\")\n",
    "plt.xlabel(\"Minutes\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "png2a = FIG_DIR/\"figure_3_3_2a_duration_hist.png\"\n",
    "plt.savefig(png2a, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", png2a)\n",
    "\n",
    "# B) Booking lead time (days) / B\uff09\u8ba2\u7968\u63d0\u524d\u671f\uff08\u5929\uff09\n",
    "lead = seg['Booking_Lead_Time_Days'].astype(float)\n",
    "q99b = np.nanpercentile(lead, 99) if lead.notna().any() else 0\n",
    "lead_clip = lead.clip(lower=0, upper=q99b)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(lead_clip.dropna(), bins=40)\n",
    "plt.title(\"Figure 3.3.2b Booking_Lead_Time_Days (clipped at 99th pct)\")\n",
    "plt.xlabel(\"Days\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "png2b = FIG_DIR/\"figure_3_3_2b_leadtime_hist.png\"\n",
    "plt.savefig(png2b, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", png2b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae99ea2-800d-411e-bc4c-a047d890baae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:46.409943Z",
     "iopub.status.busy": "2026-01-11T10:13:46.409726Z",
     "iopub.status.idle": "2026-01-11T10:13:51.658363Z",
     "shell.execute_reply": "2026-01-11T10:13:51.658030Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, sqlite3\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    conn\n",
    "except NameError:\n",
    "    import glob\n",
    "    cands = glob.glob(str(DATA_DIR / \"*.sqlite\"))+glob.glob(str(DATA_DIR / \"*.db\"))+glob.glob(str(DATA_DIR / \"*.sqlite3\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"No SQLite found under data/. Please place airlines_db.sqlite there.\")\n",
    "\n",
    "def to_utc(x): return pd.to_datetime(x, errors='coerce', utc=True)\n",
    "\n",
    "def get_json_en(sr: pd.Series):\n",
    "    def _x(s):\n",
    "        if pd.isna(s) or s == '\\\\N': return pd.NA\n",
    "        try: return json.loads(s).get('en', pd.NA)\n",
    "        except Exception: return pd.NA\n",
    "    return sr.apply(_x)\n",
    "\n",
    "def show_table(df: pd.DataFrame, title: str, csv_filename: str):\n",
    "    out_csv = TAB_DIR/csv_filename\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved CSV -> {out_csv}\")\n",
    "    t = df.copy()\n",
    "    try:\n",
    "        sty = (t.style\n",
    "               .set_table_styles([{'selector':'th','props':'font-weight:bold; text-align:center;'},\n",
    "                                  {'selector':'td','props':'text-align:center;'}])\n",
    "               .set_properties(**{'text-align':'center'}))\n",
    "        # \u517c\u5bb9\u4e0d\u540c Pandas \u7248\u672c\n",
    "        if hasattr(sty, \"hide_index\"):\n",
    "            sty = sty.hide_index()\n",
    "        else:\n",
    "            sty = sty.hide(axis=\"index\")\n",
    "        print(title)\n",
    "        display(sty)\n",
    "    except Exception:\n",
    "        print(title)\n",
    "        display(t)\n",
    "\n",
    "# 1) Load raw tables / 1\uff09\u52a0\u8f7d\u539f\u59cb\u8868\n",
    "tf = pd.read_sql(\"SELECT ticket_no, flight_id, fare_conditions, amount FROM ticket_flights;\", conn)\n",
    "t  = pd.read_sql(\"SELECT ticket_no, book_ref FROM tickets;\", conn)\n",
    "b  = pd.read_sql(\"SELECT book_ref, book_date, total_amount FROM bookings;\", conn)\n",
    "fl = pd.read_sql(\"\"\"\n",
    "    SELECT flight_id, flight_no, scheduled_departure, scheduled_arrival,\n",
    "           departure_airport, arrival_airport, status, aircraft_code\n",
    "    FROM flights;\n",
    "\"\"\", conn)\n",
    "ac = pd.read_sql(\"SELECT aircraft_code, model, range FROM aircrafts_data;\", conn)\n",
    "ap = pd.read_sql(\"SELECT airport_code, airport_name, city, timezone FROM airports_data;\", conn)\n",
    "\n",
    "# before counts / \u6e05\u6d17\u524d\u8ba1\u6570\n",
    "n_tf_raw = len(tf); n_fl_raw = len(fl); n_t_raw = len(t); n_b_raw = len(b)\n",
    "\n",
    "# keep arrived flights / \u4ec5\u4fdd\u7559\u5df2\u5230\u8fbe\u822a\u73ed\n",
    "fl_arr = fl.loc[fl['status'].str.upper()=='ARRIVED'].copy()\n",
    "n_fl_arr = len(fl_arr)\n",
    "\n",
    "# 2) Core joins (inner) with validation / 2\uff09\u6838\u5fc3\u8fde\u63a5\uff08\u5185\u8fde\u63a5\uff09\u4e0e\u6821\u9a8c\n",
    "step_log = []\n",
    "\n",
    "step_log.append([\"ticket_flights (raw)\", n_tf_raw, tf.shape[1]])\n",
    "step_log.append([\"flights (ARRIVED only)\", n_fl_arr, fl_arr.shape[1]])\n",
    "\n",
    "# tf \u00d7 flights (inner) many_to_one / tf \u00d7 flights\uff08\u5185\u8fde\u63a5\uff09\u591a\u5bf9\u4e00\n",
    "j1 = tf.merge(fl_arr, on='flight_id', how='inner', validate='many_to_one')\n",
    "step_log.append([\"join1: tf \u00d7 flights (inner)\", len(j1), j1.shape[1]])\n",
    "\n",
    "# + tickets (inner) many_to_one / + tickets\uff08\u5185\u8fde\u63a5\uff09\u591a\u5bf9\u4e00\n",
    "j2 = j1.merge(t, on='ticket_no', how='inner', validate='many_to_one')\n",
    "step_log.append([\"join2: + tickets (inner)\", len(j2), j2.shape[1]])\n",
    "\n",
    "# + bookings (inner) many_to_one / + bookings\uff08\u5185\u8fde\u63a5\uff09\u591a\u5bf9\u4e00\n",
    "j3 = j2.merge(b, on='book_ref', how='inner', validate='many_to_one')\n",
    "step_log.append([\"join3: + bookings (inner)\", len(j3), j3.shape[1]])\n",
    "\n",
    "# 3) Lookups (left) / 3\uff09\u67e5\u627e\u8868\uff08\u5de6\u8fde\u63a5\uff09\n",
    "# aircrafts_data \u2192 English model, left join (many_to_one) / aircrafts_data \u2192 \u82f1\u6587\u673a\u578b\uff0c\u5de6\u8fde\u63a5\uff08\u591a\u5bf9\u4e00\uff09\n",
    "ac2 = ac.copy()\n",
    "ac2['Aircraft_Model_EN'] = get_json_en(ac2['model'])\n",
    "j4 = j3.merge(ac2[['aircraft_code','Aircraft_Model_EN','range']], on='aircraft_code',\n",
    "              how='left', validate='many_to_one')\n",
    "step_log.append([\"join4: + aircrafts_data (left)\", len(j4), j4.shape[1]])\n",
    "\n",
    "# airports_data (dep/arr) \u2192 English name / airports_data\uff08\u51fa\u53d1/\u5230\u8fbe\uff09\u2192 \u82f1\u6587\u540d\u79f0\n",
    "ap2 = ap.copy()\n",
    "ap2['airport_name_en'] = get_json_en(ap2['airport_name'])\n",
    "\n",
    "dep = ap2.rename(columns={'airport_code':'departure_airport',\n",
    "                          'airport_name_en':'Departure_Airport_Name_EN',\n",
    "                          'city':'Departure_City','timezone':'Departure_TZ'})\n",
    "arr = ap2.rename(columns={'airport_code':'arrival_airport',\n",
    "                          'airport_name_en':'Arrival_Airport_Name_EN',\n",
    "                          'city':'Arrival_City','timezone':'Arrival_TZ'})\n",
    "\n",
    "j5 = j4.merge(dep[['departure_airport','Departure_Airport_Name_EN','Departure_City','Departure_TZ']],\n",
    "              on='departure_airport', how='left', validate='many_to_one')\n",
    "step_log.append([\"join5: + airports_data (dep,left)\", len(j5), j5.shape[1]])\n",
    "\n",
    "final_df = j5.merge(arr[['arrival_airport','Arrival_Airport_Name_EN','Arrival_City','Arrival_TZ']],\n",
    "                    on='arrival_airport', how='left', validate='many_to_one')\n",
    "step_log.append([\"join6: + airports_data (arr,left)\", len(final_df), final_df.shape[1]])\n",
    "\n",
    "# 4) Programmatic validation / 4\uff09\u7a0b\u5e8f\u5316\u6821\u9a8c\n",
    "# Row-count conservation / \u884c\u6570\u5b88\u6052\u68c0\u67e5\n",
    "assert len(final_df) <= len(j1), \"Row explosion detected after lookups.\"\n",
    "\n",
    "# Primary key uniqueness / \u4e3b\u952e\u552f\u4e00\u6027\n",
    "pk_unique = (final_df[['ticket_no','flight_id']].drop_duplicates().shape[0] == len(final_df))\n",
    "assert pk_unique, \"Primary key (ticket_no, flight_id) is NOT unique.\"\n",
    "\n",
    "# Lookup coverage (expected \u2248100%) / \u67e5\u627e\u8986\u76d6\u7387\uff08\u671f\u671b\u2248100%\uff09\n",
    "cov_ac   = final_df['Aircraft_Model_EN'].notna().mean()*100\n",
    "cov_dep  = final_df['Departure_Airport_Name_EN'].notna().mean()*100\n",
    "cov_arr  = final_df['Arrival_Airport_Name_EN'].notna().mean()*100\n",
    "\n",
    "print(f\"Lookup coverage \u2192 aircrafts_data: {cov_ac:.1f}% | dep-airport: {cov_dep:.1f}% | arr-airport: {cov_arr:.1f}%\")\n",
    "\n",
    "# 5) OUTPUTS FOR THE REPORT / 5\uff09\u62a5\u544a\u8f93\u51fa\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "ax.axis('off')\n",
    "\n",
    "# boxes (x,y,w,h) / \u65b9\u6846\u53c2\u6570\uff08x,y,w,h\uff09\n",
    "boxes = {\n",
    "    \"Bookings\": (0.05, 0.75, 0.22, 0.12),\n",
    "    \"Tickets\":  (0.35, 0.75, 0.22, 0.12),\n",
    "    \"Ticket_flights\": (0.65, 0.75, 0.27, 0.12),\n",
    "    \"Flights (ARRIVED)\": (0.35, 0.45, 0.27, 0.12),\n",
    "    \"Aircrafts_data\": (0.05, 0.15, 0.22, 0.12),\n",
    "    \"Airports_data (dep)\": (0.40, 0.15, 0.27, 0.12),\n",
    "    \"Airports_data (arr)\": (0.72, 0.15, 0.27, 0.12)\n",
    "}\n",
    "for k,(x,y,w,h) in boxes.items():\n",
    "    ax.add_patch(patches.FancyBboxPatch((x,y), w,h, boxstyle=\"round,pad=0.02\", fill=False))\n",
    "    ax.text(x+w/2, y+h/2, k, ha='center', va='center', fontsize=11)\n",
    "\n",
    "# arrows (one-to-many core, left joins) / \u8fde\u63a5\u7bad\u5934\uff08\u6838\u5fc3\u4e00\u5bf9\u591a\uff0c\u5de6\u8fde\u63a5\uff09\n",
    "def arrow(x1,y1,x2,y2):\n",
    "    ax.annotate(\"\", xy=(x2,y2), xytext=(x1,y1),\n",
    "                arrowprops=dict(arrowstyle=\"->\", lw=1.5))\n",
    "\n",
    "# core path / \u6838\u5fc3\u8def\u5f84\n",
    "arrow(0.27, 0.81, 0.35, 0.81)   # bookings -> tickets\n",
    "arrow(0.57, 0.81, 0.65, 0.81)   # tickets -> tf\n",
    "arrow(0.78, 0.75, 0.51, 0.57)   # tf -> flights\n",
    "\n",
    "# lookups (left joins) / \u67e5\u627e\u8868\uff08\u5de6\u8fde\u63a5\uff09\n",
    "arrow(0.49, 0.45, 0.16, 0.27)   # flights -> aircrafts_data\n",
    "arrow(0.49, 0.45, 0.53, 0.27)   # flights -> airports dep\n",
    "arrow(0.62, 0.45, 0.86, 0.27)   # flights -> airports arr\n",
    "\n",
    "ax.set_title(\"Figure 3.4.1 Entity\u2013Relationship Diagram for Integration\", pad=10)\n",
    "plt.tight_layout()\n",
    "f1 = FIG_DIR/\"figure_3_4_1_ER.png\"\n",
    "plt.savefig(f1, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", f1)\n",
    "\n",
    "tbl = pd.DataFrame(step_log, columns=[\"Step\",\"Rows\",\"Cols\"])\n",
    "show_table(tbl, \"Table 3.4.1 \u2013 Integration Step Row Counts\",\n",
    "           \"table_3_4_1_integration_steps.csv\")\n",
    "\n",
    "buf = StringIO()\n",
    "final_df.info(buf=buf)\n",
    "txt = buf.getvalue()\n",
    "\n",
    "plt.figure(figsize=(13, min(10, 2 + 0.22*len(final_df.columns))))\n",
    "plt.axis('off')\n",
    "plt.text(0.01, 0.99, txt, va='top', family='monospace')\n",
    "plt.title(\"Figure 3.4.2 Schema of the Final Integrated Table\", pad=10)\n",
    "plt.tight_layout()\n",
    "f2 = FIG_DIR/\"figure_3_4_2_schema.png\"\n",
    "plt.savefig(f2, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", f2)\n",
    "\n",
    "print(\"\\n\u2014 Validation summary \u2014\")\n",
    "print(f\"Rows(tf raw) = {n_tf_raw:,} | Rows(flights raw) = {n_fl_raw:,} | Rows(ARRIVED) = {n_fl_arr:,}\")\n",
    "print(f\"Rows after tf\u00d7flights = {len(j1):,} | final_df rows = {len(final_df):,}\")\n",
    "print(f\"Primary key (ticket_no, flight_id) unique? {pk_unique}\")\n",
    "print(f\"Lookup coverage: aircraft_model_en={cov_ac:.1f}%, dep_airport_name_en={cov_dep:.1f}%, arr_airport_name_en={cov_arr:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05abf6-510f-42e5-9490-babc7cf1ba42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:51.660016Z",
     "iopub.status.busy": "2026-01-11T10:13:51.659916Z",
     "iopub.status.idle": "2026-01-11T10:13:54.620485Z",
     "shell.execute_reply": "2026-01-11T10:13:54.618906Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def to_utc(s):\n",
    "    return pd.to_datetime(s, errors='coerce', utc=True)\n",
    "\n",
    "def normalise_fare(x:str):\n",
    "    if pd.isna(x): return pd.NA\n",
    "    s = str(x).strip().title()\n",
    "    mapping = {\n",
    "        'Biz':'Business','B':'Business',\n",
    "        'Premium Economy':'Comfort','Premium':'Comfort','Prem Eco':'Comfort',\n",
    "        'Eco':'Economy','E':'Economy'\n",
    "    }\n",
    "    return mapping.get(s, s)  # \u6570\u636e\u672c\u8eab\u5927\u591a\u5df2\u662f Business/Comfort/Economy\n",
    "\n",
    "def show_table(df: pd.DataFrame, title: str, csv_name: str):\n",
    "    csv_path = TAB_DIR/csv_name\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved CSV -> {_rel(csv_path)}\")\n",
    "    try:\n",
    "        sty = (df.style\n",
    "               .set_table_styles([{'selector':'th','props':'font-weight:bold; text-align:center;'},\n",
    "                                  {'selector':'td','props':'text-align:center;'}])\n",
    "               .set_properties(**{'text-align':'center'}))\n",
    "        # \u517c\u5bb9\u4e0d\u540c pandas \u7248\u672c\n",
    "        if hasattr(sty, \"hide_index\"):\n",
    "            sty = sty.hide_index()\n",
    "        else:\n",
    "            sty = sty.hide(axis=\"index\")\n",
    "        display(sty.set_caption(title))\n",
    "    except Exception:\n",
    "        print(title)\n",
    "        display(df)\n",
    "\n",
    "try:\n",
    "    _ = final_df.shape\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"\u672a\u68c0\u6d4b\u5230 final_df\u3002\u8bf7\u5148\u8fd0\u884c 3.4 \u7684\u6574\u5408\u4ee3\u7801\u4ee5\u751f\u6210 final_df DataFrame\u3002\") from e\n",
    "\n",
    "# 3.5.1 Normalize types & build light features (formatting only) / 3.5.1 \u89c4\u8303\u7c7b\u578b & \u6784\u5efa\u8f7b\u91cf\u7279\u5f81\uff08\u4ec5\u683c\u5f0f\u5316\u6240\u9700\uff09\n",
    "df = final_df.copy()\n",
    "\n",
    "# \u7edf\u4e00\u65f6\u95f4\u4e3a UTC\u3001\u5e76\u52a0\u5f3a\u5e38\u7528\u5b57\u6bb5\n",
    "df['book_date_utc']          = to_utc(df.get('book_date'))\n",
    "df['sched_departure_utc']    = to_utc(df.get('scheduled_departure'))\n",
    "df['sched_arrival_utc']      = to_utc(df.get('scheduled_arrival'))\n",
    "\n",
    "# Route code (departure-arrival) / \u8def\u7ebf\u7f16\u7801\uff08\u51fa\u53d1-\u5230\u8fbe\uff09\n",
    "df['route_code'] = (df['departure_airport'].astype(str) + '-' +\n",
    "                    df['arrival_airport'].astype(str))\n",
    "\n",
    "# Cabin normalization & categorization / \u8231\u4f4d\u5f52\u4e00\u5316\u3001\u7c7b\u522b\u5316\n",
    "df['fare_conditions'] = df['fare_conditions'].map(normalise_fare)\n",
    "df['fare_conditions'] = pd.Categorical(df['fare_conditions'],\n",
    "                                       categories=['Economy','Comfort','Business'],\n",
    "                                       ordered=True)\n",
    "\n",
    "# Compute scheduled flight duration (min) & booking lead time (days) / \u8ba1\u7b97\u8ba1\u5212\u98de\u884c\u65f6\u957f\uff08\u5206\u949f\uff09\u4e0e\u8ba2\u7968\u63d0\u524d\u671f\uff08\u5929\uff09\n",
    "df['sched_flight_duration_minutes'] = (\n",
    "    (df['sched_arrival_utc'] - df['sched_departure_utc'])\n",
    "    .dt.total_seconds()/60.0\n",
    ")\n",
    "df['booking_lead_time_days'] = (\n",
    "    (df['sched_departure_utc'] - df['book_date_utc'])\n",
    "    .dt.total_seconds()/(3600*24)\n",
    ")\n",
    "\n",
    "# \u65f6\u95f4\u6d3e\u751f\uff08\u7528\u4e8e\u540e\u7eed ColumnTransformer\uff09\n",
    "df['hour_of_day']  = df['sched_departure_utc'].dt.hour\n",
    "df['day_of_week']  = df['sched_departure_utc'].dt.dayofweek  # Monday=0\n",
    "df['is_weekend']   = df['day_of_week'].isin([5,6]).astype('int8')\n",
    "\n",
    "# Cabin numeric index & premium flag / \u8231\u4f4d\u6570\u503c\u7d22\u5f15\u4e0e\u662f\u5426\u9ad8\u7aef\u8231\n",
    "cabin_map = {'Economy':0, 'Comfort':1, 'Business':2}\n",
    "df['cabin_index']      = df['fare_conditions'].map(cabin_map).astype('Int8')\n",
    "df['is_premium_cabin'] = (df['cabin_index']>=1).astype('int8')\n",
    "\n",
    "# 3.5.2 \u89d2\u8272\u5206\u79bb\uff08y & X\uff09\u5e76\u6301\u4e45\u5316\n",
    "# \u2014\u2014 \u76ee\u6807\u53d8\u91cf\uff08y\uff09\uff1atotal_amount\uff08\u6309 4.2 \u518d\u505a\u5bf9\u6570\u53d8\u6362\uff09\n",
    "y = df['total_amount'].astype('float64')\n",
    "\n",
    "# \u2014\u2014 \u9884\u6d4b\u5b50\u96c6\uff08X\uff09\uff1a\u6570\u503c\u4e0e\u5206\u7c7b\u578b\uff08\u4f9b\u540e\u7eed ColumnTransformer\uff09\n",
    "X_num_cols = [\n",
    "    'sched_flight_duration_minutes','booking_lead_time_days',\n",
    "    'cabin_index','is_premium_cabin','hour_of_day','day_of_week'\n",
    "]\n",
    "X_cat_cols = [\n",
    "    'fare_conditions','route_code','Aircraft_Model_EN'\n",
    "]\n",
    "# \u786e\u4fdd\u7c7b\u522b dtype\n",
    "for c in X_cat_cols:\n",
    "    df[c] = df[c].astype('category')\n",
    "\n",
    "X = df[X_num_cols + X_cat_cols].copy()\n",
    "\n",
    "# \u2014\u2014 \u8f93\u51fa\u5230 Parquet\uff08\u82e5\u65e0 pyarrow \u81ea\u52a8 CSV \u515c\u5e95\uff09\n",
    "def _save_parquet_or_csv(frame, path_parquet, path_csv):\n",
    "    try:\n",
    "        frame.to_parquet(path_parquet, index=False)\n",
    "        print(f\"Saved Parquet -> {_rel(path_parquet)}\")\n",
    "    except Exception as e:\n",
    "        frame.to_csv(path_csv, index=False)\n",
    "        print(f\"[pyarrow/fastparquet \u4e0d\u53ef\u7528\uff0c\u6539\u5b58 CSV] -> {_rel(path_csv)}  ({e.__class__.__name__}: {e})\")\n",
    "\n",
    "_save_parquet_or_csv(df, OUT_DIR/\"master_formatted.parquet\", OUT_DIR/\"master_formatted.csv\")\n",
    "_save_parquet_or_csv(X,  OUT_DIR/\"X_features.parquet\",      OUT_DIR/\"X_features.csv\")\n",
    "_save_parquet_or_csv(y.to_frame('total_amount'),\n",
    "                     OUT_DIR/\"y_target.parquet\",\n",
    "                     OUT_DIR/\"y_target.csv\")\n",
    "\n",
    "print(\"\\nRoles \u2192 y shape:\", y.shape, \"| X shape:\", X.shape)\n",
    "print(\"Numeric predictors:\", X_num_cols)\n",
    "print(\"Categorical predictors:\", X_cat_cols)\n",
    "\n",
    "# 3.5.3 Figure 3.5.1 \u2014\u2014 \u67b6\u6784\u622a\u56fe\uff08df.info\uff09\n",
    "buf = StringIO()\n",
    "df.info(buf=buf)\n",
    "schema_txt = buf.getvalue()\n",
    "\n",
    "plt.figure(figsize=(13, min(10, 2 + 0.22*len(df.columns))))\n",
    "plt.axis('off')\n",
    "plt.text(0.01, 0.99, schema_txt, va='top', family='monospace')\n",
    "plt.title(\"Figure 3.5.1 Schema of Formatted Data\", pad=10)\n",
    "plt.tight_layout()\n",
    "fig_path = FIG_DIR/\"figure_3_5_1_schema.png\"\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", _rel(fig_path))\n",
    "\n",
    "# 3.5.4 Table 3.5.1 \u2014\u2014 \u8231\u4f4d\u6807\u7b7e\u5f52\u4e00\u5316\u7ed3\u679c\n",
    "fare_tbl = (df['fare_conditions']\n",
    "            .value_counts(dropna=False)\n",
    "            .rename_axis('fare_conditions')\n",
    "            .reset_index(name='rows'))\n",
    "show_table(fare_tbl, \"Table 3.5.1 \u2014 Label Normalisation for fare_conditions\",\n",
    "           \"table_3_5_1_fare_counts.csv\")\n",
    "\n",
    "# 3.5.5 Report-ready confirmation sentence / 3.5.5 \u4f9b\u62a5\u544a\u7c98\u8d34\u7684\u786e\u8ba4\u8bed\u53e5\n",
    "TARGET_TRANSFORM = \"log\"   # \u53ef\u9009\uff1a\"log\" \u6216 \"yeo-johnson\"\n",
    "print(\"\\nReport snippet:\")\n",
    "print(f\"- Target (y): total_amount. Chosen transform for Step 4.2 = {TARGET_TRANSFORM} (natural log).\")\n",
    "print(\"- Files saved under outputs/: master_formatted.parquet, X_features.parquet, y_target.parquet.\")\n",
    "print(\"- Booking-level files: not required for current plan; segment-level is the analytical grain.\")\n",
    "\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "assert 'X' in globals() and 'y' in globals(), \"\u8bf7\u5148\u5728\u4e0a\u6587\u751f\u6210 X \u548c y \u5bf9\u8c61\u3002\"\n",
    "\n",
    "y_s = pd.Series(y).astype(float)\n",
    "# \u5982\u679c y \u5df2\u7ecf\u662f\u5bf9\u6570\uff08\u5217\u540d\u542b 'log'\uff09\uff0c\u76f4\u63a5\u4fdd\u5b58\uff1b\u5426\u5219\u5bf9 total_amount \u53d6 log\n",
    "if (y_s.name or '').lower().find('log') >= 0:\n",
    "    y_out = y_s.rename('log_total_amount')\n",
    "else:\n",
    "    if (y_s <= 0).any():\n",
    "        raise ValueError(\"y \u542b\u975e\u6b63\u6570\uff0c\u65e0\u6cd5\u53d6\u5bf9\u6570\u3002\u8bf7\u786e\u8ba4 y \u662f\u5426\u5e94\u4e3a\u5bf9\u6570\u540e\u7684\u76ee\u6807\u3002\")\n",
    "    y_out = np.log(y_s).rename('log_total_amount')\n",
    "\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Persist to disk / \u843d\u76d8\n",
    "pd.DataFrame(y_out).to_parquet(OUT_DIR/\"y_target.parquet\")\n",
    "pd.DataFrame(X).to_parquet(OUT_DIR/\"X_features.parquet\")\n",
    "\n",
    "print(\"Saved -> X_features.parquet:\", pd.read_parquet(OUT_DIR/\"X_features.parquet\").shape)\n",
    "print(\"Saved -> y_target.parquet  :\", pd.read_parquet(OUT_DIR/\"y_target.parquet\").shape, list(pd.read_parquet(OUT_DIR/\"y_target.parquet\").columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfc07b-d66b-4c0e-a699-26777ea5935b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:54.622687Z",
     "iopub.status.busy": "2026-01-11T10:13:54.622563Z",
     "iopub.status.idle": "2026-01-11T10:13:54.648175Z",
     "shell.execute_reply": "2026-01-11T10:13:54.647910Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "X_path = find_path(\"X_features.parquet\")\n",
    "y_path = find_path(\"y_target.parquet\")\n",
    "\n",
    "X = pd.read_parquet(X_path)\n",
    "y = pd.read_parquet(y_path).squeeze()\n",
    "y.name = y.name or \"y\"\n",
    "\n",
    "# \u4e22\u5f03\u4efb\u4f55\u4ecd\u7136\u662f datetime \u7684\u5217\uff1b\u5e03\u5c14\u21920/1\uff08\u907f\u514d\u524d\u9762\u4f60\u9047\u5230\u7684 dtype \u62a5\u9519\uff09\n",
    "dt_cols = [c for c in X.columns if is_datetime64_any_dtype(X[c])]\n",
    "if dt_cols:\n",
    "    X = X.drop(columns=dt_cols)\n",
    "\n",
    "for c in X.select_dtypes(include=\"bool\").columns:\n",
    "    X[c] = X[c].astype(int)\n",
    "\n",
    "num_cols = list(X.select_dtypes(include=[\"number\"]).columns)\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(f\"X shape = {X.shape}; y shape = {y.shape}\")\n",
    "print(f\"Numeric cols: {len(num_cols)}, Categorical cols: {len(cat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751afc11-161e-4d04-beea-e5441f9edd2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:54.649723Z",
     "iopub.status.busy": "2026-01-11T10:13:54.649607Z",
     "iopub.status.idle": "2026-01-11T10:13:56.621262Z",
     "shell.execute_reply": "2026-01-11T10:13:56.620882Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.30\n",
    "Q = 10  # use deciles for stratification\n",
    "\n",
    "# Load target / \u52a0\u8f7d\u76ee\u6807\u53d8\u91cf\n",
    "ydf = pd.read_parquet(OUT_DIR/\"y_target.parquet\")\n",
    "assert \"log_total_amount\" in ydf.columns, \"y_target.parquet must include log_total_amount\"\n",
    "y = ydf[\"log_total_amount\"].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "index_all = y.index\n",
    "\n",
    "# Stratified split by quantiles (70/30) / \u6309\u5206\u4f4d\u5206\u5c42\u5207\u5206\uff0870/30\uff09\n",
    "bins = pd.qcut(y, q=Q, labels=False, duplicates=\"drop\")\n",
    "idx_train, idx_test = train_test_split(index_all, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=bins)\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n",
    "\n",
    "# Persist split indices / \u6301\u4e45\u5316\u5207\u5206\u7d22\u5f15\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pd.Series(idx_train, name=\"train_index\").to_csv(ART_DIR/\"train_index.csv\", index=False)\n",
    "pd.Series(idx_test,  name=\"test_index\").to_csv(ART_DIR/\"test_index.csv\", index=False)\n",
    "\n",
    "# Histogram overlay / \u76f4\u65b9\u56fe\u53e0\u52a0\n",
    "def fd_bins(a):\n",
    "    a = np.asarray(a)\n",
    "    a = a[np.isfinite(a)]\n",
    "    q75, q25 = np.percentile(a, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    n = len(a)\n",
    "    if iqr == 0 or n <= 1:\n",
    "        return 30\n",
    "    h = 2 * iqr * (n ** (-1/3))\n",
    "    return max(15, min(60, int(np.ceil((a.max() - a.min()) / h))))\n",
    "\n",
    "bins_n = fd_bins(y)\n",
    "hist_range = (min(y_train.min(), y_test.min()), max(y_train.max(), y_test.max()))\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.hist(y_train, bins=bins_n, range=hist_range, density=True, alpha=0.6, label=f\"Train (n={len(y_train)})\")\n",
    "plt.hist(y_test,  bins=bins_n, range=hist_range, density=True, alpha=0.6, label=f\"Test  (n={len(y_test)})\")\n",
    "plt.xlabel(\"log(total_amount)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Transformed Target in Train vs Test (Stratified by Quantiles)\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.legend()\n",
    "\n",
    "# QC annotation: max decile diff / \u8d28\u91cf\u6807\u6ce8\uff1a\u6700\u5927\u5206\u4f4d\u5dee\n",
    "train_q = y_train.quantile(np.linspace(0, 1, 11))\n",
    "test_q = y_test.quantile(np.linspace(0, 1, 11))\n",
    "max_abs = (train_q - test_q).abs().max()\n",
    "plt.text(0.02, 0.96, f\"Max decile diff = {max_abs:.3f}\", transform=plt.gca().transAxes, va=\"top\")\n",
    "\n",
    "# Save figures / \u4fdd\u5b58\u56fe\u8868\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "base = FIG_DIR/\"Fig4_3_1_StratifiedSplit_TargetDist_v1\"\n",
    "plt.savefig(base.with_suffix(\".png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(base.with_suffix(\".svg\"), bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved figure ->\", _rel(base.with_suffix(\".png\")), \"and\", _rel(base.with_suffix(\".svg\")))\n",
    "print(\"Indices saved ->\", _rel(ART_DIR/\"train_index.csv\"), \"and\", _rel(ART_DIR/\"test_index.csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
