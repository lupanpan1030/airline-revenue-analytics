{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "style"
    ]
   },
   "outputs": [],
   "source": [
    "from airline_revenue_analytics.viz.charts import apply_style, PLOT_COLORS\n",
    "apply_style()\n",
    "PASS_COLOR = \"#D9F2E6\"\n",
    "FAIL_COLOR = \"#FCE4E4\"\n",
    "NEG_BG_COLOR = FAIL_COLOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2011cba5e4b2aabe6b48323dc18f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:43.706382Z",
     "iopub.status.busy": "2026-01-11T10:13:43.706285Z",
     "iopub.status.idle": "2026-01-11T10:13:43.715757Z",
     "shell.execute_reply": "2026-01-11T10:13:43.715302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook setup (segment pipeline) / Notebook 初始化（segment 管线）\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from airline_revenue_analytics.config import get_paths\n",
    "except ModuleNotFoundError as exc:\n",
    "    raise ModuleNotFoundError(\"Install the package first: pip install -e .\") from exc\n",
    "\n",
    "# Resolve repo paths and DB location / 解析仓库路径与数据库位置\n",
    "paths = get_paths(\"segment\")\n",
    "REPO_ROOT = paths.repo_root\n",
    "DATA_DIR = paths.data_raw\n",
    "OUT_DIR = paths.outputs_root\n",
    "FIG_DIR = paths.figures\n",
    "TAB_DIR = paths.tables\n",
    "DB_PATH = paths.db_path\n",
    "db_path = DB_PATH\n",
    "\n",
    "# SQLite connection (shared across cells) / SQLite 连接（全局复用）\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "def _rel(p: Path) -> str:\n",
    "    \"\"\"Render repo-relative paths for display / 将路径显示为仓库相对路径.\"\"\"\n",
    "    try:\n",
    "        return str(Path(p).resolve().relative_to(REPO_ROOT))\n",
    "    except Exception:\n",
    "        return str(p)\n",
    "\n",
    "def find_path(filename: str) -> Path:\n",
    "    \"\"\"Locate a file under outputs/ or data/raw / 在 outputs/ 或 data/raw 中定位文件.\"\"\"\n",
    "    for p in (OUT_DIR / filename, DATA_DIR / filename, REPO_ROOT / filename):\n",
    "        if p.exists():\n",
    "            return p\n",
    "    for root, _, files in os.walk(OUT_DIR):\n",
    "        if filename in files:\n",
    "            return Path(root) / filename\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find {filename}. Put it under data/raw or outputs/segment.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aac9f3-e9d4-4e5d-a9c0-51971d894e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:43.717078Z",
     "iopub.status.busy": "2026-01-11T10:13:43.716955Z",
     "iopub.status.idle": "2026-01-11T10:13:44.450273Z",
     "shell.execute_reply": "2026-01-11T10:13:44.450017Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "before = {'flights': 33121, 'ticket_flights': 1045726, 'tickets': 366732, 'bookings': 262788}\n",
    "after = {'flights': 32734, 'ticket_flights': 1033331, 'tickets': 362470, 'bookings': 261888}\n",
    "\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_axis_off()\n",
    "\n",
    "def box(x,y,w,h,text):\n",
    "    rect = FancyBboxPatch((x,y), w,h, boxstyle=\"round,pad=0.02,rounding_size=0.02\", linewidth=1)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x+w/2, y+h/2, text, ha='center', va='center')\n",
    "    return (x,y,w,h)\n",
    "\n",
    "# -- Use the before/after numbers calculated in the previous section / -- 使用上一节计算的前后对比数值\n",
    "f_raw, tf_raw, t_raw, b_raw = before['flights'], before['ticket_flights'], before['tickets'], before['bookings']\n",
    "f_sel, tf_sel, t_sel, b_sel = after['flights'], after['ticket_flights'], after['tickets'], after['bookings']\n",
    "\n",
    "# -- Positioning / -- 布局定位\n",
    "raw_f  = box(0.08, 0.76, 0.36, 0.14, f\"Flights (raw)\\n{f_raw:,}\")\n",
    "raw_tf = box(0.56, 0.76, 0.36, 0.14, f\"Ticket_flights (raw)\\n{tf_raw:,}\")\n",
    "raw_t  = box(0.08, 0.56, 0.36, 0.14, f\"Tickets (raw)\\n{t_raw:,}\")\n",
    "raw_b  = box(0.56, 0.56, 0.36, 0.14, f\"Bookings (raw)\\n{b_raw:,}\")\n",
    "\n",
    "step1  = box(0.08, 0.36, 0.36, 0.14, f\"Filter flights\\nstatus='ARRIVED'\\n→ {f_sel:,}\")\n",
    "step2  = box(0.56, 0.36, 0.36, 0.14, f\"Join: tf × flights\\n(retain segments)\\n→ {tf_sel:,}\")\n",
    "step3  = box(0.08, 0.18, 0.36, 0.14, f\"Keep tickets linked\\n→ {t_sel:,}\")\n",
    "step4  = box(0.56, 0.18, 0.36, 0.14, f\"Keep bookings linked\\n→ {b_sel:,}\")\n",
    "final  = box(0.32, 0.02, 0.36, 0.14, f\"Selected dataset\\n(segments level)\\nrows = {tf_sel:,}\")\n",
    "\n",
    "def connect(p1, p2, rad=0.0):\n",
    "    x1,y1,w1,h1 = p1\n",
    "    x2,y2,w2,h2 = p2\n",
    "    start = (x1+w1/2, y1)      # bottom center\n",
    "    end   = (x2+w2/2, y2+h2)   # top center\n",
    "    ax.add_patch(FancyArrowPatch(start, end, arrowstyle='->',\n",
    "                                 mutation_scale=14, linewidth=1.2,\n",
    "                                 connectionstyle=f\"arc3,rad={rad}\"))\n",
    "\n",
    "# Direct vertical connections / 直接垂直连接\n",
    "connect(raw_f, step1, 0.0)\n",
    "connect(raw_tf, step2, 0.0)\n",
    "connect(raw_t, step3, 0.0)\n",
    "connect(raw_b, step4, 0.0)\n",
    "# Use arcs for cross-connections to avoid overlap / 交叉连接用弧线避免重叠\n",
    "connect(step1, step2, rad=-0.25)\n",
    "connect(step2, final, rad=0.0)\n",
    "connect(step3, final, rad=0.20)\n",
    "connect(step4, final, rad=-0.20)\n",
    "\n",
    "plt.title(\"Figure 3.1.1 Data Selection Flowchart\", pad=18)\n",
    "out = FIG_DIR/\"figure_3_1_1_selection_flow.png\"\n",
    "fig.savefig(out, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved PNG:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31020a-ff9a-4149-b702-48a7a10df0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:44.451802Z",
     "iopub.status.busy": "2026-01-11T10:13:44.451635Z",
     "iopub.status.idle": "2026-01-11T10:13:45.037199Z",
     "shell.execute_reply": "2026-01-11T10:13:45.036968Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure segment-level feature table is available for downstream cells / 确保航段级特征表可用于下游单元\n",
    "if \"seg\" not in globals():\n",
    "    X_path = find_path(\"X_features.parquet\")\n",
    "    y_path = find_path(\"y_target.parquet\")\n",
    "    seg = pd.read_parquet(X_path)\n",
    "    y_seg = pd.read_parquet(y_path).squeeze()\n",
    "    seg[\"amount\"] = y_seg.values\n",
    "\n",
    "# Defaults if not defined earlier / 若未提前定义则使用默认值\n",
    "SEAT_THRESHOLD = globals().get(\"SEAT_THRESHOLD\", 240)\n",
    "WIDEBODY_KEYWORDS = globals().get(\"WIDEBODY_KEYWORDS\", [\"777\",\"787\",\"747\",\"767\",\"a330\",\"a340\",\"a350\",\"a380\",\"il-96\"])\n",
    "\n",
    "\n",
    "def show_table(df: pd.DataFrame, title: str, csv_filename: str,\n",
    "               thousands_cols=None, percent_cols=None, hide_index=True):\n",
    "    thousands_cols = thousands_cols or []\n",
    "    percent_cols   = percent_cols or []\n",
    "    t = df.copy()\n",
    "    for c in thousands_cols:\n",
    "        if c in t.columns:\n",
    "            t[c] = t[c].map(lambda x: f\"{int(x):,}\" if pd.notna(x) else \"\")\n",
    "    for c in percent_cols:\n",
    "        if c in t.columns:\n",
    "            t[c] = t[c].map(lambda v: f\"{v:.1f}%\" if pd.notna(v) else \"\")\n",
    "    out_csv = TAB_DIR/csv_filename\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved CSV -> {out_csv}\")\n",
    "    try:\n",
    "        sty = (t.style\n",
    "               .set_table_styles([{'selector':'th','props':'font-weight:bold; text-align:center;'},\n",
    "                                  {'selector':'td','props':'text-align:center;'}])\n",
    "               .set_properties(**{'text-align':'center'}))\n",
    "        if hasattr(sty, \"hide_index\"):\n",
    "            sty = sty.hide_index()\n",
    "        else:\n",
    "            sty = sty.hide(axis=\"index\")\n",
    "        print(title)\n",
    "        display(sty)\n",
    "    except Exception:\n",
    "        display(t)\n",
    "\n",
    "feature_dict = [\n",
    " # Name, definition/rule, type, granularity, business motivation / 名称, 定义/规则, 类型, 分析粒度, 业务动机\n",
    " [\"Route_Code\",\n",
    "  \"Concatenate departure_airport and arrival_airport (e.g., DME-KHV).\",\n",
    "  \"string\",\"segment (ticket_flight × flight)\",\n",
    "  \"Capture route-level effects for pricing and revenue.\"],\n",
    "\n",
    " [\"Sched_Flight_Duration_Minutes\",\n",
    "  \"UTC(scheduled_arrival) − UTC(scheduled_departure) in minutes; negatives set to NaN.\",\n",
    "  \"float\",\"segment\",\n",
    "  \"Proxy for distance/service level; strong price driver.\"],\n",
    "\n",
    " [\"Booking_Lead_Time_Days\",\n",
    "  \"UTC(dep) − UTC(book_date) in days; negatives set to NaN per 3.2 policy.\",\n",
    "  \"float\",\"segment\",\n",
    "  \"Captures demand timing and the early-bird premium phenomenon.\"],\n",
    "\n",
    " [\"Departure_DOW\",\"Day of week of UTC(dep) [0=Mon].\",\"int\",\"segment\",\n",
    "  \"Weekly patterns without requiring long-span seasonality.\"],\n",
    "\n",
    " [\"Departure_Hour\",\"Hour of day of UTC(dep) [0–23].\",\"int\",\"segment\",\n",
    "  \"Intra-day pricing and operational patterns.\"],\n",
    "\n",
    " [\"Is_Weekend\",\"1 if DOW ∈ {Sat,Sun}, else 0.\",\"int (0/1)\",\"segment\",\n",
    "  \"Captures leisure/business mix by calendar.\"],\n",
    "\n",
    " [\"Fare_Class\",\"Cleaned fare_conditions ∈ {Economy, Comfort, Business}.\",\n",
    "  \"category\",\"segment\",\"Primary driver of per-segment price.\"],\n",
    "\n",
    " [\"Fare_Class_Ordinal\",\"Map Economy=0, Comfort=1, Business=2.\",\"int\",\"segment\",\n",
    "  \"Order-aware representation for linear models.\"],\n",
    "\n",
    " [\"Is_Premium_Cabin\",\"1 if Fare_Class ∈ {Comfort,Business}, else 0.\",\n",
    "  \"int (0/1)\",\"segment\",\"Binary simplification for rule cards and trees.\"],\n",
    "\n",
    " [\"Aircraft_Model_EN\",\"English label parsed from JSON model.\",\"string\",\"segment\",\n",
    "  \"Human-readable aircraft feature for interpretation.\"],\n",
    "\n",
    " [\"Seats_Per_Aircraft\",\"Count seats per aircraft_code from seats table.\",\n",
    "  \"int\",\"segment\",\"Capacity proxy; helps explain cabin mix & price.\"],\n",
    "\n",
    " [\"Is_Widebody\",\n",
    "  f\"1 if Seats_Per_Aircraft ≥ {SEAT_THRESHOLD} OR Aircraft_Model_EN contains any of {WIDEBODY_KEYWORDS}.\",\n",
    "  \"int (0/1)\",\"segment\",\"Flag widebody operations affecting price & perception.\"],\n",
    "\n",
    " [\"Is_LongHaul\",\"1 if Sched_Flight_Duration_Minutes ≥ 240, else 0.\",\n",
    "  \"int (0/1)\",\"segment\",\"Separates long vs short haul for pattern search.\"]\n",
    "]\n",
    "\n",
    "tbl_331 = pd.DataFrame(feature_dict, columns=[\n",
    "    \"Feature Name\",\"Definition / Rule\",\"Data Type\",\"Analytical Grain\",\"Rationale\"\n",
    "])\n",
    "show_table(tbl_331, \"Table 3.3.1 – Feature Dictionary\",\n",
    "           \"table_3_3_1_feature_dictionary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c9c34-c3e5-48ab-b697-76dac6b235c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:45.038660Z",
     "iopub.status.busy": "2026-01-11T10:13:45.038515Z",
     "iopub.status.idle": "2026-01-11T10:13:45.067351Z",
     "shell.execute_reply": "2026-01-11T10:13:45.067097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Type checks / 类型检查\n",
    "print(\"Type Check:\",\n",
    "      isinstance(seg['Sched_Flight_Duration_Minutes'].dropna().iloc[0], (int,float,np.floating)),\n",
    "      isinstance(seg['Booking_Lead_Time_Days'].dropna().iloc[0], (int,float,np.floating)),\n",
    "      seg['Is_Premium_Cabin'].dropna().isin([0,1]).all(),\n",
    "      seg['Is_Widebody'].dropna().isin([0,1]).all())\n",
    "\n",
    "# Range checks / 取值范围检查\n",
    "neg_dur = int((seg['Sched_Flight_Duration_Minutes'] < 0).sum(skipna=True))\n",
    "neg_lead = int((seg['Booking_Lead_Time_Days'] < 0).sum(skipna=True))\n",
    "print(f\"Range Check: negatives – duration={neg_dur}, lead_time={neg_lead} (both expected 0 after cleaning)\")\n",
    "\n",
    "# Logic check: premium cabin 比例 ≈ Business/Comfort 占比\n",
    "p1 = seg['Is_Premium_Cabin'].mean()\n",
    "p2 = seg['Fare_Class'].isin(['Business','Comfort']).mean()\n",
    "print(f\"Logic Check: Is_Premium_Cabin share = {p1:.4f} | \"\n",
    "      f\"share(Business/Comfort) = {p2:.4f} | Δ={abs(p1-p2):.6f} (expect ~0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a97bb2-0334-4229-9261-c0285a978e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:45.068667Z",
     "iopub.status.busy": "2026-01-11T10:13:45.068580Z",
     "iopub.status.idle": "2026-01-11T10:13:46.408285Z",
     "shell.execute_reply": "2026-01-11T10:13:46.407889Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "preview_cols = ['ticket_no','Route_Code','Fare_Class','Is_Premium_Cabin',\n",
    "                'Sched_Flight_Duration_Minutes','Booking_Lead_Time_Days',\n",
    "                'Departure_DOW','Departure_Hour','Is_Weekend',\n",
    "                'Aircraft_Model_EN','Seats_Per_Aircraft','Is_Widebody','amount']\n",
    "preview_df = seg[preview_cols].head(12).copy()\n",
    "\n",
    "fig_h = 1.2 + 0.35*len(preview_df)\n",
    "fig, ax = plt.subplots(figsize=(16, fig_h))\n",
    "ax.axis('off')\n",
    "tbl = ax.table(cellText=preview_df.values,\n",
    "               colLabels=preview_df.columns, loc='center')\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(8)\n",
    "tbl.scale(1, 1.2)\n",
    "plt.title(\"Figure 3.3.1 Preview of Constructed Features\", pad=10)\n",
    "plt.tight_layout()\n",
    "png1 = FIG_DIR/\"figure_3_3_1_feature_preview.png\"\n",
    "plt.savefig(png1, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", png1)\n",
    "\n",
    "# A) Flight duration (minutes) / A）飞行时长（分钟）\n",
    "dur = seg['Sched_Flight_Duration_Minutes'].astype(float)\n",
    "q99 = np.nanpercentile(dur, 99) if dur.notna().any() else 0\n",
    "dur_clip = dur.clip(upper=q99)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(dur_clip.dropna(), bins=40)\n",
    "plt.title(\"Figure 3.3.2a Sched_Flight_Duration_Minutes (clipped at 99th pct)\")\n",
    "plt.xlabel(\"Minutes\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "png2a = FIG_DIR/\"figure_3_3_2a_duration_hist.png\"\n",
    "plt.savefig(png2a, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", png2a)\n",
    "\n",
    "# B) Booking lead time (days) / B）订票提前期（天）\n",
    "lead = seg['Booking_Lead_Time_Days'].astype(float)\n",
    "q99b = np.nanpercentile(lead, 99) if lead.notna().any() else 0\n",
    "lead_clip = lead.clip(lower=0, upper=q99b)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(lead_clip.dropna(), bins=40)\n",
    "plt.title(\"Figure 3.3.2b Booking_Lead_Time_Days (clipped at 99th pct)\")\n",
    "plt.xlabel(\"Days\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "png2b = FIG_DIR/\"figure_3_3_2b_leadtime_hist.png\"\n",
    "plt.savefig(png2b, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", png2b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae99ea2-800d-411e-bc4c-a047d890baae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:46.409943Z",
     "iopub.status.busy": "2026-01-11T10:13:46.409726Z",
     "iopub.status.idle": "2026-01-11T10:13:51.658363Z",
     "shell.execute_reply": "2026-01-11T10:13:51.658030Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, sqlite3\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    conn\n",
    "except NameError:\n",
    "    import glob\n",
    "    cands = glob.glob(str(DATA_DIR / \"*.sqlite\"))+glob.glob(str(DATA_DIR / \"*.db\"))+glob.glob(str(DATA_DIR / \"*.sqlite3\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"No SQLite found under data/. Please place airlines_db.sqlite there.\")\n",
    "\n",
    "def to_utc(x): return pd.to_datetime(x, errors='coerce', utc=True)\n",
    "\n",
    "def get_json_en(sr: pd.Series):\n",
    "    def _x(s):\n",
    "        if pd.isna(s) or s == '\\\\N': return pd.NA\n",
    "        try: return json.loads(s).get('en', pd.NA)\n",
    "        except Exception: return pd.NA\n",
    "    return sr.apply(_x)\n",
    "\n",
    "def show_table(df: pd.DataFrame, title: str, csv_filename: str):\n",
    "    out_csv = TAB_DIR/csv_filename\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved CSV -> {out_csv}\")\n",
    "    t = df.copy()\n",
    "    try:\n",
    "        sty = (t.style\n",
    "               .set_table_styles([{'selector':'th','props':'font-weight:bold; text-align:center;'},\n",
    "                                  {'selector':'td','props':'text-align:center;'}])\n",
    "               .set_properties(**{'text-align':'center'}))\n",
    "        # 兼容不同 Pandas 版本\n",
    "        if hasattr(sty, \"hide_index\"):\n",
    "            sty = sty.hide_index()\n",
    "        else:\n",
    "            sty = sty.hide(axis=\"index\")\n",
    "        print(title)\n",
    "        display(sty)\n",
    "    except Exception:\n",
    "        print(title)\n",
    "        display(t)\n",
    "\n",
    "# 1) Load raw tables / 1）加载原始表\n",
    "tf = pd.read_sql(\"SELECT ticket_no, flight_id, fare_conditions, amount FROM ticket_flights;\", conn)\n",
    "t  = pd.read_sql(\"SELECT ticket_no, book_ref FROM tickets;\", conn)\n",
    "b  = pd.read_sql(\"SELECT book_ref, book_date, total_amount FROM bookings;\", conn)\n",
    "fl = pd.read_sql(\"\"\"\n",
    "    SELECT flight_id, flight_no, scheduled_departure, scheduled_arrival,\n",
    "           departure_airport, arrival_airport, status, aircraft_code\n",
    "    FROM flights;\n",
    "\"\"\", conn)\n",
    "ac = pd.read_sql(\"SELECT aircraft_code, model, range FROM aircrafts_data;\", conn)\n",
    "ap = pd.read_sql(\"SELECT airport_code, airport_name, city, timezone FROM airports_data;\", conn)\n",
    "\n",
    "# before counts / 清洗前计数\n",
    "n_tf_raw = len(tf); n_fl_raw = len(fl); n_t_raw = len(t); n_b_raw = len(b)\n",
    "\n",
    "# keep arrived flights / 仅保留已到达航班\n",
    "fl_arr = fl.loc[fl['status'].str.upper()=='ARRIVED'].copy()\n",
    "n_fl_arr = len(fl_arr)\n",
    "\n",
    "# 2) Core joins (inner) with validation / 2）核心连接（内连接）与校验\n",
    "step_log = []\n",
    "\n",
    "step_log.append([\"ticket_flights (raw)\", n_tf_raw, tf.shape[1]])\n",
    "step_log.append([\"flights (ARRIVED only)\", n_fl_arr, fl_arr.shape[1]])\n",
    "\n",
    "# tf × flights (inner) many_to_one / tf × flights（内连接）多对一\n",
    "j1 = tf.merge(fl_arr, on='flight_id', how='inner', validate='many_to_one')\n",
    "step_log.append([\"join1: tf × flights (inner)\", len(j1), j1.shape[1]])\n",
    "\n",
    "# + tickets (inner) many_to_one / + tickets（内连接）多对一\n",
    "j2 = j1.merge(t, on='ticket_no', how='inner', validate='many_to_one')\n",
    "step_log.append([\"join2: + tickets (inner)\", len(j2), j2.shape[1]])\n",
    "\n",
    "# + bookings (inner) many_to_one / + bookings（内连接）多对一\n",
    "j3 = j2.merge(b, on='book_ref', how='inner', validate='many_to_one')\n",
    "step_log.append([\"join3: + bookings (inner)\", len(j3), j3.shape[1]])\n",
    "\n",
    "# 3) Lookups (left) / 3）查找表（左连接）\n",
    "# aircrafts_data → English model, left join (many_to_one) / aircrafts_data → 英文机型，左连接（多对一）\n",
    "ac2 = ac.copy()\n",
    "ac2['Aircraft_Model_EN'] = get_json_en(ac2['model'])\n",
    "j4 = j3.merge(ac2[['aircraft_code','Aircraft_Model_EN','range']], on='aircraft_code',\n",
    "              how='left', validate='many_to_one')\n",
    "step_log.append([\"join4: + aircrafts_data (left)\", len(j4), j4.shape[1]])\n",
    "\n",
    "# airports_data (dep/arr) → English name / airports_data（出发/到达）→ 英文名称\n",
    "ap2 = ap.copy()\n",
    "ap2['airport_name_en'] = get_json_en(ap2['airport_name'])\n",
    "\n",
    "dep = ap2.rename(columns={'airport_code':'departure_airport',\n",
    "                          'airport_name_en':'Departure_Airport_Name_EN',\n",
    "                          'city':'Departure_City','timezone':'Departure_TZ'})\n",
    "arr = ap2.rename(columns={'airport_code':'arrival_airport',\n",
    "                          'airport_name_en':'Arrival_Airport_Name_EN',\n",
    "                          'city':'Arrival_City','timezone':'Arrival_TZ'})\n",
    "\n",
    "j5 = j4.merge(dep[['departure_airport','Departure_Airport_Name_EN','Departure_City','Departure_TZ']],\n",
    "              on='departure_airport', how='left', validate='many_to_one')\n",
    "step_log.append([\"join5: + airports_data (dep,left)\", len(j5), j5.shape[1]])\n",
    "\n",
    "final_df = j5.merge(arr[['arrival_airport','Arrival_Airport_Name_EN','Arrival_City','Arrival_TZ']],\n",
    "                    on='arrival_airport', how='left', validate='many_to_one')\n",
    "step_log.append([\"join6: + airports_data (arr,left)\", len(final_df), final_df.shape[1]])\n",
    "\n",
    "# 4) Programmatic validation / 4）程序化校验\n",
    "# Row-count conservation / 行数守恒检查\n",
    "assert len(final_df) <= len(j1), \"Row explosion detected after lookups.\"\n",
    "\n",
    "# Primary key uniqueness / 主键唯一性\n",
    "pk_unique = (final_df[['ticket_no','flight_id']].drop_duplicates().shape[0] == len(final_df))\n",
    "assert pk_unique, \"Primary key (ticket_no, flight_id) is NOT unique.\"\n",
    "\n",
    "# Lookup coverage (expected ≈100%) / 查找覆盖率（期望≈100%）\n",
    "cov_ac   = final_df['Aircraft_Model_EN'].notna().mean()*100\n",
    "cov_dep  = final_df['Departure_Airport_Name_EN'].notna().mean()*100\n",
    "cov_arr  = final_df['Arrival_Airport_Name_EN'].notna().mean()*100\n",
    "\n",
    "print(f\"Lookup coverage → aircrafts_data: {cov_ac:.1f}% | dep-airport: {cov_dep:.1f}% | arr-airport: {cov_arr:.1f}%\")\n",
    "\n",
    "# 5) OUTPUTS FOR THE REPORT / 5）报告输出\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "ax.axis('off')\n",
    "\n",
    "# boxes (x,y,w,h) / 方框参数（x,y,w,h）\n",
    "boxes = {\n",
    "    \"Bookings\": (0.05, 0.75, 0.22, 0.12),\n",
    "    \"Tickets\":  (0.35, 0.75, 0.22, 0.12),\n",
    "    \"Ticket_flights\": (0.65, 0.75, 0.27, 0.12),\n",
    "    \"Flights (ARRIVED)\": (0.35, 0.45, 0.27, 0.12),\n",
    "    \"Aircrafts_data\": (0.05, 0.15, 0.22, 0.12),\n",
    "    \"Airports_data (dep)\": (0.40, 0.15, 0.27, 0.12),\n",
    "    \"Airports_data (arr)\": (0.72, 0.15, 0.27, 0.12)\n",
    "}\n",
    "for k,(x,y,w,h) in boxes.items():\n",
    "    ax.add_patch(patches.FancyBboxPatch((x,y), w,h, boxstyle=\"round,pad=0.02\", fill=False))\n",
    "    ax.text(x+w/2, y+h/2, k, ha='center', va='center', fontsize=11)\n",
    "\n",
    "# arrows (one-to-many core, left joins) / 连接箭头（核心一对多，左连接）\n",
    "def arrow(x1,y1,x2,y2):\n",
    "    ax.annotate(\"\", xy=(x2,y2), xytext=(x1,y1),\n",
    "                arrowprops=dict(arrowstyle=\"->\", lw=1.5))\n",
    "\n",
    "# core path / 核心路径\n",
    "arrow(0.27, 0.81, 0.35, 0.81)   # bookings -> tickets\n",
    "arrow(0.57, 0.81, 0.65, 0.81)   # tickets -> tf\n",
    "arrow(0.78, 0.75, 0.51, 0.57)   # tf -> flights\n",
    "\n",
    "# lookups (left joins) / 查找表（左连接）\n",
    "arrow(0.49, 0.45, 0.16, 0.27)   # flights -> aircrafts_data\n",
    "arrow(0.49, 0.45, 0.53, 0.27)   # flights -> airports dep\n",
    "arrow(0.62, 0.45, 0.86, 0.27)   # flights -> airports arr\n",
    "\n",
    "ax.set_title(\"Figure 3.4.1 Entity–Relationship Diagram for Integration\", pad=10)\n",
    "plt.tight_layout()\n",
    "f1 = FIG_DIR/\"figure_3_4_1_ER.png\"\n",
    "plt.savefig(f1, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", f1)\n",
    "\n",
    "tbl = pd.DataFrame(step_log, columns=[\"Step\",\"Rows\",\"Cols\"])\n",
    "show_table(tbl, \"Table 3.4.1 – Integration Step Row Counts\",\n",
    "           \"table_3_4_1_integration_steps.csv\")\n",
    "\n",
    "buf = StringIO()\n",
    "final_df.info(buf=buf)\n",
    "txt = buf.getvalue()\n",
    "\n",
    "plt.figure(figsize=(13, min(10, 2 + 0.22*len(final_df.columns))))\n",
    "plt.axis('off')\n",
    "plt.text(0.01, 0.99, txt, va='top', family='monospace')\n",
    "plt.title(\"Figure 3.4.2 Schema of the Final Integrated Table\", pad=10)\n",
    "plt.tight_layout()\n",
    "f2 = FIG_DIR/\"figure_3_4_2_schema.png\"\n",
    "plt.savefig(f2, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", f2)\n",
    "\n",
    "print(\"\\n— Validation summary —\")\n",
    "print(f\"Rows(tf raw) = {n_tf_raw:,} | Rows(flights raw) = {n_fl_raw:,} | Rows(ARRIVED) = {n_fl_arr:,}\")\n",
    "print(f\"Rows after tf×flights = {len(j1):,} | final_df rows = {len(final_df):,}\")\n",
    "print(f\"Primary key (ticket_no, flight_id) unique? {pk_unique}\")\n",
    "print(f\"Lookup coverage: aircraft_model_en={cov_ac:.1f}%, dep_airport_name_en={cov_dep:.1f}%, arr_airport_name_en={cov_arr:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05abf6-510f-42e5-9490-babc7cf1ba42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:51.660016Z",
     "iopub.status.busy": "2026-01-11T10:13:51.659916Z",
     "iopub.status.idle": "2026-01-11T10:13:54.620485Z",
     "shell.execute_reply": "2026-01-11T10:13:54.618906Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def to_utc(s):\n",
    "    return pd.to_datetime(s, errors='coerce', utc=True)\n",
    "\n",
    "def normalise_fare(x:str):\n",
    "    if pd.isna(x): return pd.NA\n",
    "    s = str(x).strip().title()\n",
    "    mapping = {\n",
    "        'Biz':'Business','B':'Business',\n",
    "        'Premium Economy':'Comfort','Premium':'Comfort','Prem Eco':'Comfort',\n",
    "        'Eco':'Economy','E':'Economy'\n",
    "    }\n",
    "    return mapping.get(s, s)  # 数据本身大多已是 Business/Comfort/Economy\n",
    "\n",
    "def show_table(df: pd.DataFrame, title: str, csv_name: str):\n",
    "    csv_path = TAB_DIR/csv_name\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved CSV -> {_rel(csv_path)}\")\n",
    "    try:\n",
    "        sty = (df.style\n",
    "               .set_table_styles([{'selector':'th','props':'font-weight:bold; text-align:center;'},\n",
    "                                  {'selector':'td','props':'text-align:center;'}])\n",
    "               .set_properties(**{'text-align':'center'}))\n",
    "        # 兼容不同 pandas 版本\n",
    "        if hasattr(sty, \"hide_index\"):\n",
    "            sty = sty.hide_index()\n",
    "        else:\n",
    "            sty = sty.hide(axis=\"index\")\n",
    "        display(sty.set_caption(title))\n",
    "    except Exception:\n",
    "        print(title)\n",
    "        display(df)\n",
    "\n",
    "try:\n",
    "    _ = final_df.shape\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"未检测到 final_df。请先运行 3.4 的整合代码以生成 final_df DataFrame。\") from e\n",
    "\n",
    "# 3.5.1 Normalize types & build light features (formatting only) / 3.5.1 规范类型 & 构建轻量特征（仅格式化所需）\n",
    "df = final_df.copy()\n",
    "\n",
    "# 统一时间为 UTC、并加强常用字段\n",
    "df['book_date_utc']          = to_utc(df.get('book_date'))\n",
    "df['sched_departure_utc']    = to_utc(df.get('scheduled_departure'))\n",
    "df['sched_arrival_utc']      = to_utc(df.get('scheduled_arrival'))\n",
    "\n",
    "# Route code (departure-arrival) / 路线编码（出发-到达）\n",
    "df['route_code'] = (df['departure_airport'].astype(str) + '-' +\n",
    "                    df['arrival_airport'].astype(str))\n",
    "\n",
    "# Cabin normalization & categorization / 舱位归一化、类别化\n",
    "df['fare_conditions'] = df['fare_conditions'].map(normalise_fare)\n",
    "df['fare_conditions'] = pd.Categorical(df['fare_conditions'],\n",
    "                                       categories=['Economy','Comfort','Business'],\n",
    "                                       ordered=True)\n",
    "\n",
    "# Compute scheduled flight duration (min) & booking lead time (days) / 计算计划飞行时长（分钟）与订票提前期（天）\n",
    "df['sched_flight_duration_minutes'] = (\n",
    "    (df['sched_arrival_utc'] - df['sched_departure_utc'])\n",
    "    .dt.total_seconds()/60.0\n",
    ")\n",
    "df['booking_lead_time_days'] = (\n",
    "    (df['sched_departure_utc'] - df['book_date_utc'])\n",
    "    .dt.total_seconds()/(3600*24)\n",
    ")\n",
    "\n",
    "# 时间派生（用于后续 ColumnTransformer）\n",
    "df['hour_of_day']  = df['sched_departure_utc'].dt.hour\n",
    "df['day_of_week']  = df['sched_departure_utc'].dt.dayofweek  # Monday=0\n",
    "df['is_weekend']   = df['day_of_week'].isin([5,6]).astype('int8')\n",
    "\n",
    "# Cabin numeric index & premium flag / 舱位数值索引与是否高端舱\n",
    "cabin_map = {'Economy':0, 'Comfort':1, 'Business':2}\n",
    "df['cabin_index']      = df['fare_conditions'].map(cabin_map).astype('Int8')\n",
    "df['is_premium_cabin'] = (df['cabin_index']>=1).astype('int8')\n",
    "\n",
    "# 3.5.2 角色分离（y & X）并持久化\n",
    "# —— 目标变量（y）：amount（segment-level ticket_flight amount）\n",
    "y = df['amount'].astype('float64')\n",
    "\n",
    "# —— 预测子集（X）：数值与分类型（供后续 ColumnTransformer）\n",
    "X_num_cols = [\n",
    "    'sched_flight_duration_minutes','booking_lead_time_days',\n",
    "    'cabin_index','is_premium_cabin','hour_of_day','day_of_week'\n",
    "]\n",
    "X_cat_cols = [\n",
    "    'fare_conditions','route_code','Aircraft_Model_EN'\n",
    "]\n",
    "# 确保类别 dtype\n",
    "for c in X_cat_cols:\n",
    "    df[c] = df[c].astype('category')\n",
    "\n",
    "X = df[X_num_cols + X_cat_cols].copy()\n",
    "\n",
    "# —— 输出到 Parquet（若无 pyarrow 自动 CSV 兜底）\n",
    "def _save_parquet_or_csv(frame, path_parquet, path_csv):\n",
    "    try:\n",
    "        frame.to_parquet(path_parquet, index=False)\n",
    "        print(f\"Saved Parquet -> {_rel(path_parquet)}\")\n",
    "    except Exception as e:\n",
    "        frame.to_csv(path_csv, index=False)\n",
    "        print(f\"[pyarrow/fastparquet 不可用，改存 CSV] -> {_rel(path_csv)}  ({e.__class__.__name__}: {e})\")\n",
    "\n",
    "_save_parquet_or_csv(df, OUT_DIR/\"master_formatted.parquet\", OUT_DIR/\"master_formatted.csv\")\n",
    "_save_parquet_or_csv(X,  OUT_DIR/\"X_features.parquet\",      OUT_DIR/\"X_features.csv\")\n",
    "_save_parquet_or_csv(y.to_frame('amount'),\n",
    "                     OUT_DIR/\"y_target.parquet\",\n",
    "                     OUT_DIR/\"y_target.csv\")\n",
    "\n",
    "print(\"\\nRoles → y shape:\", y.shape, \"| X shape:\", X.shape)\n",
    "print(\"Numeric predictors:\", X_num_cols)\n",
    "print(\"Categorical predictors:\", X_cat_cols)\n",
    "\n",
    "# 3.5.3 Figure 3.5.1 —— 架构截图（df.info）\n",
    "buf = StringIO()\n",
    "df.info(buf=buf)\n",
    "schema_txt = buf.getvalue()\n",
    "\n",
    "plt.figure(figsize=(13, min(10, 2 + 0.22*len(df.columns))))\n",
    "plt.axis('off')\n",
    "plt.text(0.01, 0.99, schema_txt, va='top', family='monospace')\n",
    "plt.title(\"Figure 3.5.1 Schema of Formatted Data\", pad=10)\n",
    "plt.tight_layout()\n",
    "fig_path = FIG_DIR/\"figure_3_5_1_schema.png\"\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", _rel(fig_path))\n",
    "\n",
    "# 3.5.4 Table 3.5.1 —— 舱位标签归一化结果\n",
    "fare_tbl = (df['fare_conditions']\n",
    "            .value_counts(dropna=False)\n",
    "            .rename_axis('fare_conditions')\n",
    "            .reset_index(name='rows'))\n",
    "show_table(fare_tbl, \"Table 3.5.1 — Label Normalisation for fare_conditions\",\n",
    "           \"table_3_5_1_fare_counts.csv\")\n",
    "\n",
    "# 3.5.5 Report-ready confirmation sentence / 3.5.5 供报告粘贴的确认语句\n",
    "TARGET_TRANSFORM = \"log1p\"   # 可选：\"log1p\" 或 \"yeo-johnson\"\n",
    "print(\"\\nReport snippet:\")\n",
    "print(f\"- Target (y): amount. Chosen transform for Step 4.2 = {TARGET_TRANSFORM} (ln(1+x)).\")\n",
    "print(\"- Files saved under outputs/: master_formatted.parquet, X_features.parquet, y_target.parquet.\")\n",
    "print(\"- Booking-level files: not required for current plan; segment-level is the analytical grain.\")\n",
    "\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "assert 'X' in globals() and 'y' in globals(), \"请先在上文生成 X 和 y 对象。\"\n",
    "\n",
    "y_s = pd.Series(y).astype(float)\n",
    "# Optional: log1p transform for analysis only; saved separately to avoid overwriting y_target.parquet\n",
    "if (y_s < 0).any():\n",
    "    raise ValueError(\"y 含负数，无法进行 log1p。请确认 y 是否为 segment-level amount。\")\n",
    "y_log = np.log1p(y_s).rename(\"log1p_amount\")\n",
    "\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Persist to disk / 落盘\n",
    "pd.DataFrame(y_log).to_parquet(OUT_DIR/\"y_target_log.parquet\")\n",
    "pd.DataFrame(X).to_parquet(OUT_DIR/\"X_features.parquet\")\n",
    "\n",
    "print(\"Saved -> X_features.parquet:\", pd.read_parquet(OUT_DIR/\"X_features.parquet\").shape)\n",
    "print(\"Saved -> y_target_log.parquet  :\", pd.read_parquet(OUT_DIR/\"y_target_log.parquet\").shape, list(pd.read_parquet(OUT_DIR/\"y_target_log.parquet\").columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfc07b-d66b-4c0e-a699-26777ea5935b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:54.622687Z",
     "iopub.status.busy": "2026-01-11T10:13:54.622563Z",
     "iopub.status.idle": "2026-01-11T10:13:54.648175Z",
     "shell.execute_reply": "2026-01-11T10:13:54.647910Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "X_path = find_path(\"X_features.parquet\")\n",
    "y_path = find_path(\"y_target.parquet\")\n",
    "\n",
    "X = pd.read_parquet(X_path)\n",
    "y = pd.read_parquet(y_path).squeeze()\n",
    "y.name = y.name or \"y\"\n",
    "\n",
    "# 丢弃任何仍然是 datetime 的列；布尔→0/1（避免前面你遇到的 dtype 报错）\n",
    "dt_cols = [c for c in X.columns if is_datetime64_any_dtype(X[c])]\n",
    "if dt_cols:\n",
    "    X = X.drop(columns=dt_cols)\n",
    "\n",
    "for c in X.select_dtypes(include=\"bool\").columns:\n",
    "    X[c] = X[c].astype(int)\n",
    "\n",
    "num_cols = list(X.select_dtypes(include=[\"number\"]).columns)\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(f\"X shape = {X.shape}; y shape = {y.shape}\")\n",
    "print(f\"Numeric cols: {len(num_cols)}, Categorical cols: {len(cat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751afc11-161e-4d04-beea-e5441f9edd2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:54.649723Z",
     "iopub.status.busy": "2026-01-11T10:13:54.649607Z",
     "iopub.status.idle": "2026-01-11T10:13:56.621262Z",
     "shell.execute_reply": "2026-01-11T10:13:56.620882Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.30\n",
    "Q = 10  # use deciles for stratification\n",
    "\n",
    "# Load target / 加载目标变量\n",
    "ydf = pd.read_parquet(OUT_DIR/\"y_target.parquet\")\n",
    "assert \"amount\" in ydf.columns, \"y_target.parquet must include amount\"\n",
    "y_raw = pd.to_numeric(ydf[\"amount\"], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).dropna()\n",
    "index_all = y_raw.index\n",
    "\n",
    "# Stratify on log1p(amount) to reduce skewness\n",
    "if (y_raw < 0).any():\n",
    "    raise ValueError(\"amount contains negative values; cannot apply log1p.\")\n",
    "y = np.log1p(y_raw)\n",
    "\n",
    "# Stratified split by quantiles (70/30) / 按分位分层切分（70/30）\n",
    "bins = pd.qcut(y, q=Q, labels=False, duplicates=\"drop\")\n",
    "idx_train, idx_test = train_test_split(index_all, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=bins)\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n",
    "\n",
    "# Persist split indices / 持久化切分索引\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pd.Series(idx_train, name=\"train_index\").to_csv(ART_DIR/\"train_index.csv\", index=False)\n",
    "pd.Series(idx_test,  name=\"test_index\").to_csv(ART_DIR/\"test_index.csv\", index=False)\n",
    "\n",
    "# Histogram overlay / 直方图叠加\n",
    "def fd_bins(a):\n",
    "    a = np.asarray(a)\n",
    "    a = a[np.isfinite(a)]\n",
    "    q75, q25 = np.percentile(a, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    n = len(a)\n",
    "    if iqr == 0 or n <= 1:\n",
    "        return 30\n",
    "    h = 2 * iqr * (n ** (-1/3))\n",
    "    return max(15, min(60, int(np.ceil((a.max() - a.min()) / h))))\n",
    "\n",
    "bins_n = fd_bins(y)\n",
    "hist_range = (min(y_train.min(), y_test.min()), max(y_train.max(), y_test.max()))\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.hist(y_train, bins=bins_n, range=hist_range, density=True, alpha=0.6, label=f\"Train (n={len(y_train)})\")\n",
    "plt.hist(y_test,  bins=bins_n, range=hist_range, density=True, alpha=0.6, label=f\"Test  (n={len(y_test)})\")\n",
    "plt.xlabel(\"log1p(amount)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of log1p(amount) in Train vs Test (Stratified by Quantiles)\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.legend()\n",
    "\n",
    "# QC annotation: max decile diff / 质量标注：最大分位差\n",
    "train_q = y_train.quantile(np.linspace(0, 1, 11))\n",
    "test_q = y_test.quantile(np.linspace(0, 1, 11))\n",
    "max_abs = (train_q - test_q).abs().max()\n",
    "plt.text(0.02, 0.96, f\"Max decile diff = {max_abs:.3f}\", transform=plt.gca().transAxes, va=\"top\")\n",
    "\n",
    "# Save figures / 保存图表\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "base = FIG_DIR/\"Fig4_3_1_StratifiedSplit_TargetDist_v1\"\n",
    "plt.savefig(base.with_suffix(\".png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(base.with_suffix(\".svg\"), bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved figure ->\", _rel(base.with_suffix(\".png\")), \"and\", _rel(base.with_suffix(\".svg\")))\n",
    "print(\"Indices saved ->\", _rel(ART_DIR/\"train_index.csv\"), \"and\", _rel(ART_DIR/\"test_index.csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
