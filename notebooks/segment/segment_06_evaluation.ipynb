{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "style"
    ]
   },
   "outputs": [],
   "source": [
    "from airline_revenue_analytics.viz.charts import apply_style, PLOT_COLORS\n",
    "apply_style()\n",
    "PASS_COLOR = \"#D9F2E6\"\n",
    "FAIL_COLOR = \"#FCE4E4\"\n",
    "NEG_BG_COLOR = FAIL_COLOR\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Notebook setup (segment pipeline) / Notebook \u521d\u59cb\u5316\uff08segment \u7ba1\u7ebf\uff09\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from airline_revenue_analytics.config import get_paths\n",
    "except ModuleNotFoundError as exc:\n",
    "    raise ModuleNotFoundError(\"Install the package first: pip install -e .\") from exc\n",
    "\n",
    "# Resolve repo paths and DB location / \u89e3\u6790\u4ed3\u5e93\u8def\u5f84\u4e0e\u6570\u636e\u5e93\u4f4d\u7f6e\n",
    "paths = get_paths(\"segment\")\n",
    "REPO_ROOT = paths.repo_root\n",
    "DATA_DIR = paths.data_raw\n",
    "OUT_DIR = paths.outputs_root\n",
    "FIG_DIR = paths.figures\n",
    "TAB_DIR = paths.tables\n",
    "DB_PATH = paths.db_path\n",
    "db_path = DB_PATH\n",
    "\n",
    "# SQLite connection (shared across cells) / SQLite \u8fde\u63a5\uff08\u5168\u5c40\u590d\u7528\uff09\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "def _rel(p: Path) -> str:\n",
    "    \"\"\"Render repo-relative paths for display / \u5c06\u8def\u5f84\u663e\u793a\u4e3a\u4ed3\u5e93\u76f8\u5bf9\u8def\u5f84.\"\"\"\n",
    "    try:\n",
    "        return str(Path(p).resolve().relative_to(REPO_ROOT))\n",
    "    except Exception:\n",
    "        return str(p)\n",
    "\n",
    "def find_path(filename: str) -> Path:\n",
    "    \"\"\"Locate a file under outputs/ or data/raw / \u5728 outputs/ \u6216 data/raw \u4e2d\u5b9a\u4f4d\u6587\u4ef6.\"\"\"\n",
    "    for p in (OUT_DIR / filename, DATA_DIR / filename, REPO_ROOT / filename):\n",
    "        if p.exists():\n",
    "            return p\n",
    "    for root, _, files in os.walk(OUT_DIR):\n",
    "        if filename in files:\n",
    "            return Path(root) / filename\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find {filename}. Put it under data/raw or outputs/segment.\"\n",
    "    )\n"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "5bca2ffe14f8491db859891cd8084505"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b30d22-a018-46d0-9eb5-d6821edc2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pandas.api.types import (\n",
    "    is_numeric_dtype, is_datetime64_any_dtype, is_bool_dtype\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "X_path = find_path(\"X_features.parquet\")\n",
    "y_path = find_path(\"y_target.parquet\")\n",
    "\n",
    "X = pd.read_parquet(X_path)\n",
    "y = pd.read_parquet(y_path).squeeze()\n",
    "y.name = y.name or \"target\"\n",
    "\n",
    "# drop datetime-like columns (not directly used in modelling pipelines here) / \u5220\u9664\u7c7b\u65f6\u95f4\u5217\uff08\u6b64\u5904\u5efa\u6a21\u4e0d\u76f4\u63a5\u4f7f\u7528\uff09\n",
    "dt_cols = [c for c in X.columns if is_datetime64_any_dtype(X[c])]\n",
    "if dt_cols:\n",
    "    X = X.drop(columns=dt_cols)\n",
    "\n",
    "# cast booleans to 0/1 / \u5e03\u5c14\u503c\u8f6c\u4e3a 0/1\n",
    "for c in X.columns:\n",
    "    if is_bool_dtype(X[c]):\n",
    "        X[c] = X[c].astype(\"int64\")\n",
    "\n",
    "# numeric columns (including pandas extension dtypes) -> float64 (stable for sklearn) / \u6570\u503c\u5217\uff08\u542b\u6269\u5c55\u7c7b\u578b\uff09\u2192 float64\uff08\u4fbf\u4e8e sklearn \u7a33\u5b9a\uff09\n",
    "num_cols = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "X[num_cols] = X[num_cols].apply(pd.to_numeric, errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "# the rest treated as categorical/object / \u5176\u4f59\u5217\u89c6\u4e3a\u7c7b\u522b/\u5bf9\u8c61\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Loaded X: {X.shape}, y: {y.shape}\")\n",
    "print(f\"Train/Test: {X_train.shape} / {X_test.shape}\")\n",
    "print(f\"Numeric cols = {len(num_cols)}, Categorical cols = {len(cat_cols)}\")\n",
    "\n",
    "# OneHotEncoder API compatibility (sparse_output in >=1.2; sparse in older) / OneHotEncoder API \u517c\u5bb9\u6027\uff08>=1.2 \u7528 sparse_output\uff0c\u65e7\u7248\u7528 sparse\uff09\n",
    "def _ohe_sparse():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "def _ohe_dense():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# \u6ce8\u610f\uff1a\u7a00\u758f\u65f6 StandardScaler \u8981\u7528 with_mean=False\n",
    "preprocess_sparse = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\",  StandardScaler(with_mean=False))\n",
    "        ]), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", _ohe_sparse())\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# \u5173\u952e\uff1asparse_threshold=0 \u4fdd\u8bc1 ColumnTransformer \u8f93\u51fa dense\n",
    "preprocess_dense = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\",  StandardScaler())\n",
    "        ]), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", _ohe_dense())\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "print(\"preprocess_sparse: sparse OHE + scaler(with_mean=False)\")\n",
    "print(\"preprocess_dense : dense OHE  + scaler; ColumnTransformer forces dense output\")\n",
    "\n",
    "def eval_on_test(pipe: Pipeline, X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"\n",
    "    Fit pipeline on train, predict on test, return (R2, RMSE, MAE, y_pred).\n",
    "    Uses sqrt(MSE) to be compatible with older sklearn that lacks squared=False.\n",
    "    \"\"\"\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_pred = pipe.predict(X_te)\n",
    "    r2  = float(r2_score(y_te, y_pred))\n",
    "    mse = float(mean_squared_error(y_te, y_pred))   # version-safe\n",
    "    rmse = mse ** 0.5\n",
    "    mae  = float(mean_absolute_error(y_te, y_pred))\n",
    "    return r2, rmse, mae, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98b8f3-0462-49db-9cb7-5c1d7efc3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "# Lightweight debug: enable sampling for faster plots (e.g., 50_000) / \u8f7b\u91cf\u8c03\u8bd5\uff1a\u60f3\u5feb\u70b9\u51fa\u56fe\u53ef\u6253\u5f00\u62bd\u6837\uff08\u4f8b\u5982 50_000\uff09\n",
    "SAMPLE_FOR_TRAIN = None  # e.g., 50000\uff1bNone \u8868\u793a\u7528\u5168\u91cf\u8bad\u7ec3\u96c6\n",
    "X_tr = X_train if SAMPLE_FOR_TRAIN is None else X_train.sample(SAMPLE_FOR_TRAIN, random_state=42)\n",
    "y_tr = y_train.loc[X_tr.index]\n",
    "\n",
    "# \u68c0\u6d4b\u662f\u5426\u6709 XGBoost\uff1b\u6ca1\u6709\u5c31\u7528 HGBR\n",
    "USE_XGB = False\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    USE_XGB = True\n",
    "except Exception:\n",
    "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "candidates = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"DecisionTree\",     DecisionTreeRegressor(random_state=42)),\n",
    "    (\"RandomForest\",     RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),\n",
    "]\n",
    "\n",
    "if USE_XGB:\n",
    "    candidates.append((\"XGBRegressor\", XGBRegressor(\n",
    "        n_estimators=400, learning_rate=0.1, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "        n_jobs=-1, tree_method=\"hist\"\n",
    "    )))\n",
    "else:\n",
    "    candidates.append((\"HistGradientBoosting\", HistGradientBoostingRegressor(random_state=42)))\n",
    "\n",
    "def pick_preprocessor(model_name: str):\n",
    "    \"\"\"HGBR / XGB \u5fc5\u987b\u7528 dense\uff1b\u5176\u4f59\u7528 sparse\uff08\u66f4\u7701\u5185\u5b58\uff09\"\"\"\n",
    "    return preprocess_dense if model_name in (\"HistGradientBoosting\", \"XGBRegressor\") else preprocess_sparse\n",
    "\n",
    "# Small check (optional): confirm sparse matrix after preprocessing / \u2014\u2014 \u5c0f\u68c0\u67e5\u51fd\u6570\uff08\u53ef\u6ce8\u91ca\u6389\uff09\uff1a\u786e\u8ba4\u9884\u5904\u7406\u540e\u7684\u77e9\u9635\u662f\u5426\u7a00\u758f\n",
    "def debug_check_dense(preproc, tag=\"\"):\n",
    "    Xt = preproc.fit_transform(X_train.head(500))\n",
    "    print(f\"[DEBUG] {tag} sparse? {sparse.issparse(Xt)}  type={type(Xt)}  shape={getattr(Xt,'shape',None)}\")\n",
    "\n",
    "rows, pred_cache = [], {}\n",
    "for name, est in candidates:\n",
    "    pre = pick_preprocessor(name)\n",
    "    # \u8c03\u8bd5\u8f93\u51fa\uff1a\u786e\u8ba4 HGBR/XGB \u7528\u7684\u662f dense\n",
    "    debug_check_dense(pre, tag=name)\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"model\", est)])\n",
    "    r2, rmse, mae, y_pred = eval_on_test(pipe, X_tr, y_tr, X_test, y_test)\n",
    "    rows.append([name, r2, rmse, mae])\n",
    "    pred_cache[name] = y_pred\n",
    "    print(f\"[DONE] {name}: R2={r2:.3f}, RMSE={rmse:.1f}, MAE={mae:.1f}\")\n",
    "\n",
    "tbl_61 = (pd.DataFrame(rows, columns=[\"Model\", \"R2\", \"RMSE\", \"MAE\"])\n",
    "          .sort_values(\"R2\", ascending=False).reset_index(drop=True))\n",
    "\n",
    "# Save tables & plots / \u4fdd\u5b58\u8868\u683c & \u753b\u56fe\n",
    "tbl_61.to_csv(TAB_DIR/\"table_6_1_initial_model_test_performance.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(tbl_61[\"Model\"], tbl_61[\"R2\"])\n",
    "plt.ylabel(\"R\u00b2 on test set\"); plt.title(\"Figure 6.1 \u2014 Initial Model R\u00b2 Comparison (70/30 split)\")\n",
    "plt.xticks(rotation=20); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"figure_6_1_r2_bar.png\", dpi=220)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved:\",\n",
    "      \"\\n  tables/table_6_1_initial_model_test_performance.csv\",\n",
    "      \"\\n  figures/figure_6_1_r2_bar.png\")\n",
    "tbl_61\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef2eb3-0f78-49cd-a3c0-004596ef035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import json, pandas as pd\n",
    "\n",
    "if not USE_XGB:\n",
    "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Sample to control search time (tune to machine capacity) / \u62bd\u6837\u4ee5\u63a7\u5236\u641c\u7d22\u65f6\u95f4\uff08\u6309\u673a\u5668\u6027\u80fd\u8c03\u6574\uff09\n",
    "SAMPLE_FOR_TUNING = min(100_000, len(X_train))\n",
    "X_tune = X_train.sample(SAMPLE_FOR_TUNING, random_state=42)\n",
    "y_tune = y_train.loc[X_tune.index]\n",
    "\n",
    "grids = [\n",
    "    (\"DecisionTree\",\n",
    "     DecisionTreeRegressor(random_state=42),\n",
    "     {\"model__max_depth\":[5,10,None],\n",
    "      \"model__min_samples_leaf\":[1,5,20]}),\n",
    "\n",
    "    (\"RandomForest\",\n",
    "     RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "     {\"model__n_estimators\":[200,400],\n",
    "      \"model__max_depth\":[None,20],\n",
    "      \"model__min_samples_leaf\":[1,5]}),\n",
    "]\n",
    "\n",
    "if USE_XGB:\n",
    "    from xgboost import XGBRegressor\n",
    "    grids.append((\"XGBRegressor\",\n",
    "                  XGBRegressor(random_state=42, n_jobs=-1, tree_method=\"hist\",\n",
    "                               subsample=0.8, colsample_bytree=0.8),\n",
    "                  {\"model__n_estimators\":[300,500],\n",
    "                   \"model__max_depth\":[4,6,8],\n",
    "                   \"model__learning_rate\":[0.1,0.05]}))\n",
    "else:\n",
    "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "    grids.append((\"HistGradientBoosting\",\n",
    "                  HistGradientBoostingRegressor(random_state=42),\n",
    "                  {\"model__max_leaf_nodes\":[31,63],\n",
    "                   \"model__learning_rate\":[0.1,0.05],\n",
    "                   \"model__max_iter\":[200,400]}))\n",
    "\n",
    "tune_rows, best_pipes = [], {}\n",
    "for name, est, param_grid in grids:\n",
    "    pre = pick_preprocessor(name)      # \u5173\u952e\uff1aHGBR/XGB \u7528 dense\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"model\", est)])\n",
    "    gs = GridSearchCV(pipe, param_grid=param_grid, scoring=\"r2\", cv=5, n_jobs=-1, verbose=1)\n",
    "    gs.fit(X_tune, y_tune)\n",
    "    best_pipes[name] = gs.best_estimator_\n",
    "    tune_rows.append([name, json.dumps(gs.best_params_), float(gs.best_score_)])\n",
    "    print(f\"[BEST] {name}: {gs.best_params_}  CV R2={gs.best_score_:.3f}\")\n",
    "\n",
    "tbl_62 = (pd.DataFrame(tune_rows, columns=[\"Model\",\"Best Parameters (GridSearchCV)\",\"Mean CV R2 (5-fold)\"])\n",
    "          .sort_values(\"Mean CV R2 (5-fold)\", ascending=False).reset_index(drop=True))\n",
    "tbl_62.to_csv(TAB_DIR/\"table_6_2_gridsearch_summary.csv\", index=False)\n",
    "print(\"Saved: tables/table_6_2_gridsearch_summary.csv\")\n",
    "\n",
    "# \u6301\u4e45\u5316 best_pipes \u4ee5\u4fbf 6.3 \u4f7f\u7528\uff08Jupyter \u91cc\u540c\u4e00\u4f1a\u8bdd\u4e0b\u53ef\u76f4\u63a5\u5f15\u7528\uff09\n",
    "best_pipes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe78c15-a798-413d-bc72-42740bd42e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rows = []\n",
    "final_preds = {}\n",
    "final_pipes = {}\n",
    "\n",
    "for name, pipe in best_pipes.items():\n",
    "    r2, rmse, mae, y_pred = eval_on_test(pipe, X_train, y_train, X_test, y_test)\n",
    "    final_rows.append([name, r2, rmse, mae, pipe])\n",
    "    final_preds[name] = y_pred\n",
    "    final_pipes[name] = pipe\n",
    "\n",
    "tbl_final = pd.DataFrame(final_rows, columns=[\"Model\", \"R2\", \"RMSE\", \"MAE\", \"Pipeline\"]) \\\n",
    "             .sort_values(\"R2\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save final test-set performance / \u2014\u2014 \u4fdd\u5b58\u6700\u7ec8\u6d4b\u8bd5\u96c6\u6027\u80fd\n",
    "tbl_final_show = tbl_final[[\"Model\", \"R2\", \"RMSE\", \"MAE\"]]\n",
    "tbl_final_show.to_csv(TAB_DIR/\"table_6_3_final_test_performance.csv\", index=False)\n",
    "\n",
    "# Plot with the best model / \u2014\u2014 \u9009\u62e9\u6700\u4f73\u6a21\u578b\u4f5c\u56fe\n",
    "best_name = tbl_final.iloc[0][\"Model\"]\n",
    "best_pred = final_preds[best_name]\n",
    "\n",
    "# Figure 6.2: Predicted vs. Observed\uff08\u542b y=x \u53c2\u8003\u7ebf\uff09\n",
    "plt.figure(figsize=(6.8, 6))\n",
    "plt.scatter(y_test, best_pred, s=8, alpha=0.4)\n",
    "mn = float(min(np.min(y_test), np.min(best_pred)))\n",
    "mx = float(max(np.max(y_test), np.max(best_pred)))\n",
    "plt.plot([mn, mx], [mn, mx])\n",
    "plt.xlabel(\"Observed (y_test)\")\n",
    "plt.ylabel(f\"Predicted ({best_name})\")\n",
    "plt.title(\"Figure 6.2 \u2014 Predicted vs. Observed (Best Final Model)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"figure_6_2_pred_vs_observed.png\", dpi=220)\n",
    "plt.show()\n",
    "\n",
    "# Figure 6.3: Residuals vs. Fitted / \u56fe 6.3\uff1a\u6b8b\u5dee vs \u62df\u5408\u503c\n",
    "resid = best_pred - y_test\n",
    "plt.figure(figsize=(6.8, 5.2))\n",
    "plt.scatter(best_pred, resid, s=8, alpha=0.4)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Fitted values (Predicted)\")\n",
    "plt.ylabel(\"Residuals (Pred - Obs)\")\n",
    "plt.title(\"Figure 6.3 \u2014 Residuals vs. Fitted (Best Final Model)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"figure_6_3_residuals_vs_fitted.png\", dpi=220)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved:\",\n",
    "      \"\\n  tables/table_6_3_final_test_performance.csv\",\n",
    "      \"\\n  figures/figure_6_2_pred_vs_observed.png\",\n",
    "      \"\\n  figures/figure_6_3_residuals_vs_fitted.png\")\n",
    "tbl_final_show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce8854-8a50-48b1-ad45-2c0fe613b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATERIAL_GAIN_THRESHOLD = 0.20  # \u4f60\u53ef\u4fee\u6539\u4e3a 0.15 / 0.25 \u7b49\n",
    "\n",
    "# \u9009\u62e9\u201c\u6700\u53ef\u89e3\u91ca\u201d\u7684\u5f3a\u57fa\u7ebf\uff08\u7ebf\u6027\u56de\u5f52 OR \u51b3\u7b56\u6811\uff09\u4e0e\u201c\u6700\u5f3a\u9ed1\u76d2\u201d\uff08XGB/HGB/RF\uff09\n",
    "interpretable_mask = tbl_final_show[\"Model\"].isin([\"LinearRegression\", \"DecisionTree\"])\n",
    "blackbox_mask      = ~interpretable_mask\n",
    "\n",
    "if interpretable_mask.any() and blackbox_mask.any():\n",
    "    best_interpretable = tbl_final_show[interpretable_mask].sort_values(\"RMSE\").iloc[0]\n",
    "    best_blackbox      = tbl_final_show[blackbox_mask].sort_values(\"RMSE\").iloc[0]\n",
    "    gain = (best_interpretable[\"RMSE\"] - best_blackbox[\"RMSE\"]) / best_interpretable[\"RMSE\"]\n",
    "    decision = \"RECOMMEND DEPLOYMENT\" if gain >= MATERIAL_GAIN_THRESHOLD else \"KEEP INTERPRETABLE\"\n",
    "    print(f\"Best interpretable = {best_interpretable['Model']}  RMSE={best_interpretable['RMSE']:.3f}\")\n",
    "    print(f\"Best black-box     = {best_blackbox['Model']}      RMSE={best_blackbox['RMSE']:.3f}\")\n",
    "    print(f\"Relative RMSE reduction = {gain*100:.1f}%  \u2192  Decision: {decision}\")\n",
    "else:\n",
    "    print(\"Skip decision: need at least one interpretable and one black-box model to compare.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016ca8e-f256-4d3a-9050-0b150c43c74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
