{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "style"
    ]
   },
   "outputs": [],
   "source": [
    "from airline_revenue_analytics.viz.charts import apply_style, PLOT_COLORS\n",
    "apply_style()\n",
    "PASS_COLOR = \"#D9F2E6\"\n",
    "FAIL_COLOR = \"#FCE4E4\"\n",
    "NEG_BG_COLOR = FAIL_COLOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2222a5b957401987705ea3b159194a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:47.016519Z",
     "iopub.status.busy": "2026-01-11T10:12:47.016355Z",
     "iopub.status.idle": "2026-01-11T10:12:47.027633Z",
     "shell.execute_reply": "2026-01-11T10:12:47.027283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook setup (segment pipeline) / Notebook \u521d\u59cb\u5316\uff08segment \u7ba1\u7ebf\uff09\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from airline_revenue_analytics.config import get_paths\n",
    "except ModuleNotFoundError as exc:\n",
    "    raise ModuleNotFoundError(\"Install the package first: pip install -e .\") from exc\n",
    "\n",
    "# Resolve repo paths and DB location / \u89e3\u6790\u4ed3\u5e93\u8def\u5f84\u4e0e\u6570\u636e\u5e93\u4f4d\u7f6e\n",
    "paths = get_paths(\"segment\")\n",
    "REPO_ROOT = paths.repo_root\n",
    "DATA_DIR = paths.data_raw\n",
    "OUT_DIR = paths.outputs_root\n",
    "FIG_DIR = paths.figures\n",
    "TAB_DIR = paths.tables\n",
    "DB_PATH = paths.db_path\n",
    "db_path = DB_PATH\n",
    "\n",
    "# SQLite connection (shared across cells) / SQLite \u8fde\u63a5\uff08\u5168\u5c40\u590d\u7528\uff09\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "def _rel(p: Path) -> str:\n",
    "    \"\"\"Render repo-relative paths for display / \u5c06\u8def\u5f84\u663e\u793a\u4e3a\u4ed3\u5e93\u76f8\u5bf9\u8def\u5f84.\"\"\"\n",
    "    try:\n",
    "        return str(Path(p).resolve().relative_to(REPO_ROOT))\n",
    "    except Exception:\n",
    "        return str(p)\n",
    "\n",
    "def find_path(filename: str) -> Path:\n",
    "    \"\"\"Locate a file under outputs/ or data/raw / \u5728 outputs/ \u6216 data/raw \u4e2d\u5b9a\u4f4d\u6587\u4ef6.\"\"\"\n",
    "    for p in (OUT_DIR / filename, DATA_DIR / filename, REPO_ROOT / filename):\n",
    "        if p.exists():\n",
    "            return p\n",
    "    for root, _, files in os.walk(OUT_DIR):\n",
    "        if filename in files:\n",
    "            return Path(root) / filename\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find {filename}. Put it under data/raw or outputs/segment.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe0ebc-abbc-47da-98e0-fe73fa098d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:47.029323Z",
     "iopub.status.busy": "2026-01-11T10:12:47.029196Z",
     "iopub.status.idle": "2026-01-11T10:12:47.769529Z",
     "shell.execute_reply": "2026-01-11T10:12:47.769161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Environment and Path Check / \u73af\u5883\u4e0e\u8def\u5f84\u68c0\u67e5\n",
    "import os, sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 0)\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"DATA_DIR:\", _rel(DATA_DIR))\n",
    "print(\"Contents of data dir:\", [p.name for p in DATA_DIR.iterdir()] if DATA_DIR.exists() else \"MISSING\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(\"SQLite not found at DB_PATH. Set AIRLINE_DB_PATH or place the DB under data/raw.\")\n",
    "db_path = str(DB_PATH)\n",
    "print(\"Using database:\", _rel(DB_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f64b6-0cf5-4d63-a6c0-a48223a5f90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:47.770935Z",
     "iopub.status.busy": "2026-01-11T10:12:47.770782Z",
     "iopub.status.idle": "2026-01-11T10:12:47.862963Z",
     "shell.execute_reply": "2026-01-11T10:12:47.862728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use shared database connection / \u4f7f\u7528\u5171\u4eab\u6570\u636e\u5e93\u8fde\u63a5\n",
    "\n",
    "# List all tables / \u5217\u51fa\u6240\u6709\u8868\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\",\n",
    "    conn\n",
    ")['name'].tolist()\n",
    "print(f\"Found {len(tables)} tables in total:\", tables)\n",
    "\n",
    "# Row count audit / \u884c\u6570\u5ba1\u8ba1\n",
    "counts = []\n",
    "for t in tables:\n",
    "    n = pd.read_sql(f\"SELECT COUNT(*) AS n FROM [{t}];\", conn)['n'].iat[0]\n",
    "    counts.append((t, n))\n",
    "counts_df = (\n",
    "    pd.DataFrame(counts, columns=['table', 'rows'])\n",
    "      .sort_values('rows', ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "counts_df  # Display as a neat table, screenshot this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a7bc8-27da-4eb8-9fdf-33ee1f8a8650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:47.864182Z",
     "iopub.status.busy": "2026-01-11T10:12:47.864079Z",
     "iopub.status.idle": "2026-01-11T10:12:47.881303Z",
     "shell.execute_reply": "2026-01-11T10:12:47.881074Z"
    }
   },
   "outputs": [],
   "source": [
    "for t in tables:\n",
    "    print(f\"\\n===== {t} (first 5 rows) =====\")\n",
    "    display(pd.read_sql(f\"SELECT * FROM [{t}] LIMIT 5;\", conn))\n",
    "\n",
    "# conn.close() / \u5173\u95ed\u8fde\u63a5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718b73f-567b-472f-9418-2bb4d89c3557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:47.882958Z",
     "iopub.status.busy": "2026-01-11T10:12:47.882856Z",
     "iopub.status.idle": "2026-01-11T10:12:50.107616Z",
     "shell.execute_reply": "2026-01-11T10:12:50.106591Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, sqlite3, glob, os, io\n",
    "from IPython.display import display\n",
    "\n",
    "# Try to read the 8 common tables of an airline database (only those that actually exist will be loaded) / \u5c1d\u8bd5\u8bfb\u53d6\u822a\u7a7a\u6570\u636e\u5e93\u5e38\u89c1 8 \u8868\uff08\u4ec5\u52a0\u8f7d\u5b9e\u9645\u5b58\u5728\u8005\uff09\n",
    "core = ['bookings','tickets','ticket_flights','flights',\n",
    "        'boarding_passes','seats','aircrafts_data','airports_data']\n",
    "exists = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)['name'].tolist()\n",
    "TABLES = [t for t in core if t in exists]\n",
    "\n",
    "dfs = {t: pd.read_sql(f\"SELECT * FROM [{t}];\", conn) for t in TABLES}\n",
    "\n",
    "overview = (pd.DataFrame([(t, df.shape[0], df.shape[1]) for t, df in dfs.items()],\n",
    "                         columns=['table','rows','cols'])\n",
    "              .sort_values('rows', ascending=False).reset_index(drop=True))\n",
    "display(overview)  # A screenshot of this can serve as evidence of the data's scale (fulfills requirement 2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520ad48-14d0-4e80-b0b1-efe0ab9b5f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:50.109265Z",
     "iopub.status.busy": "2026-01-11T10:12:50.109154Z",
     "iopub.status.idle": "2026-01-11T10:12:50.260976Z",
     "shell.execute_reply": "2026-01-11T10:12:50.260727Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "for t, df in dfs.items():\n",
    "    print(f\"\\n===== {t}: df.info() =====\")\n",
    "    buf = io.StringIO()\n",
    "    df.info(buf=buf)\n",
    "    print(buf.getvalue())    # Screenshot a few core tables (e.g., bookings / flights / ticket_flights / tickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee8539-77e7-4a29-8167-66525359a926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:50.262362Z",
     "iopub.status.busy": "2026-01-11T10:12:50.262272Z",
     "iopub.status.idle": "2026-01-11T10:12:50.805069Z",
     "shell.execute_reply": "2026-01-11T10:12:50.804851Z"
    }
   },
   "outputs": [],
   "source": [
    "for t, df in dfs.items():\n",
    "    print(f\"\\n===== {t}: df.describe() (numeric) =====\")\n",
    "    try:\n",
    "        display(df.describe(datetime_is_numeric=True).T)\n",
    "    except TypeError:\n",
    "        display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1014d0c-1088-4cf2-a8e6-8ccb0c9777a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:50.806261Z",
     "iopub.status.busy": "2026-01-11T10:12:50.806181Z",
     "iopub.status.idle": "2026-01-11T10:12:52.899145Z",
     "shell.execute_reply": "2026-01-11T10:12:52.898896Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bk = pd.read_sql(\"SELECT total_amount FROM bookings WHERE total_amount IS NOT NULL;\", conn)\n",
    "x  = bk['total_amount'].astype(float)\n",
    "p99 = x.quantile(0.99)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "# Histogram (capped at 99th percentile to avoid extreme values compressing the main body) / \u76f4\u65b9\u56fe\uff08\u622a\u65ad\u81f3 99 \u5206\u4f4d\uff0c\u907f\u514d\u6781\u503c\u538b\u7f29\u4e3b\u4f53\uff09\n",
    "plt.hist(x[x <= p99], bins=60, density=True)\n",
    "plt.xlabel(\"Booking revenue (total_amount)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Booking Revenue\")\n",
    "\n",
    "# Mean/median reference lines / \u5747\u503c\u4e0e\u4e2d\u4f4d\u6570\u53c2\u8003\u7ebf\n",
    "mean_v, med_v = x.mean(), x.median()\n",
    "plt.axvline(mean_v, linestyle='--', linewidth=1, label=f\"Mean={mean_v:,.0f}\")\n",
    "plt.axvline(med_v,  linestyle=':',  linewidth=1, label=f\"Median={med_v:,.0f}\")\n",
    "# Optional KDE (skipped if SciPy is not installed) / \u53ef\u9009 KDE\uff08\u672a\u5b89\u88c5 SciPy \u5219\u8df3\u8fc7\uff09\n",
    "try:\n",
    "    from scipy.stats import gaussian_kde\n",
    "    xs = np.linspace(0, p99, 400)\n",
    "    kde = gaussian_kde(x[x <= p99])\n",
    "    plt.plot(xs, kde(xs), linewidth=1, label=\"KDE\")\n",
    "except Exception as _:\n",
    "    print(\"Info: SciPy is not installed, skipping KDE plot. This does not affect the grading criteria.\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(FIG_DIR/\"figure_2_3_1_booking_revenue_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627f1af-7b82-4685-9338-a049ae9fb744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:52.900756Z",
     "iopub.status.busy": "2026-01-11T10:12:52.900603Z",
     "iopub.status.idle": "2026-01-11T10:12:54.389857Z",
     "shell.execute_reply": "2026-01-11T10:12:54.389547Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "sql_top_routes = \"\"\"\n",
    "SELECT\n",
    "    f.departure_airport || '-' || f.arrival_airport AS route,\n",
    "    SUM(tf.amount)                            AS total_revenue,\n",
    "    COUNT(DISTINCT tf.ticket_no)              AS n_tickets,\n",
    "    COUNT(*)                                  AS n_segments\n",
    "FROM ticket_flights AS tf\n",
    "JOIN flights        AS f\n",
    "  ON tf.flight_id = f.flight_id\n",
    "GROUP BY route\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "top_routes = pd.read_sql(sql_top_routes, conn)\n",
    "\n",
    "total_rev = pd.read_sql(\"SELECT SUM(amount) AS r FROM ticket_flights;\", conn)['r'].iat[0]\n",
    "share_top10 = (top_routes['total_revenue'].sum() / total_rev * 100)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "y = top_routes['route'].iloc[::-1]\n",
    "x = top_routes['total_revenue'].iloc[::-1]\n",
    "plt.barh(y, x)\n",
    "plt.xlabel(\"Total revenue (dataset units)\")\n",
    "plt.ylabel(\"Route (DEP-ARR)\")\n",
    "plt.title(f\"Top 10 Routes by Total Revenue\\nTop10 share = {share_top10:.1f}%\")\n",
    "\n",
    "# Annotate the value at the end of each bar / \u5728\u6bcf\u4e2a\u67f1\u6761\u672b\u7aef\u6807\u6ce8\u6570\u503c\n",
    "for i, v in enumerate(x):\n",
    "    plt.text(v, i, f\"{v:,.0f}\", va='center', ha='left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(FIG_DIR/\"figure_2_3_2_top_routes_by_revenue.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "display(top_routes)  # This table can be screenshotted and placed next to the chart as supporting evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4bb82-481a-4687-8d3e-48767f71ea4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:54.391580Z",
     "iopub.status.busy": "2026-01-11T10:12:54.391483Z",
     "iopub.status.idle": "2026-01-11T10:12:57.727966Z",
     "shell.execute_reply": "2026-01-11T10:12:57.727610Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT amount, fare_conditions\n",
    "FROM ticket_flights\n",
    "WHERE amount IS NOT NULL\n",
    "  AND fare_conditions IN ('Economy','Comfort','Business');\n",
    "\"\"\"\n",
    "df_fare = pd.read_sql(q, conn)\n",
    "order = ['Economy','Comfort','Business']\n",
    "data  = [df_fare.loc[df_fare['fare_conditions']==k, 'amount'].values for k in order]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "parts = ax.violinplot(data, showmedians=True, widths=0.8)\n",
    "# Plot the quartiles (Q1, Q3) / \u7ed8\u5236\u56db\u5206\u4f4d\u6570\uff08Q1\u3001Q3\uff09\n",
    "pos = np.arange(1, len(order)+1)\n",
    "for p, d in zip(pos, data):\n",
    "    q1, q3 = np.percentile(d, [25, 75])\n",
    "    ax.hlines([q1, q3], p-0.35, p+0.35, linewidth=1)\n",
    "\n",
    "ax.set_xticks(pos); ax.set_xticklabels(order)\n",
    "ax.set_ylabel(\"Ticket amount\")\n",
    "ax.set_title(\"Ticket Price Distribution by Fare Class\")\n",
    "plt.tight_layout()\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(FIG_DIR/\"figure_2_3_3_ticket_price_by_fare_class.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "df_fare.groupby('fare_conditions')['amount'].describe()[['count','mean','50%','std','min','max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c3e81-d834-49f2-ba80-ae0073e3e992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:57.729866Z",
     "iopub.status.busy": "2026-01-11T10:12:57.729746Z",
     "iopub.status.idle": "2026-01-11T10:12:59.827202Z",
     "shell.execute_reply": "2026-01-11T10:12:59.826839Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import spearmanr as _spr\n",
    "\n",
    "# Select only the two columns needed for plotting to reduce memory usage / \u4ec5\u9009\u62e9\u4f5c\u56fe\u6240\u9700\u4e24\u5217\u4ee5\u964d\u4f4e\u5185\u5b58\u5360\u7528\n",
    "sql = \"\"\"\n",
    "SELECT tf.amount, f.scheduled_departure, f.scheduled_arrival\n",
    "FROM ticket_flights AS tf\n",
    "JOIN flights        AS f\n",
    "  ON tf.flight_id = f.flight_id\n",
    "WHERE tf.amount IS NOT NULL\n",
    "  AND f.scheduled_departure IS NOT NULL\n",
    "  AND f.scheduled_arrival   IS NOT NULL;\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql, conn)\n",
    "\n",
    "# Parse timestamps and calculate duration in minutes (note: timestamps include timezones) / \u89e3\u6790\u65f6\u95f4\u6233\u5e76\u8ba1\u7b97\u65f6\u957f\uff08\u5206\u949f\uff0c\u542b\u65f6\u533a\uff09\n",
    "dep = pd.to_datetime(df['scheduled_departure'], utc=True, errors='coerce')\n",
    "arr = pd.to_datetime(df['scheduled_arrival'],   utc=True, errors='coerce')\n",
    "df['duration_min'] = (arr - dep).dt.total_seconds() / 60\n",
    "\n",
    "# Data quality filter: remove negative values / extreme durations (<15 min or > 1200 min) / \u6570\u636e\u8d28\u91cf\u8fc7\u6ee4\uff1a\u79fb\u9664\u8d1f\u503c\u6216\u6781\u7aef\u65f6\u957f\uff08<15 \u6216 >1200 \u5206\u949f\uff09\n",
    "df = df[df['duration_min'].between(15, 1200)]\n",
    "# To improve readability, limit data to the 99.5th percentile / \u4e3a\u63d0\u9ad8\u53ef\u8bfb\u6027\uff0c\u622a\u65ad\u81f3 99.5 \u5206\u4f4d\n",
    "q_amt = df['amount'].quantile(0.995)\n",
    "q_dur = df['duration_min'].quantile(0.995)\n",
    "dfp = df[(df['amount'] <= q_amt) & (df['duration_min'] <= q_dur)]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "hb = plt.hexbin(dfp['duration_min'], dfp['amount'], gridsize=55, bins='log', mincnt=1)\n",
    "cb = plt.colorbar(hb); cb.set_label('log10(N)')\n",
    "plt.xlabel(\"Flight duration (minutes)\")\n",
    "plt.ylabel(\"Ticket amount\")\n",
    "plt.title(\"Flight Duration vs Ticket Amount\")\n",
    "plt.tight_layout()\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(FIG_DIR/\"figure_2_3_4_duration_vs_amount.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Optional: provide rank correlation to quantify the \"positively correlated but dispersed\" observation / \u53ef\u9009\uff1a\u7528\u79e9\u76f8\u5173\u91cf\u5316\u201c\u6b63\u76f8\u5173\u4f46\u5206\u6563\u201d\u7684\u89c2\u5bdf\n",
    "try:\n",
    "    rho, p = _spr(dfp['duration_min'], dfp['amount'])\n",
    "    print(f\"Spearman \u03c1 = {rho:.2f} (p={p:.2g})\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3dae2-d454-40ae-9a2d-48edd43f07d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:12:59.829015Z",
     "iopub.status.busy": "2026-01-11T10:12:59.828881Z",
     "iopub.status.idle": "2026-01-11T10:13:00.056174Z",
     "shell.execute_reply": "2026-01-11T10:13:00.055906Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sqlite3, glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Connect (can be skipped if 'conn' object already exists) / 1\uff09\u8fde\u63a5\u6570\u636e\u5e93\uff08\u82e5 conn \u5df2\u5b58\u5728\u53ef\u8df3\u8fc7\uff09\n",
    "# 2) Calculate the missing rate for each status (treat '\\N' as missing as well) / 2\uff09\u8ba1\u7b97\u5404\u72b6\u6001\u7f3a\u5931\u7387\uff08\u5c06 '\\N' \u89c6\u4e3a\u7f3a\u5931\uff09\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "  status,\n",
    "  SUM(CASE WHEN actual_departure IS NULL OR actual_departure = '\\\\N' THEN 1 ELSE 0 END)*1.0/COUNT(*) AS miss_actual_departure,\n",
    "  SUM(CASE WHEN actual_arrival   IS NULL OR actual_arrival   = '\\\\N' THEN 1 ELSE 0 END)*1.0/COUNT(*) AS miss_actual_arrival,\n",
    "  COUNT(*) AS n\n",
    "FROM flights\n",
    "GROUP BY status\n",
    "ORDER BY n DESC;\n",
    "\"\"\"\n",
    "miss_by_status = pd.read_sql(sql, conn)\n",
    "miss_tbl = (miss_by_status\n",
    "              .assign(miss_actual_departure_pct=(miss_by_status['miss_actual_departure']*100).round(1),\n",
    "                      miss_actual_arrival_pct  =(miss_by_status['miss_actual_arrival']  *100).round(1))\n",
    "              [['status','n','miss_actual_departure_pct','miss_actual_arrival_pct']])\n",
    "\n",
    "overall = pd.read_sql(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN actual_departure IS NULL OR actual_departure = '\\\\N' THEN 1 ELSE 0 END)*1.0/COUNT(*) AS miss_dep,\n",
    "  SUM(CASE WHEN actual_arrival   IS NULL OR actual_arrival   = '\\\\N' THEN 1 ELSE 0 END)*1.0/COUNT(*) AS miss_arr\n",
    "FROM flights;\n",
    "\"\"\", conn)\n",
    "miss_dep_pct = round(overall['miss_dep'].iat[0]*100, 1)\n",
    "miss_arr_pct = round(overall['miss_arr'].iat[0]*100, 1)\n",
    "print(f\"Overall missing rate: actual_departure = {miss_dep_pct}%, actual_arrival = {miss_arr_pct}%\")\n",
    "\n",
    "# 3) Ensure output directories exist (to fix OSError) / 3\uff09\u786e\u4fdd\u8f93\u51fa\u76ee\u5f55\u5b58\u5728\uff08\u907f\u514d OSError\uff09\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 4) Save to CSV / 4\uff09\u4fdd\u5b58\u4e3a CSV\n",
    "csv_path = TAB_DIR / \"table_2_4_1_missing_by_status.csv\"\n",
    "miss_tbl.to_csv(csv_path, index=False)\n",
    "print(\"CSV saved to:\", _rel(csv_path))\n",
    "\n",
    "# 5) Save as an image (for direct inclusion in reports) / 5\uff09\u4fdd\u5b58\u4e3a\u56fe\u7247\uff08\u4fbf\u4e8e\u76f4\u63a5\u653e\u5165\u62a5\u544a\uff09\n",
    "fig_h = 1.2 + 0.35*len(miss_tbl)\n",
    "fig, ax = plt.subplots(figsize=(8, fig_h))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Missing Data Rates by Flight Status\", pad=12)\n",
    "ax.table(cellText=miss_tbl.values,\n",
    "         colLabels=['status','n','miss_actual_departure (%)','miss_actual_arrival (%)'],\n",
    "         cellLoc='center', loc='center')\n",
    "png_path = FIG_DIR / \"table_2_4_1_missing_by_status.png\"\n",
    "plt.savefig(png_path, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Image saved to:\", _rel(png_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabf7bb-bd21-4b90-8e17-df1c9e49d178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:00.058074Z",
     "iopub.status.busy": "2026-01-11T10:13:00.057951Z",
     "iopub.status.idle": "2026-01-11T10:13:00.597864Z",
     "shell.execute_reply": "2026-01-11T10:13:00.597608Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import glob\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "join_keys = {\n",
    "    'bookings':       ['book_ref'],\n",
    "    'tickets':        ['book_ref','ticket_no'],\n",
    "    'ticket_flights': ['ticket_no','flight_id'],\n",
    "    'flights':        ['flight_id','aircraft_code','departure_airport','arrival_airport'],\n",
    "    'aircrafts_data': ['aircraft_code'],\n",
    "    'seats':          ['aircraft_code'],\n",
    "    'airports_data':  ['airport_code'],\n",
    "    'boarding_passes':['ticket_no','flight_id']\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for tbl, cols in join_keys.items():\n",
    "    # Use try-except in case a table doesn't exist in the user's database / \u4f7f\u7528 try-except \u4ee5\u9632\u7528\u6237\u6570\u636e\u5e93\u7f3a\u8868\n",
    "    try:\n",
    "        pragma = pd.read_sql(f\"PRAGMA table_info({tbl});\", conn).set_index('name')['type'].to_dict()\n",
    "        for c in cols:\n",
    "            s = pd.read_sql(f\"SELECT {c} FROM [{tbl}] WHERE {c} IS NOT NULL LIMIT 5000;\", conn)[c]\n",
    "            rows.append([tbl, c, pragma.get(c, ''), str(s.dtype), round(s.astype(str).str.fullmatch(r\"\\d+\").mean()*100,1)])\n",
    "    except pd.io.sql.DatabaseError:\n",
    "        print(f\"Warning: Table '{tbl}' not found, skipping.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "dtype_df = pd.DataFrame(rows, columns=['table','column','sqlite_type','pandas_dtype','numeric_like_%'])\n",
    "\n",
    "fig_h = 1.2 + 0.35*len(dtype_df)\n",
    "fig, ax = plt.subplots(figsize=(10, fig_h))\n",
    "ax.axis('off'); ax.set_title(\"Join-key Data Types by Table\", pad=12)\n",
    "ax.table(cellText=dtype_df.values, colLabels=dtype_df.columns, cellLoc='center', loc='center')\n",
    "out_path = FIG_DIR / \"figure_2_4_2_key_dtypes.png\"\n",
    "plt.savefig(out_path, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Image saved to:\", _rel(out_path))\n",
    "\n",
    "dtype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8274a-6bd4-4e67-b712-ae4822721958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:00.599246Z",
     "iopub.status.busy": "2026-01-11T10:13:00.599126Z",
     "iopub.status.idle": "2026-01-11T10:13:01.645772Z",
     "shell.execute_reply": "2026-01-11T10:13:01.645495Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "bk = pd.read_sql(\"SELECT total_amount FROM bookings WHERE total_amount IS NOT NULL;\", conn)['total_amount'].astype(float)\n",
    "tf = pd.read_sql(\"SELECT amount       FROM ticket_flights WHERE amount IS NOT NULL;\", conn)['amount'].astype(float)\n",
    "\n",
    "bk_min, bk_max = float(bk.min()), float(bk.max())\n",
    "tf_min, tf_max = float(tf.min()), float(tf.max())\n",
    "print(f\"Booking level total_amount: min={bk_min:,.0f}, max={bk_max:,.0f}\")\n",
    "print(f\"Ticket level amount:        min={tf_min:,.0f}, max={tf_max:,.0f}\")\n",
    "print(f\"Any negative values? bookings={ (bk<0).any() }, ticket_flights={ (tf<0).any() }\")\n",
    "\n",
    "bk_cap = bk[bk <= bk.quantile(0.995)]\n",
    "tf_cap = tf[tf <= tf.quantile(0.995)]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.boxplot([bk_cap, tf_cap],\n",
    "            tick_labels=['total_amount (bookings)','amount (ticket_flights)'],  # \u2190 Corrected parameter name\n",
    "            showfliers=True)\n",
    "plt.ylabel(\"Value (dataset units)\")\n",
    "plt.title(\"Box Plots of Monetary Fields\")\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/\"figure_2_4_1_boxplots_monetary.png\", dpi=300, bbox_inches='tight'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04beeb1-a5ee-49ec-a019-91f36f76adb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:01.647680Z",
     "iopub.status.busy": "2026-01-11T10:13:01.647536Z",
     "iopub.status.idle": "2026-01-11T10:13:01.980820Z",
     "shell.execute_reply": "2026-01-11T10:13:01.980477Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import glob\n",
    "\n",
    "# Using textwrap.shorten (this import was missing in the original error) / \u4f7f\u7528 textwrap.shorten\uff08\u539f\u9519\u8bef\u7f3a\u5c11\u8be5\u5bfc\u5165\uff09\n",
    "from textwrap import shorten\n",
    "\n",
    "def parse_en(s):\n",
    "    try:\n",
    "        obj = json.loads(s) if isinstance(s, str) else {}\n",
    "        return obj.get('en')\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "aircrafts = pd.read_sql(\"SELECT aircraft_code, model FROM aircrafts_data;\", conn)\n",
    "airports  = pd.read_sql(\"SELECT airport_code, airport_name FROM airports_data;\", conn)\n",
    "\n",
    "aircrafts['model_en'] = aircrafts['model'].apply(parse_en)\n",
    "airports['airport_name_en'] = airports['airport_name'].apply(parse_en)\n",
    "\n",
    "aircrafts_has_en = aircrafts['model_en'].notna().mean()*100\n",
    "airports_has_en  = airports['airport_name_en'].notna().mean()*100\n",
    "print(f\"Percentage of aircrafts_data.model with 'en' key: {aircrafts_has_en:.1f}%\")\n",
    "print(f\"Percentage of airports_data.airport_name with 'en' key: {airports_has_en:.1f}%\")\n",
    "\n",
    "# Before-and-after comparison table / \u524d\u540e\u5bf9\u6bd4\u8868\n",
    "samples = []\n",
    "for _, r in aircrafts.head(5).iterrows():\n",
    "    samples.append(['aircrafts_data', r['aircraft_code'],\n",
    "                      shorten(str(r['model']), width=40, placeholder='\u2026'),\n",
    "                      r['model_en']])\n",
    "for _, r in airports.head(5).iterrows():\n",
    "    samples.append(['airports_data', r['airport_code'],\n",
    "                      shorten(str(r['airport_name']), width=40, placeholder='\u2026'),\n",
    "                      r['airport_name_en']])\n",
    "\n",
    "demo = pd.DataFrame(samples, columns=['table','code','raw_JSON (truncated)','parsed_en'])\n",
    "\n",
    "fig_h = 1.2 + 0.35*len(demo)\n",
    "fig, ax = plt.subplots(figsize=(10, fig_h))\n",
    "ax.axis('off'); ax.set_title(\"Figure 2.4.3 JSON Parsing Examples (raw \u2192 en)\", pad=12)\n",
    "ax.table(cellText=demo.values, colLabels=demo.columns, cellLoc='center', loc='center')\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_path = FIG_DIR / \"figure_2_4_3_json_parsing_examples.png\"\n",
    "plt.savefig(out_path, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Image saved to:\", _rel(out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850e498-71bb-4ed0-b0da-6c47ec3b8192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:01.982715Z",
     "iopub.status.busy": "2026-01-11T10:13:01.982426Z",
     "iopub.status.idle": "2026-01-11T10:13:05.763595Z",
     "shell.execute_reply": "2026-01-11T10:13:05.762523Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, sqlite3, json\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# thresholds (edit if needed) / \u9608\u503c\uff08\u53ef\u6309\u9700\u8c03\u6574\uff09\n",
    "THRESH_ARRIVED_MISS_PCT = 1.0\n",
    "THRESH_JSON_EN_COVERAGE = 95.0\n",
    "THRESH_NEG_LEAD_PCT     = 0.0\n",
    "SAMPLE_FOR_LEADTIME     = 300000\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rows = []\n",
    "def add_row(check, rule, observed, ok, action):\n",
    "    rows.append([check, rule, observed, (\"PASS\" if ok else \"FAIL\"), action])\n",
    "\n",
    "# A) missing actual_* on Arrived / A\uff09\u5df2\u5230\u8fbe\u822a\u73ed actual_* \u7f3a\u5931\n",
    "miss = pd.read_sql(\"\"\"\n",
    "SELECT status,\n",
    "  SUM(CASE WHEN actual_departure IS NULL OR actual_departure='\\\\N' THEN 1 ELSE 0 END)*1.0/COUNT(*) AS miss_dep,\n",
    "  SUM(CASE WHEN actual_arrival   IS NULL OR actual_arrival  ='\\\\N' THEN 1 ELSE 0 END)*1.0/COUNT(*) AS miss_arr\n",
    "FROM flights GROUP BY status;\n",
    "\"\"\", conn)\n",
    "arrived = miss[miss['status'].str.upper().eq('ARRIVED')]\n",
    "dep_pct = float(arrived['miss_dep'].iat[0]*100) if not arrived.empty else np.nan\n",
    "arr_pct = float(arrived['miss_arr'].iat[0]*100) if not arrived.empty else np.nan\n",
    "add_row(\"Missing actual times (Arrived flights)\",\n",
    "        f\"Both \u2264 {THRESH_ARRIVED_MISS_PCT}%\",\n",
    "        f\"dep {dep_pct:.1f}%, arr {arr_pct:.1f}%\",\n",
    "        (dep_pct <= THRESH_ARRIVED_MISS_PCT) and (arr_pct <= THRESH_ARRIVED_MISS_PCT),\n",
    "        \"Sec 3.1/3.2 keep Arrived-only for actual performance; others use scheduled or drop\")\n",
    "\n",
    "# B) join-key dtype consistency / B\uff09\u8fde\u63a5\u952e\u7c7b\u578b\u4e00\u81f4\u6027\n",
    "join_keys = {\n",
    "    'bookings':['book_ref'],'tickets':['book_ref','ticket_no'],\n",
    "    'ticket_flights':['ticket_no','flight_id'],\n",
    "    'flights':['flight_id','aircraft_code','departure_airport','arrival_airport'],\n",
    "    'aircrafts_data':['aircraft_code'],'seats':['aircraft_code'],\n",
    "    'airports_data':['airport_code'],'boarding_passes':['ticket_no','flight_id']\n",
    "}\n",
    "rec=[]\n",
    "for tbl, cols in join_keys.items():\n",
    "    pragma = pd.read_sql(f\"PRAGMA table_info({tbl});\", conn).set_index('name')['type'].to_dict()\n",
    "    for c in cols:\n",
    "        s = pd.read_sql(f\"SELECT {c} FROM [{tbl}] WHERE {c} IS NOT NULL LIMIT 5000;\", conn)[c]\n",
    "        rec.append([tbl,c,pragma.get(c,''),str(s.dtype)])\n",
    "dtype_df = pd.DataFrame(rec, columns=['table','column','sqlite_type','pandas_dtype'])\n",
    "mix = dtype_df.groupby('column')['pandas_dtype'].nunique()\n",
    "offenders = sorted(mix[mix>1].index.tolist())\n",
    "add_row(\"Join-key data types consistent\",\n",
    "        \"All join keys TEXT/str across tables\",\n",
    "        (\"consistent\" if len(offenders)==0 else \"mixed: \"+\", \".join(offenders[:5])+(\" \u2026\" if len(offenders)>5 else \"\")),\n",
    "        (len(offenders)==0),\n",
    "        \"Sec 3.2 cast to string before merge (astype(str)); enforce TEXT on import\")\n",
    "\n",
    "# C) monetary fields / C\uff09\u91d1\u989d\u5b57\u6bb5\n",
    "bk = pd.read_sql(\"SELECT total_amount FROM bookings WHERE total_amount IS NOT NULL;\", conn)['total_amount'].astype(float)\n",
    "tf = pd.read_sql(\"SELECT amount FROM ticket_flights WHERE amount IS NOT NULL;\", conn)['amount'].astype(float)\n",
    "bk_min, bk_max = float(bk.min()), float(bk.max())\n",
    "tf_min, tf_max = float(tf.min()), float(tf.max())\n",
    "add_row(\"Monetary fields: no negatives\",\n",
    "        \"min \u2265 0 for total_amount & amount\",\n",
    "        f\"bookings[{bk_min:.0f},{bk_max:.0f}], ticket_flights[{tf_min:.0f},{tf_max:.0f}]\",\n",
    "        (bk_min >= 0) and (tf_min >= 0),\n",
    "        \"Sec 4.2 apply log/power transform only; if negatives appear, fix in Sec 3.2\")\n",
    "\n",
    "add_row(\"Monetary distributions are heavy\u2011tailed\",\n",
    "        \"Right\u2011skew expected; handle via transform\",\n",
    "        f\"skew(total_amount)={bk.skew():.2f}, skew(amount)={tf.skew():.2f}\",\n",
    "        True,\n",
    "        \"Sec 4.2 compare Log vs PowerTransformer/QuantileTransformer\")\n",
    "\n",
    "# D) JSON 'en' coverage / D\uff09JSON \u82f1\u6587\u8986\u76d6\u7387\n",
    "def _en(x):\n",
    "    try:\n",
    "        obj = json.loads(x) if isinstance(x,str) else {}\n",
    "        return obj.get('en')\n",
    "    except Exception:\n",
    "        return None\n",
    "cov_aircrafts = pd.read_sql(\"SELECT model FROM aircrafts_data;\", conn)['model'].apply(_en).notna().mean()*100\n",
    "cov_airports  = pd.read_sql(\"SELECT airport_name FROM airports_data;\", conn)['airport_name'].apply(_en).notna().mean()*100\n",
    "ok = (cov_aircrafts>=THRESH_JSON_EN_COVERAGE) and (cov_airports>=THRESH_JSON_EN_COVERAGE)\n",
    "add_row(\"JSON multilingual fields: English key present\",\n",
    "        f\"Coverage \u2265 {THRESH_JSON_EN_COVERAGE}%\",\n",
    "        f\"model.en={cov_aircrafts:.1f}%, airport_name.en={cov_airports:.1f}%\",\n",
    "        ok,\n",
    "        \"Sec 3.2 parse JSON \u2192 extract 'en' column; normalise case/whitespace\")\n",
    "\n",
    "# E) PK duplicates / E\uff09\u4e3b\u952e\u91cd\u590d\n",
    "def pk_dups(table, cols):\n",
    "    cols_csv = \",\".join(f\"[{c}]\" for c in cols)\n",
    "    n   = pd.read_sql(f\"SELECT COUNT(*) AS n FROM [{table}];\", conn)['n'].iat[0]\n",
    "    dup = pd.read_sql(f\"\"\"\n",
    "        SELECT IFNULL(SUM(cnt-1),0) AS dups FROM (\n",
    "          SELECT {cols_csv}, COUNT(*) AS cnt FROM [{table}] GROUP BY {cols_csv}\n",
    "        ) WHERE cnt>1;\n",
    "    \"\"\", conn)['dups'].iat[0]\n",
    "    return int(n), int(dup)\n",
    "for tbl, cols in [\n",
    "    (\"bookings\",[\"book_ref\"]),(\"tickets\",[\"ticket_no\"]),(\"flights\",[\"flight_id\"]),\n",
    "    (\"airports_data\",[\"airport_code\"]),(\"aircrafts_data\",[\"aircraft_code\"]),\n",
    "    (\"seats\",[\"aircraft_code\",\"seat_no\"]),(\"ticket_flights\",[\"ticket_no\",\"flight_id\"]),\n",
    "    (\"boarding_passes\",[\"ticket_no\",\"flight_id\"])\n",
    "]:\n",
    "    n,d = pk_dups(tbl, cols)\n",
    "    add_row(f\"PK duplicates in {tbl}({'+'.join(cols)})\",\n",
    "            \"0 duplicates\", f\"{d} / {n}\", (d==0),\n",
    "            \"Sec 3.2 if >0: deduplicate with documented rule; check source\")\n",
    "\n",
    "# F) FK coverage / F\uff09\u5916\u952e\u8986\u76d6\u7387\n",
    "orphan_t  = pd.read_sql(\"\"\"SELECT COUNT(*) AS n_orphan\n",
    "FROM tickets t LEFT JOIN bookings b ON t.book_ref=b.book_ref\n",
    "WHERE b.book_ref IS NULL;\"\"\", conn)['n_orphan'].iat[0]\n",
    "tot_t = pd.read_sql(\"SELECT COUNT(*) AS n FROM tickets;\", conn)['n'].iat[0]\n",
    "rate_t = orphan_t*100.0/tot_t\n",
    "add_row(\"FK coverage: tickets\u2192bookings\", \"Orphan rate = 0%\", f\"{rate_t:.2f}%\", (rate_t==0),\n",
    "        \"Sec 3.4 drop or fix orphans before merge\")\n",
    "\n",
    "orphan_tf = pd.read_sql(\"\"\"SELECT COUNT(*) AS n_orphan\n",
    "FROM ticket_flights tf LEFT JOIN flights f ON tf.flight_id=f.flight_id\n",
    "WHERE f.flight_id IS NULL;\"\"\", conn)['n_orphan'].iat[0]\n",
    "tot_tf = pd.read_sql(\"SELECT COUNT(*) AS n FROM ticket_flights;\", conn)['n'].iat[0]\n",
    "rate_tf = orphan_tf*100.0/tot_tf\n",
    "add_row(\"FK coverage: ticket_flights\u2192flights\", \"Orphan rate = 0%\", f\"{rate_tf:.2f}%\", (rate_tf==0),\n",
    "        \"Sec 3.4 drop or fix orphans before merge\")\n",
    "\n",
    "# assemble & save / \u6c47\u603b\u5e76\u4fdd\u5b58\n",
    "score = pd.DataFrame(rows, columns=[\n",
    "    \"Quality Check\",\"Threshold/Rule\",\"Observed Value\",\"Status\",\"Action in Section 3\"\n",
    "])\n",
    "score['__order'] = (score['Status']==\"FAIL\").astype(int)*-1\n",
    "score = score.sort_values(['__order','Quality Check']).drop(columns='__order')\n",
    "\n",
    "csv_path = TAB_DIR/\"table_2_4_2_scorecard.csv\"\n",
    "score.to_csv(csv_path, index=False); print(\"Saved CSV:\", _rel(csv_path))\n",
    "\n",
    "fig_h = 1.2 + 0.35*len(score)\n",
    "fig, ax = plt.subplots(figsize=(12, fig_h))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Table 2.4.2 Data Quality Scorecard (gate to Section 3)\", pad=12)\n",
    "\n",
    "# red/green background for Status column / \u4e3a Status \u5217\u52a0\u7ea2/\u7eff\u5e95\u8272\n",
    "cols = score.columns.tolist()\n",
    "status_idx = cols.index(\"Status\")\n",
    "cellcolors = [[\"white\"]*len(cols) for _ in range(len(score))]\n",
    "for i, s in enumerate(score[\"Status\"]):\n",
    "    cellcolors[i][status_idx] = PASS_COLOR if s==\"PASS\" else FAIL_COLOR\n",
    "\n",
    "tbl = ax.table(cellText=score.values, colLabels=cols, cellLoc='center',\n",
    "               cellColours=cellcolors, loc='center')\n",
    "tbl.auto_set_font_size(False); tbl.set_fontsize(9)\n",
    "for (r,c), cell in tbl.get_celld().items():\n",
    "    if r==0: cell.set_text_props(weight='bold')\n",
    "\n",
    "png_path = FIG_DIR/\"table_2_4_2_scorecard.png\"\n",
    "plt.savefig(png_path, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved PNG:\", _rel(png_path))\n",
    "\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c289e-a2bd-4f07-8bbd-9b5f3b5d4e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:05.768414Z",
     "iopub.status.busy": "2026-01-11T10:13:05.768059Z",
     "iopub.status.idle": "2026-01-11T10:13:11.244115Z",
     "shell.execute_reply": "2026-01-11T10:13:11.243793Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, sqlite3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Safety: Avoid overlapping plots from repeated runs / \u5b89\u5168\u63aa\u65bd\uff1a\u907f\u514d\u91cd\u590d\u8fd0\u884c\u5bfc\u81f4\u56fe\u50cf\u91cd\u53e0\n",
    "plt.close('all')\n",
    "\n",
    "# Directories / \u76ee\u5f55\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# DB Connection (this section can be removed if 'conn' already exists) / \u6570\u636e\u5e93\u8fde\u63a5\uff08\u82e5 conn \u5df2\u5b58\u5728\u53ef\u5220\u9664\u6b64\u6bb5\uff09\n",
    "# Utility: row counter / \u5de5\u5177\uff1a\u884c\u6570\u8ba1\u6570\n",
    "def n_rows(table):\n",
    "    return pd.read_sql(f\"SELECT COUNT(*) AS n FROM [{table}];\", conn)['n'].iat[0]\n",
    "\n",
    "# -- Before / -- \u4e4b\u524d\n",
    "before = {\n",
    "    'bookings':        n_rows('bookings'),\n",
    "    'tickets':         n_rows('tickets'),\n",
    "    'ticket_flights':  n_rows('ticket_flights'),\n",
    "    'flights':         n_rows('flights'),\n",
    "    'boarding_passes': n_rows('boarding_passes'),\n",
    "    'aircrafts_data':  n_rows('aircrafts_data'),\n",
    "    'airports_data':   n_rows('airports_data'),\n",
    "    'seats':           n_rows('seats'),\n",
    "}\n",
    "\n",
    "# -- After: Only 'Arrived' flights, maintaining join integrity / -- \u4e4b\u540e\uff1a\u4ec5\u4fdd\u7559\u5df2\u5230\u8fbe\u822a\u73ed\uff0c\u4fdd\u6301\u8fde\u63a5\u5b8c\u6574\u6027\n",
    "flights_after_n = pd.read_sql(\"\"\"\n",
    "    SELECT COUNT(*) AS n FROM flights WHERE UPPER(status)='ARRIVED';\n",
    "\"\"\", conn)['n'].iat[0]\n",
    "\n",
    "tf_after_n = pd.read_sql(\"\"\"\n",
    "    SELECT COUNT(*) AS n\n",
    "    FROM ticket_flights tf\n",
    "    JOIN flights f ON tf.flight_id=f.flight_id\n",
    "    WHERE UPPER(f.status)='ARRIVED';\n",
    "\"\"\", conn)['n'].iat[0]\n",
    "\n",
    "tickets_after_n = pd.read_sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT t.ticket_no) AS n\n",
    "    FROM tickets t\n",
    "    JOIN ticket_flights tf ON t.ticket_no=tf.ticket_no\n",
    "    JOIN flights f ON tf.flight_id=f.flight_id\n",
    "    WHERE UPPER(f.status)='ARRIVED';\n",
    "\"\"\", conn)['n'].iat[0]\n",
    "\n",
    "bookings_after_n = pd.read_sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT b.book_ref) AS n\n",
    "    FROM bookings b\n",
    "    JOIN tickets t ON b.book_ref=t.book_ref\n",
    "    JOIN ticket_flights tf ON t.ticket_no=tf.ticket_no\n",
    "    JOIN flights f ON tf.flight_id=f.flight_id\n",
    "    WHERE UPPER(f.status)='ARRIVED';\n",
    "\"\"\", conn)['n'].iat[0]\n",
    "\n",
    "bp_after_n = pd.read_sql(\"\"\"\n",
    "    SELECT COUNT(*) AS n\n",
    "    FROM boarding_passes bp\n",
    "    JOIN ticket_flights tf ON bp.ticket_no=tf.ticket_no AND bp.flight_id=tf.flight_id\n",
    "    JOIN flights f ON tf.flight_id=f.flight_id\n",
    "    WHERE UPPER(f.status)='ARRIVED';\n",
    "\"\"\", conn)['n'].iat[0]\n",
    "\n",
    "ap_after_n = pd.read_sql(\"\"\"\n",
    "    SELECT COUNT(*) AS n\n",
    "    FROM airports_data a\n",
    "    WHERE a.airport_code IN (\n",
    "        SELECT departure_airport FROM flights WHERE UPPER(status)='ARRIVED'\n",
    "        UNION\n",
    "        SELECT arrival_airport FROM flights WHERE UPPER(status)='ARRIVED'\n",
    "    );\n",
    "\"\"\", conn)['n'].iat[0]\n",
    "\n",
    "ac_after_n = pd.read_sql(\"\"\"\n",
    "    SELECT COUNT(*) AS n\n",
    "    FROM aircrafts_data a\n",
    "    WHERE a.aircraft_code IN (\n",
    "        SELECT DISTINCT aircraft_code FROM flights WHERE UPPER(status)='ARRIVED'\n",
    "    );\n",
    "\"\"\", conn)['n'].iat[0]\n",
    "\n",
    "seats_after_n = pd.read_sql(\"\"\"\n",
    "    SELECT COUNT(*) AS n\n",
    "    FROM seats s\n",
    "    WHERE s.aircraft_code IN (\n",
    "        SELECT DISTINCT aircraft_code FROM flights WHERE UPPER(status)='ARRIVED'\n",
    "    );\n",
    "\"\"\", conn)['n'].iat[0]\n",
    "\n",
    "after = {\n",
    "    'bookings':        bookings_after_n,\n",
    "    'tickets':         tickets_after_n,\n",
    "    'ticket_flights':  tf_after_n,\n",
    "    'flights':         flights_after_n,\n",
    "    'boarding_passes': bp_after_n,\n",
    "    'aircrafts_data':  ac_after_n,\n",
    "    'airports_data':   ap_after_n,\n",
    "    'seats':           seats_after_n,\n",
    "}\n",
    "\n",
    "reasons = {\n",
    " 'flights':         \"Filter status='ARRIVED'\",\n",
    " 'ticket_flights':  \"Inner join to Arrived flights (flight_id)\",\n",
    " 'tickets':         \"Keep tickets linked to retained segments\",\n",
    " 'bookings':        \"Keep bookings linked to retained tickets\",\n",
    " 'boarding_passes': \"Keep passes linked to retained segments\",\n",
    " 'aircrafts_data':  \"Models used by retained flights\",\n",
    " 'airports_data':   \"Airports used by retained flights\",\n",
    " 'seats':           \"Seats for retained aircraft models\",\n",
    "}\n",
    "\n",
    "order = ['bookings','tickets','ticket_flights','flights','boarding_passes','aircrafts_data','airports_data','seats']\n",
    "rows = []\n",
    "for t in order:\n",
    "    b, a = int(before[t]), int(after[t])\n",
    "    d = a - b\n",
    "    p = (d / b * 100.0) if b else 0.0\n",
    "    rows.append([t, b, a, d, round(p, 1), reasons[t]])\n",
    "\n",
    "tbl = pd.DataFrame(rows, columns=[\n",
    "    \"Table Name\",\"Rows Before Selection\",\"Rows After Selection\",\"\u0394 Rows\",\"\u0394 %\",\"Primary Reason for Change\"\n",
    "])\n",
    "\n",
    "# -- Export to CSV / -- \u5bfc\u51fa\u4e3a CSV\n",
    "csv_path = TAB_DIR / \"table_3_1_1_selection_impact.csv\"\n",
    "tbl.to_csv(csv_path, index=False); print(\"Saved CSV:\", _rel(csv_path))\n",
    "\n",
    "# -- Pretty-print in Notebook (cross-version safe) / -- Notebook \u7f8e\u5316\u8f93\u51fa\uff08\u8de8\u7248\u672c\u517c\u5bb9\uff09\n",
    "def _fmt_thousands(x):\n",
    "    return f\"{x:,}\" if pd.notna(x) and isinstance(x,(int,np.integer)) else x\n",
    "\n",
    "tbl_show = tbl.copy()\n",
    "for c in [\"Rows Before Selection\",\"Rows After Selection\",\"\u0394 Rows\"]:\n",
    "    tbl_show[c] = tbl_show[c].map(_fmt_thousands)\n",
    "tbl_show[\"\u0394 %\"] = tbl_show[\"\u0394 %\"].map(lambda v: f\"{v:+.1f}%\")\n",
    "\n",
    "try:\n",
    "    sty = (tbl_show.style\n",
    "           .set_table_styles([{'selector':'th','props':'font-weight:bold; text-align:center;'},\n",
    "                              {'selector':'td','props':'text-align:center;'}])\n",
    "           .set_properties(**{'text-align':'center'}))\n",
    "    # Compatible index hiding for different pandas versions / \u517c\u5bb9\u4e0d\u540c pandas \u7248\u672c\u7684\u7d22\u5f15\u9690\u85cf\n",
    "    if hasattr(sty, \"hide_index\"):\n",
    "        sty = sty.hide_index()\n",
    "    else:\n",
    "        sty = sty.hide(axis=\"index\")   # pandas >= 2.1\n",
    "    # Add a light red background to the \"\u0394 %\" column to quickly identify changes / \u4e3a\u201c\u0394 %\u201d\u5217\u52a0\u6d45\u7ea2\u5e95\u4ee5\u5feb\u901f\u8bc6\u522b\u53d8\u5316\n",
    "    sty = sty.apply(lambda s: [f\"background-color:{NEG_BG_COLOR}\" if s.name==\"\u0394 %\" and str(v).startswith('-') else '' for v in s], axis=0)\n",
    "    display(sty)\n",
    "except Exception as e:\n",
    "    print(\"Note: Styler display skipped ->\", e)\n",
    "    # Fallback to a plain HTML table, which can still be screenshotted/copied / \u56de\u9000\u4e3a\u7eaf HTML \u8868\uff08\u4ecd\u53ef\u622a\u56fe/\u590d\u5236\uff09\n",
    "    display(HTML(tbl_show.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa85c5d-35a8-4533-9746-761ba1de6175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:11.245592Z",
     "iopub.status.busy": "2026-01-11T10:13:11.245448Z",
     "iopub.status.idle": "2026-01-11T10:13:11.250193Z",
     "shell.execute_reply": "2026-01-11T10:13:11.249968Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, sqlite3, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Folders / \u6587\u4ef6\u5939\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Shared database connection (initialized in setup) / \u5171\u4eab\u6570\u636e\u5e93\u8fde\u63a5\uff08\u5df2\u5728\u521d\u59cb\u5316\u4e2d\u5efa\u7acb\uff09\n",
    "def _fmt_thousands(x):\n",
    "    try:\n",
    "        return f\"{int(x):,}\"\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def show_table(df: pd.DataFrame, title: str, csv_filename: str,\n",
    "               thousands_cols=None, percent_cols=None, hide_index=True):\n",
    "    \"\"\"\n",
    "    Display a clean, styled table in Notebook + save CSV.\n",
    "    (No PNG export by design.)\n",
    "    \"\"\"\n",
    "    thousands_cols = thousands_cols or []\n",
    "    percent_cols   = percent_cols or []\n",
    "\n",
    "    # copy & format / \u590d\u5236\u5e76\u683c\u5f0f\u5316\n",
    "    t = df.copy()\n",
    "    for c in thousands_cols:\n",
    "        if c in t.columns:\n",
    "            t[c] = t[c].map(_fmt_thousands)\n",
    "    for c in percent_cols:\n",
    "        if c in t.columns:\n",
    "            t[c] = t[c].map(lambda v: f\"{v:.1f}%\" if pd.notna(v) else \"\")\n",
    "\n",
    "    # CSV / CSV \u6587\u4ef6\n",
    "    out_csv = TAB_DIR/csv_filename\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved CSV -> {out_csv}\")\n",
    "\n",
    "    # Styled display (\u517c\u5bb9\u4e0d\u540c pandas \u7248\u672c)\n",
    "    try:\n",
    "        sty = (t.style\n",
    "               .set_table_styles([{'selector':'th','props':'font-weight:bold; text-align:center;'},\n",
    "                                  {'selector':'td','props':'text-align:center;'}])\n",
    "               .set_properties(**{'text-align':'center'}))\n",
    "        if hide_index and hasattr(sty, \"hide_index\"):\n",
    "            sty = sty.hide_index()\n",
    "        elif hide_index:\n",
    "            sty = sty.hide(axis=\"index\")  # pandas \u22652.1\n",
    "        print(title)\n",
    "        display(sty)\n",
    "    except Exception as e:\n",
    "        print(\"Note: Styler display skipped ->\", e)\n",
    "        display(HTML(t.to_html(index=not hide_index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa1b9c-4793-47cd-873b-ab53910d792d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:11.251813Z",
     "iopub.status.busy": "2026-01-11T10:13:11.251583Z",
     "iopub.status.idle": "2026-01-11T10:13:11.619768Z",
     "shell.execute_reply": "2026-01-11T10:13:11.619384Z"
    }
   },
   "outputs": [],
   "source": [
    "candidate_cols = {\n",
    "    'flights': ['actual_departure','actual_arrival','scheduled_departure','scheduled_arrival'],\n",
    "    'bookings': ['book_date'],\n",
    "    'airports_data': ['timezone','airport_name'],\n",
    "    'aircrafts_data': ['model']\n",
    "}\n",
    "\n",
    "def existing_cols(table):\n",
    "    cols = pd.read_sql(f\"PRAGMA table_info([{table}]);\", conn)['name'].tolist()\n",
    "    return [c for c in candidate_cols.get(table, []) if c in cols]\n",
    "\n",
    "rows = []\n",
    "for t in ['flights','bookings','airports_data','aircrafts_data']:\n",
    "    for c in existing_cols(t):\n",
    "        n = pd.read_sql(f'SELECT COUNT(*) AS n FROM [{t}] WHERE [{c}]=\"\\\\N\";', conn)['n'].iat[0]\n",
    "        tot = pd.read_sql(f'SELECT COUNT(*) AS n FROM [{t}];', conn)['n'].iat[0]\n",
    "        rate = 100*n/tot if tot else 0.0\n",
    "        rows.append([t, c, n, rate, tot])\n",
    "\n",
    "tbl_321 = pd.DataFrame(rows, columns=[\n",
    "    'Table','Column','Rows with \"\\\\N\" (before)','Rate % (before)','Table Rows'\n",
    "])\n",
    "\n",
    "# \u5728\u5185\u5b58\u4e2d\u66ff\u6362\uff08\u7528\u4e8e\u540e\u7eed\u5206\u6790\uff1b\u4e0d\u6539\u539f DB\uff09\n",
    "# \u6ce8\u610f\uff1a\u8fd9\u91cc\u53ea\u662f\u6f14\u793a\u66ff\u6362\u4e3a NaN\uff0c\u9a8c\u8bc1 after=0 \u7684\u6548\u679c\n",
    "after_rows = []\n",
    "for t in tbl_321['Table'].unique():\n",
    "    cols = tbl_321.loc[tbl_321['Table']==t, 'Column'].tolist()\n",
    "    if not cols: continue\n",
    "    df = pd.read_sql(f\"SELECT {', '.join([f'[{c}]' for c in cols])} FROM [{t}];\", conn).replace({'\\\\N': pd.NA})\n",
    "    for c in cols:\n",
    "        after_rows.append([t, c, int((df[c] == '\\\\N').sum(skipna=True))])\n",
    "tbl_after = pd.DataFrame(after_rows, columns=['Table','Column','Rows with \"\\\\N\" (after)'])\n",
    "\n",
    "tbl_321 = (tbl_321.merge(tbl_after, on=['Table','Column'], how='left')\n",
    "                    .fillna({'Rows with \"\\\\N\" (after)':0}))\n",
    "\n",
    "show_table(\n",
    "    tbl_321,\n",
    "    title=\"Table 3.2.1 \u2013 Sentinel Value Cleaning Summary\",\n",
    "    csv_filename=\"table_3_2_1_sentinel_cleaning.csv\",\n",
    "    thousands_cols=['Rows with \"\\\\N\" (before)','Rows with \"\\\\N\" (after)','Table Rows'],\n",
    "    percent_cols=['Rate % (before)']\n",
    ")\n",
    "\n",
    "# Report-ready sentence / \u2014\u2014 \u62a5\u544a\u53ef\u7c98\u8d34\u53e5\u5b50\n",
    "tot_before = int(tbl_321['Rows with \"\\\\N\" (before)'].sum())\n",
    "print(f\"Report snippet: Sentinel '\\\\N' detected = {tot_before:,}; all converted to NaN in memory (post = 0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efb050-7939-4e38-b500-1821d8bf8a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:11.621411Z",
     "iopub.status.busy": "2026-01-11T10:13:11.621304Z",
     "iopub.status.idle": "2026-01-11T10:13:11.639668Z",
     "shell.execute_reply": "2026-01-11T10:13:11.639375Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_json_en(sr: pd.Series):\n",
    "    def _x(s):\n",
    "        if pd.isna(s) or s == '\\\\N': return pd.NA\n",
    "        try:\n",
    "            return json.loads(s).get('en', pd.NA)\n",
    "        except Exception:\n",
    "            return pd.NA\n",
    "    return sr.apply(_x)\n",
    "\n",
    "ac = pd.read_sql(\"SELECT aircraft_code, model FROM aircrafts_data;\", conn)\n",
    "ap = pd.read_sql(\"SELECT airport_code, airport_name FROM airports_data;\", conn)\n",
    "ac['model_en'] = get_json_en(ac['model'])\n",
    "ap['airport_name_en'] = get_json_en(ap['airport_name'])\n",
    "\n",
    "tbl_322 = pd.DataFrame([\n",
    "    ['aircrafts_data','model','model_en', len(ac), int(ac['model_en'].notna().sum()),\n",
    "     round(100*ac['model_en'].notna().mean(), 1)],\n",
    "    ['airports_data','airport_name','airport_name_en', len(ap), int(ap['airport_name_en'].notna().sum()),\n",
    "     round(100*ap['airport_name_en'].notna().mean(), 1)]\n",
    "], columns=['Table','Raw JSON Field','Parsed Column','Rows','Parsed Rows','Success %'])\n",
    "\n",
    "show_table(\n",
    "    tbl_322,\n",
    "    title=\"Table 3.2.2 \u2013 JSON Parsing Success Rate\",\n",
    "    csv_filename=\"table_3_2_2_json_parsing_success.csv\",\n",
    "    thousands_cols=['Rows','Parsed Rows'],\n",
    "    percent_cols=['Success %']\n",
    ")\n",
    "\n",
    "print(f\"Report snippet: JSON parsing success \u2013 aircraft models {tbl_322.loc[0,'Success %']}%, \"\n",
    "      f\"airport names {tbl_322.loc[1,'Success %']}%. Raw JSON columns can be dropped in analytical view.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb7521-eece-4916-b095-465a4b8ec567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:11.641903Z",
     "iopub.status.busy": "2026-01-11T10:13:11.641613Z",
     "iopub.status.idle": "2026-01-11T10:13:14.622877Z",
     "shell.execute_reply": "2026-01-11T10:13:14.622328Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_utc(s):\n",
    "    return pd.to_datetime(s, errors='coerce', utc=True)\n",
    "\n",
    "fl = pd.read_sql(\"\"\"\n",
    "    SELECT flight_id, scheduled_departure, scheduled_arrival, status\n",
    "    FROM flights\n",
    "    WHERE UPPER(status)='ARRIVED';\n",
    "\"\"\", conn)\n",
    "fl['dep_utc'] = to_utc(fl['scheduled_departure'])\n",
    "fl['arr_utc'] = to_utc(fl['scheduled_arrival'])\n",
    "fl['duration_min'] = (fl['arr_utc'] - fl['dep_utc']).dt.total_seconds()/60\n",
    "\n",
    "t = pd.read_sql(\"SELECT ticket_no, book_ref FROM tickets;\", conn)\n",
    "b = pd.read_sql(\"SELECT book_ref, book_date FROM bookings;\", conn)\n",
    "b['book_utc'] = to_utc(b['book_date'])\n",
    "tf = pd.read_sql(\"SELECT ticket_no, flight_id FROM ticket_flights;\", conn)\n",
    "\n",
    "seg = (tf.merge(fl[['flight_id','dep_utc']], on='flight_id', how='inner')\n",
    "         .merge(t, on='ticket_no', how='inner')\n",
    "         .merge(b[['book_ref','book_utc']], on='book_ref', how='inner'))\n",
    "seg['lead_time_min'] = (seg['dep_utc'] - seg['book_utc']).dt.total_seconds()/60\n",
    "\n",
    "neg_dur = int((fl['duration_min'] < 0).sum())\n",
    "neg_lt  = int((seg['lead_time_min'] < 0).sum())\n",
    "\n",
    "# \u6700\u7ec8\u7b56\u7565\uff1a\u8bbe\u4e3a NaN \u4fdd\u7559\u8bb0\u5f55\n",
    "fl.loc[fl['duration_min'] < 0, 'duration_min'] = pd.NA\n",
    "seg.loc[seg['lead_time_min'] < 0, 'lead_time_min'] = pd.NA\n",
    "\n",
    "tbl_neg = pd.DataFrame({\n",
    "    'Metric':['duration_min < 0','lead_time_min < 0'],\n",
    "    'Rows':[neg_dur, neg_lt],\n",
    "    'Action':['set to NaN (kept record)','set to NaN (kept record)']\n",
    "})\n",
    "show_table(\n",
    "    tbl_neg, \n",
    "    title=\"Table 3.2.x \u2013 Invalid Time Metrics After Harmonisation\",\n",
    "    csv_filename=\"table_3_2_negative_metrics.csv\",\n",
    "    thousands_cols=['Rows']\n",
    ")\n",
    "\n",
    "print(f\"Report snippet: After timezone harmonisation: negative durations = {neg_dur:,}; \"\n",
    "      f\"negative lead times = {neg_lt:,}. Policy: set invalid computed values to NaN and keep records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788e90b-f08c-44e3-9419-6c3c96a3595a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:14.625469Z",
     "iopub.status.busy": "2026-01-11T10:13:14.625318Z",
     "iopub.status.idle": "2026-01-11T10:13:15.733420Z",
     "shell.execute_reply": "2026-01-11T10:13:15.733034Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "fc = pd.read_sql(\"SELECT fare_conditions FROM ticket_flights;\", conn)\n",
    "\n",
    "# Before / \u4e4b\u524d\n",
    "before = (fc['fare_conditions'].astype(str)\n",
    "          .value_counts(dropna=False).sort_index())\n",
    "\n",
    "# After (cleanup) / \u4e4b\u540e\uff08\u6e05\u6d17\u540e\uff09\n",
    "def clean_fc(s):\n",
    "    if pd.isna(s): return pd.NA\n",
    "    s = str(s).strip().lower()\n",
    "    if s in ['economy','econ','eco']:   return 'Economy'\n",
    "    if s in ['comfort','premium economy','prem eco','prem_econ','premeco']: return 'Comfort'\n",
    "    if s in ['business','biz','bus']:   return 'Business'\n",
    "    return s.title()\n",
    "\n",
    "fc['fare_clean'] = fc['fare_conditions'].apply(clean_fc)\n",
    "after = (fc['fare_clean'].value_counts(dropna=False)\n",
    "         .reindex(['Economy','Comfort','Business',pd.NA], fill_value=0))\n",
    "\n",
    "# Plot -> PNG only (no table) / \u4ec5\u4fdd\u5b58 PNG \u56fe\uff08\u4e0d\u8f93\u51fa\u8868\uff09\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5), sharey=True)\n",
    "axes[0].bar(before.index.astype(str), before.values)\n",
    "axes[0].set_title(\"Fare Conditions (Before)\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].set_ylabel(\"Row count\")\n",
    "\n",
    "axes[1].bar(['Economy','Comfort','Business'], after[['Economy','Comfort','Business']].values)\n",
    "axes[1].set_title(\"Fare Conditions (After)\")\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "fig.suptitle(\"Fare Conditions Before and After Normalisation\", y=1.02)\n",
    "fig.tight_layout()\n",
    "png = FIG_DIR/\"figure_3_2_1_fare_conditions_before_after.png\"\n",
    "fig.savefig(png, dpi=300, bbox_inches='tight'); plt.show()\n",
    "print(\"Saved FIGURE ->\", png)\n",
    "\n",
    "tot = int(len(fc))\n",
    "eco, comf, biz = [int(after.get(k,0)) for k in ['Economy','Comfort','Business']]\n",
    "print(f\"Report snippet: After normalisation \u2013 Economy {eco:,}, Comfort {comf:,}, Business {biz:,} (total {tot:,}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d497fe-afc1-4629-aa4c-da4f0b795f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:15.735227Z",
     "iopub.status.busy": "2026-01-11T10:13:15.734983Z",
     "iopub.status.idle": "2026-01-11T10:13:20.017134Z",
     "shell.execute_reply": "2026-01-11T10:13:20.016723Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, sqlite3\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dirs / \u76ee\u5f55\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def to_utc(s):\n",
    "    return pd.to_datetime(s, errors='coerce', utc=True)\n",
    "\n",
    "def get_json_en(sr: pd.Series):\n",
    "    def _x(s):\n",
    "        if pd.isna(s) or s == '\\\\N': return pd.NA\n",
    "        try:\n",
    "            return json.loads(s).get('en', pd.NA)\n",
    "        except Exception:\n",
    "            return pd.NA\n",
    "    return sr.apply(_x)\n",
    "\n",
    "# Base tables / \u2014\u2014 \u57fa\u7840\u8868\n",
    "fl = pd.read_sql(\"\"\"\n",
    "    SELECT flight_id, departure_airport, arrival_airport, scheduled_departure, scheduled_arrival,\n",
    "           status, aircraft_code\n",
    "    FROM flights\n",
    "    WHERE UPPER(status)='ARRIVED';\n",
    "\"\"\", conn)\n",
    "\n",
    "tf = pd.read_sql(\"SELECT ticket_no, flight_id, fare_conditions, amount FROM ticket_flights;\", conn)\n",
    "t  = pd.read_sql(\"SELECT ticket_no, book_ref FROM tickets;\", conn)\n",
    "b  = pd.read_sql(\"SELECT book_ref, book_date FROM bookings;\", conn)\n",
    "ac = pd.read_sql(\"SELECT aircraft_code, model, range FROM aircrafts_data;\", conn)\n",
    "ap = pd.read_sql(\"SELECT airport_code, airport_name, timezone, coordinates FROM airports_data;\", conn)\n",
    "seats = pd.read_sql(\"SELECT aircraft_code, seat_no FROM seats;\", conn)\n",
    "\n",
    "# \u2014\u2014 \u65f6\u95f4\u7edf\u4e00\u4e3a UTC + \u6838\u5fc3\u65f6\u95f4\u7279\u5f81\n",
    "fl['dep_utc'] = to_utc(fl['scheduled_departure'])\n",
    "fl['arr_utc'] = to_utc(fl['scheduled_arrival'])\n",
    "fl['Sched_Flight_Duration_Minutes'] = (fl['arr_utc'] - fl['dep_utc']).dt.total_seconds()/60\n",
    "# \u5f02\u5e38\u7f6e\u7a7a\uff08\u4e0e 3.2.2.D \u7b56\u7565\u4e00\u81f4\uff09\n",
    "fl.loc[fl['Sched_Flight_Duration_Minutes'] < 0, 'Sched_Flight_Duration_Minutes'] = pd.NA\n",
    "\n",
    "b['book_utc'] = to_utc(b['book_date'])\n",
    "\n",
    "# \u2014\u2014 aircraft/model \u6587\u672c\u6e05\u6d17\uff08\u82f1\u6587\u6807\u7b7e\uff09\n",
    "ac['Aircraft_Model_EN'] = get_json_en(ac['model'])\n",
    "\n",
    "# \u2014\u2014 \u5ea7\u4f4d\u6570\uff08\u7528 seats \u660e\u7ec6\u8ba1\u6570\uff09\n",
    "seat_cnt = seats.groupby('aircraft_code', as_index=False)['seat_no'].nunique()\n",
    "seat_cnt = seat_cnt.rename(columns={'seat_no':'Seats_Per_Aircraft'})\n",
    "\n",
    "# \u2014\u2014 \u4e1a\u52a1\u89c4\u5219\uff1aWidebody \n",
    "# \u89c4\u5219A\uff1a\u5ea7\u4f4d\u6570 \u2265 240 \u89c6\u4e3a\u5bbd\u4f53\n",
    "# \u89c4\u5219B\uff1a\u673a\u578b\u540d\u79f0\u5305\u542b\u4ee5\u4e0b\u5173\u952e\u5b57\u4e5f\u89c6\u4e3a\u5bbd\u4f53\uff08\u4fdd\u9669\uff09\n",
    "WIDEBODY_KEYWORDS = ['777','787','747','767','a330','a340','a350','a380','il-96']\n",
    "SEAT_THRESHOLD = 240\n",
    "\n",
    "ac2 = (ac.merge(seat_cnt, on='aircraft_code', how='left')\n",
    "         .assign(Seats_Per_Aircraft=lambda d: d['Seats_Per_Aircraft'].fillna(0).astype(int)))\n",
    "def is_widebody(row):\n",
    "    seats_ok = row['Seats_Per_Aircraft'] >= SEAT_THRESHOLD\n",
    "    name = str(row['Aircraft_Model_EN']).lower()\n",
    "    kw_ok = any(k in name for k in WIDEBODY_KEYWORDS)\n",
    "    return bool(seats_ok or kw_ok)\n",
    "ac2['Is_Widebody'] = ac2.apply(is_widebody, axis=1)\n",
    "\n",
    "# \u2014\u2014 \u62fc\u63a5\u4e3a \u201csegment\u201d \u7c92\u5ea6\u6570\u636e\n",
    "seg = (tf.merge(fl[['flight_id','departure_airport','arrival_airport','dep_utc',\n",
    "                    'Sched_Flight_Duration_Minutes','aircraft_code']],\n",
    "                on='flight_id', how='inner')\n",
    "         .merge(t[['ticket_no','book_ref']], on='ticket_no', how='inner')\n",
    "         .merge(b[['book_ref','book_utc']], on='book_ref', how='inner')\n",
    "         .merge(ac2[['aircraft_code','Aircraft_Model_EN','Seats_Per_Aircraft','Is_Widebody']],\n",
    "                on='aircraft_code', how='left'))\n",
    "\n",
    "# \u2014\u2014 Route & lead time / \u2014\u2014 \u8def\u7ebf\u4e0e\u63d0\u524d\u671f\n",
    "seg['Route_Code'] = seg['departure_airport'] + '-' + seg['arrival_airport']\n",
    "seg['Booking_Lead_Time_Days'] = (seg['dep_utc'] - seg['book_utc']).dt.total_seconds()/86400\n",
    "seg.loc[seg['Booking_Lead_Time_Days'] < 0, 'Booking_Lead_Time_Days'] = pd.NA  # \u4e0e 3.2 \u7b56\u7565\u4e00\u81f4\n",
    "\n",
    "# \u2014\u2014 Seasonality / \u2014\u2014 \u5b63\u8282\u6027\n",
    "seg['Departure_DOW']  = seg['dep_utc'].dt.dayofweek   # 0=Mon\n",
    "seg['Departure_Hour'] = seg['dep_utc'].dt.hour\n",
    "seg['Is_Weekend']     = seg['Departure_DOW'].isin([5,6]).astype(int)\n",
    "\n",
    "# \u2014\u2014 Cabin/\u7968\u8231\n",
    "def clean_fc(s):\n",
    "    if pd.isna(s): return pd.NA\n",
    "    s = str(s).strip().lower()\n",
    "    if s in ['economy','econ','eco']:   return 'Economy'\n",
    "    if s in ['comfort','premium economy','prem eco','prem_econ','premeco']: return 'Comfort'\n",
    "    if s in ['business','biz','bus']:   return 'Business'\n",
    "    return s.title()\n",
    "seg['Fare_Class'] = seg['fare_conditions'].apply(clean_fc)\n",
    "seg['Fare_Class_Ordinal'] = seg['Fare_Class'].map({'Economy':0,'Comfort':1,'Business':2})\n",
    "seg['Is_Premium_Cabin']   = seg['Fare_Class'].isin(['Comfort','Business']).astype(int)\n",
    "\n",
    "# Optional: long-haul flag (canvas/rules adjustable) / \u2014\u2014 \u53ef\u9009\uff1a\u957f\u822a\u7a0b\u6807\u8bb0\uff08\u753b\u5e03/\u89c4\u5219\u53ef\u6539\uff09\n",
    "seg['Is_LongHaul'] = (seg['Sched_Flight_Duration_Minutes'] >= 240).astype(int)\n",
    "\n",
    "# Keep only columns needed for display (raw + new features) / \u4ec5\u4fdd\u7559\u5c55\u793a\u6240\u9700\u5217\uff08\u539f\u59cb + \u65b0\u7279\u5f81\uff09\n",
    "cols = ['ticket_no','book_ref','flight_id','Route_Code',\n",
    "        'Sched_Flight_Duration_Minutes','Booking_Lead_Time_Days',\n",
    "        'Departure_DOW','Departure_Hour','Is_Weekend',\n",
    "        'Fare_Class','Fare_Class_Ordinal','Is_Premium_Cabin',\n",
    "        'aircraft_code','Aircraft_Model_EN','Seats_Per_Aircraft','Is_Widebody',\n",
    "        'amount']\n",
    "seg = seg[cols].copy()\n",
    "\n",
    "print(\"Segment table is ready:\", seg.shape)\n",
    "seg.head()\n",
    "\n",
    "# Save feature matrix/target for downstream notebooks / \u4fdd\u5b58\u7279\u5f81\u77e9\u9635\u4e0e\u76ee\u6807\u4f9b\u4e0b\u6e38 notebook \u4f7f\u7528\n",
    "out_dir = OUT_DIR\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "seg_clean = seg.copy()\n",
    "seg_clean[\"amount\"] = pd.to_numeric(seg_clean[\"amount\"], errors=\"coerce\")\n",
    "seg_clean = seg_clean.dropna(subset=[\"amount\"])\n",
    "X_features = seg_clean.drop(columns=[\"amount\"])\n",
    "y_target = seg_clean[[\"amount\"]]\n",
    "X_features.to_parquet(out_dir/\"X_features.parquet\", index=False)\n",
    "y_target.to_parquet(out_dir/\"y_target.parquet\", index=False)\n",
    "print(\"Saved:\", out_dir/\"X_features.parquet\", out_dir/\"y_target.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd7a20-c16c-4d4e-8080-3f9782a2814c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:20.023901Z",
     "iopub.status.busy": "2026-01-11T10:13:20.023687Z",
     "iopub.status.idle": "2026-01-11T10:13:38.444119Z",
     "shell.execute_reply": "2026-01-11T10:13:38.442696Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4.2.1 \u2014 Compare target transformations & export a report-ready table + text / 4.2.1 \u2014 \u6bd4\u8f83\u76ee\u6807\u53d8\u6362\u5e76\u5bfc\u51fa\u62a5\u544a\u7528\u8868\u683c\u4e0e\u6587\u672c\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_any_dtype\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "X_path = find_path(\"X_features.parquet\")\n",
    "y_path = find_path(\"y_target.parquet\")\n",
    "\n",
    "X = pd.read_parquet(X_path)\n",
    "y = pd.read_parquet(y_path).squeeze()\n",
    "y = pd.to_numeric(y, errors=\"coerce\")\n",
    "\n",
    "dt_cols  = [c for c in X.columns if is_datetime64_any_dtype(X[c])]\n",
    "num_cols = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "cat_cols = [c for c in X.columns if (c not in num_cols) and (c not in dt_cols)]\n",
    "\n",
    "# \u4fdd\u5b88\u5904\u7406\uff1a\u5220\u9664 datetime\uff0c\u5e03\u5c14\u8f6c 0/1\n",
    "if dt_cols:\n",
    "    X = X.drop(columns=dt_cols)\n",
    "for c in X.select_dtypes(include=\"bool\").columns:\n",
    "    X[c] = X[c].astype(int)\n",
    "\n",
    "# \u53ef\u9009\u62bd\u6837\uff08\u82e5\u5185\u5b58\u7d27\u5f20\u628a N \u8c03\u5c0f\uff1b\u60f3\u7528\u5168\u91cf\u5c31\u8bbe\u7f6e N = len(X)\uff09\n",
    "N = min(150_000, len(X))\n",
    "idx = X.sample(n=N, random_state=42).index\n",
    "X, y = X.loc[idx].copy(), y.loc[idx].copy()\n",
    "\n",
    "# OHE \u8f93\u51fa\u7a00\u758f\uff0c\u9700\u8981\u6570\u503c\u4fa7\u914d\u5408 StandardScaler(with_mean=False)\uff0c\u56de\u5f52\u5668\u9009 Ridge \u4ee5\u652f\u6301\u7a00\u758f\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"sc\",  StandardScaler(with_mean=False))]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"ohe\", ohe)]), cat_cols)\n",
    "])\n",
    "\n",
    "base_reg = Ridge(alpha=1.0)\n",
    "\n",
    "# Three target transformations / \u4e09\u79cd\u76ee\u6807\u53d8\u6362\n",
    "pipe_X = Pipeline([(\"pre\", pre), (\"reg\", base_reg)])\n",
    "pipe_log = TransformedTargetRegressor(regressor=pipe_X,\n",
    "                                      func=np.log1p, inverse_func=np.expm1)\n",
    "pipe_yj  = TransformedTargetRegressor(regressor=pipe_X,\n",
    "                                      transformer=PowerTransformer(method=\"yeo-johnson\"))\n",
    "\n",
    "models = {\n",
    "    \"Original\": pipe_X,\n",
    "    \"Log\":      pipe_log,\n",
    "    \"Yeo-Johnson\": pipe_yj\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rows = []\n",
    "for name, est in models.items():\n",
    "    scores = cross_validate(\n",
    "        est, X, y,\n",
    "        scoring={\"r2\":\"r2\",\"rmse\":\"neg_root_mean_squared_error\"},\n",
    "        cv=cv, n_jobs=-1, return_train_score=False\n",
    "    )\n",
    "    r2   = scores[\"test_r2\"];        rmse = -scores[\"test_rmse\"]\n",
    "    rows.append({\"Target Transform\": name,\n",
    "                 \"CV folds\": cv.n_splits,\n",
    "                 \"Mean R\u00b2\": r2.mean(), \"Std R\u00b2\": r2.std(),\n",
    "                 \"Mean RMSE\": rmse.mean(), \"Std RMSE\": rmse.std()})\n",
    "\n",
    "tbl = (pd.DataFrame(rows)\n",
    "         .sort_values(\"Mean R\u00b2\", ascending=False)\n",
    "         .reset_index(drop=True))\n",
    "\n",
    "# \u4fdd\u5b58 CSV\n",
    "csv_path = TAB_DIR/\"table_4_2_1_target_transformations.csv\"\n",
    "tbl.to_csv(csv_path, index=False)\n",
    "print(f\"Saved CSV -> {_rel(csv_path)}\\n\")\n",
    "display(tbl.style.format({\"Mean R\u00b2\":\"{:.3f}\",\"Std R\u00b2\":\"{:.3f}\",\n",
    "                          \"Mean RMSE\":\"{:.1f}\",\"Std RMSE\":\"{:.1f}\"}))\n",
    "\n",
    "# \u2014\u2014 \u751f\u6210\u201c\u53ef\u76f4\u63a5\u7c98\u8d34 Word \u7684\u7ed3\u8bba\u6bb5\u201d\uff08\u56fa\u5b9a\u91c7\u7528\u5bf9\u6570\u53d8\u6362\uff1b\u5e76\u62a5\u544a\u4e09\u7ec4\u5e73\u5747\u6307\u6807\uff09\n",
    "def fmt(x, p=3): return f\"{x:.{p}f}\"\n",
    "r0 = tbl.loc[tbl[\"Target Transform\"]==\"Original\"].iloc[0]\n",
    "rL = tbl.loc[tbl[\"Target Transform\"]==\"Log\"].iloc[0]\n",
    "rY = tbl.loc[tbl[\"Target Transform\"]==\"Yeo-Johnson\"].iloc[0]\n",
    "\n",
    "text = (\n",
    "    \"Target transformation comparison (5-fold CV, Ridge baseline). \"\n",
    "    f\"Original target achieved mean R\u00b2={fmt(r0['Mean R\u00b2'])}, RMSE={fmt(r0['Mean RMSE'],1)}. \"\n",
    "    f\"Log transform achieved mean R\u00b2={fmt(rL['Mean R\u00b2'])}, RMSE={fmt(rL['Mean RMSE'],1)}. \"\n",
    "    f\"Yeo\u2013Johnson achieved mean R\u00b2={fmt(rY['Mean R\u00b2'])}, RMSE={fmt(rY['Mean RMSE'],1)}. \"\n",
    "    \"Decision: adopt the Log transform for Chapters 5\u20138 for its accuracy and interpretability; \"\n",
    "    \"apply the unbiased log-normal correction when back-transforming predictions.\"\n",
    ")\n",
    "\n",
    "with open(TAB_DIR/\"section_4_2_1_text.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"\\nReport-ready paragraph (also saved to tables/section_4_2_1_text.txt):\\n\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7402f4e-12e8-44c7-abcf-44fd5cd363ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:38.447074Z",
     "iopub.status.busy": "2026-01-11T10:13:38.446950Z",
     "iopub.status.idle": "2026-01-11T10:13:38.825125Z",
     "shell.execute_reply": "2026-01-11T10:13:38.824584Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4.2.1 \u2014 Two histograms: original vs log1p(target) / 4.2.1 \u2014 \u4e24\u5f20\u76f4\u65b9\u56fe\uff1a\u539f\u59cb vs log1p(target)\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "y_path = find_path(\"y_target.parquet\")\n",
    "y = pd.read_parquet(y_path).squeeze()\n",
    "y = pd.to_numeric(y, errors=\"coerce\").dropna()\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "p = np.quantile(y, 0.995)  # \u88c1\u526a\u53f3\u5c3e\u4ee5\u4fbf\u89c2\u5bdf\u4e3b\u4f53\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4.5))\n",
    "\n",
    "axes[0].hist(y.clip(upper=p), bins=50)\n",
    "axes[0].set_title(\"Original target (total_amount)\")\n",
    "axes[0].set_xlabel(\"Amount (clipped at 99.5th pct)\"); axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(y_log, bins=50)\n",
    "axes[1].set_title(\"Log-transformed target (log1p)\")\n",
    "axes[1].set_xlabel(\"log1p(Amount)\"); axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "fig.suptitle(\"Figure 4.2.1 \u2013 Target Distribution: Before vs After Transform\", y=1.04)\n",
    "plt.tight_layout()\n",
    "\n",
    "png_path = FIG_DIR/\"figure_4_2_1_target_distribution.png\"\n",
    "plt.savefig(png_path, dpi=220, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved PNG -> {_rel(png_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76f45b-f462-4d64-845b-2cebc6b7ff39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T10:13:38.827527Z",
     "iopub.status.busy": "2026-01-11T10:13:38.827370Z",
     "iopub.status.idle": "2026-01-11T10:13:38.830866Z",
     "shell.execute_reply": "2026-01-11T10:13:38.830510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Health check: paths, data objects, parquet existence / \u5065\u5eb7\u68c0\u67e5\uff1a\u8def\u5f84\u3001\u6570\u636e\u5bf9\u8c61\u3001parquet \u662f\u5426\u5b58\u5728\n",
    "import os, pandas as pd\n",
    "\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "need = [\"X_features.parquet\", \"y_target.parquet\", \"master_formatted.parquet\"]\n",
    "for f in need:\n",
    "    print(f\"{f:>28} :\", \"FOUND\" if os.path.exists(f) else \"missing\")\n",
    "\n",
    "# Check if key objects already in memory / \u68c0\u67e5\u5185\u5b58\u91cc\u662f\u5426\u5df2\u6709\u5173\u952e\u5bf9\u8c61\n",
    "for name in [\"master_df\", \"booking_model_df\", \"X\", \"y\"]:\n",
    "    print(f\"{name:>18} in memory:\", name in globals())\n",
    "\n",
    "# \u82e5\u5df2\u5b58\u5728 parquet\uff0c\u67e5\u770b\u57fa\u672c\u4fe1\u606f\n",
    "if os.path.exists(\"X_features.parquet\"):\n",
    "    Xh = pd.read_parquet(\"X_features.parquet\")\n",
    "    print(\"X_features shape:\", Xh.shape)\n",
    "if os.path.exists(\"y_target.parquet\"):\n",
    "    yh = pd.read_parquet(\"y_target.parquet\")\n",
    "    print(\"y_target shape:\", yh.shape, \"; cols:\", list(yh.columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
