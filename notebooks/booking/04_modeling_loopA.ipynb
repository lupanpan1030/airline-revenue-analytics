{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "style"
    ]
   },
   "outputs": [],
   "source": [
    "from airline_revenue_analytics.viz.charts import apply_style, PLOT_COLORS\n",
    "apply_style()\n",
    "PASS_COLOR = \"#D9F2E6\"\n",
    "FAIL_COLOR = \"#FCE4E4\"\n",
    "NEG_BG_COLOR = FAIL_COLOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdf7fa26cb4fd9a70a241b9fb8a374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:46.409353Z",
     "iopub.status.busy": "2026-01-11T09:24:46.409246Z",
     "iopub.status.idle": "2026-01-11T09:24:46.418558Z",
     "shell.execute_reply": "2026-01-11T09:24:46.418211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project paths (booking pipeline)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"pyproject.toml\").exists() and (p / \"src\" / \"airline_revenue_analytics\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "PROJECT_ROOT = REPO_ROOT\n",
    "SRC_ROOT = REPO_ROOT / \"src\"\n",
    "if str(SRC_ROOT) not in sys.path:\n",
    "    sys.path.append(str(SRC_ROOT))\n",
    "\n",
    "from airline_revenue_analytics.config import get_paths\n",
    "\n",
    "PATHS = get_paths(\"booking\")\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "RAW_DIR = PATHS.data_raw\n",
    "DB_PATH = PATHS.db_path\n",
    "OUTPUT_DIR = PATHS.outputs_root\n",
    "FIG_DIR = PATHS.figures\n",
    "TAB_DIR = PATHS.tables\n",
    "ART_DIR = PATHS.artifacts\n",
    "\n",
    "def _rel(p: Path) -> str:\n",
    "    try:\n",
    "        return str(Path(p).resolve().relative_to(REPO_ROOT))\n",
    "    except Exception:\n",
    "        return Path(p).name\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT.name)\n",
    "print(\"DB_PATH:\", _rel(DB_PATH))\n",
    "print(\"OUTPUT_DIR:\", _rel(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Modeling Loop A (Steps 5–7)\n",
    "Goal: train baseline models (Linear vs Tree) on the fixed 70/30 split,\n",
    "save metrics/plots, and export artifacts for the report (Step 7 evidence).\n",
    "Outputs:\n",
    "- `outputs/booking/tables/model_baselines_loopA.csv`\n",
    "- `outputs/booking/tables/table_7_1_predictions_loopA.csv`\n",
    "- `outputs/booking/tables/table_5_2_linear_coefficients.csv`\n",
    "- `outputs/booking/figures/figure_7_1_residuals_vs_fitted_loopA.png`\n",
    "- `outputs/booking/figures/figure_7_1_actual_vs_pred_loopA.png`\n",
    "- `outputs/booking/artifacts/pipe_linear_loopA.joblib`\n",
    "- `outputs/booking/artifacts/pipe_tree_loopA.joblib`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856453a-439d-484c-ab94-4d703f734315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:46.422353Z",
     "iopub.status.busy": "2026-01-11T09:24:46.422274Z",
     "iopub.status.idle": "2026-01-11T09:24:48.448332Z",
     "shell.execute_reply": "2026-01-11T09:24:48.448053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports & paths\n",
    "import sys, pathlib, numpy as np, pandas as pd\n",
    "import matplotlib; import matplotlib.pyplot as plt\n",
    "\n",
    "# ensure \"src\" importable when running from notebooks/\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "OUT_DIR = OUTPUT_DIR\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "TAB_DIR = OUT_DIR / \"tables\"\n",
    "ART_DIR = OUT_DIR / \"artifacts\"\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"FIG_DIR:\", _rel(FIG_DIR))\n",
    "print(\"TAB_DIR:\", _rel(TAB_DIR))\n",
    "print(\"ART_DIR:\", _rel(ART_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da91247-0e45-4e1a-b7ac-9271565af4f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:48.449636Z",
     "iopub.status.busy": "2026-01-11T09:24:48.449489Z",
     "iopub.status.idle": "2026-01-11T09:24:48.711468Z",
     "shell.execute_reply": "2026-01-11T09:24:48.711166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load modeling dataset (from 02) and fixed split membership (from 03)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(TAB_DIR / \"booking_model_df.parquet\")\n",
    "split_ids = pd.read_csv(TAB_DIR / \"table_4_3_1_split_ids.csv\")\n",
    "\n",
    "# Target & features（与 03 保持一致）\n",
    "target = \"log_total_amount\"\n",
    "\n",
    "num_cols = [\n",
    "    \"n_segments\", \"sum_sched_duration_min\", \"avg_sched_duration_min\",\n",
    "    \"max_sched_duration_min\", \"share_premium_cabin\", \"max_cabin_index\",\n",
    "    \"has_longhaul\", \"n_unique_routes\", \"avg_booking_lead_days\"\n",
    "]\n",
    "\n",
    "# ---------- 重建切分（优先用新文件的 index；否则用 book_ref 回填） ----------\n",
    "if \"index\" in split_ids.columns:\n",
    "    idx_train = split_ids.loc[split_ids[\"split\"].eq(\"train\"), \"index\"].to_numpy()\n",
    "    idx_test  = split_ids.loc[~split_ids[\"split\"].eq(\"train\"), \"index\"].to_numpy()\n",
    "else:\n",
    "    # 兼容旧版本：split_ids 只有 ['split','book_ref']，按 book_ref 回找 index\n",
    "    idx_map = df.reset_index()[[\"index\", \"book_ref\"]]\n",
    "    merged  = split_ids.merge(idx_map, on=\"book_ref\", how=\"left\", validate=\"one_to_one\")\n",
    "    idx_train = merged.loc[merged[\"split\"].eq(\"train\"), \"index\"].to_numpy()\n",
    "    idx_test  = merged.loc[~merged[\"split\"].eq(\"train\"), \"index\"].to_numpy()\n",
    "\n",
    "# Categorical: top-K routes（仅训练集统计，避免泄漏）\n",
    "cat_raw = df[\"primary_route_code\"].astype(\"string\").fillna(\"Unknown\")\n",
    "top_k = 20\n",
    "top_routes = cat_raw.loc[idx_train].value_counts().index[:top_k].tolist()\n",
    "df[\"primary_route_code_top\"] = np.where(cat_raw.isin(top_routes), cat_raw, \"Other\")\n",
    "cat_cols = [\"primary_route_code_top\"]\n",
    "\n",
    "# Compose X/y\n",
    "X_all = df[num_cols + cat_cols].copy()\n",
    "y_all = df[target].copy()\n",
    "\n",
    "# 还原出与 03 完全一致的训练/测试集\n",
    "X_train, y_train = X_all.loc[idx_train], y_all.loc[idx_train]\n",
    "X_test,  y_test  = X_all.loc[idx_test],  y_all.loc[idx_test]\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c637b-ec6b-463e-b1b6-e7d1bf3b7658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:48.713126Z",
     "iopub.status.busy": "2026-01-11T09:24:48.713004Z",
     "iopub.status.idle": "2026-01-11T09:24:48.723385Z",
     "shell.execute_reply": "2026-01-11T09:24:48.722984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a fresh preprocessor (fit on TRAIN only via pipeline.fit)\n",
    "# New/old sklearn compatibility for OneHotEncoder parameter\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", ohe)\n",
    "    ]), cat_cols),\n",
    "])\n",
    "\n",
    "pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc100512-8a76-4fc8-8ff9-ceb868724da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:48.724671Z",
     "iopub.status.busy": "2026-01-11T09:24:48.724567Z",
     "iopub.status.idle": "2026-01-11T09:24:49.471246Z",
     "shell.execute_reply": "2026-01-11T09:24:49.470989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train baselines & evaluate (version-robust metrics)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    # make sure 1D arrays\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    # robust RMSE: try modern API first; fallback to sqrt(MSE)\n",
    "    try:\n",
    "        rmse = float(mean_squared_error(y_true, y_pred, squared=False))\n",
    "    except TypeError:\n",
    "        rmse = float(mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    return {\"R2\": r2, \"RMSE\": rmse, \"MAE\": mae}\n",
    "\n",
    "# Linear Regression\n",
    "pipe_lin = Pipeline([(\"pre\", pre), (\"model\", LinearRegression())])\n",
    "pipe_lin.fit(X_train, y_train)\n",
    "pred_lin = pipe_lin.predict(X_test)\n",
    "m_lin = {\"model\": \"Linear\"} | evaluate(y_test, pred_lin)\n",
    "\n",
    "# Decision Tree (simple baseline)\n",
    "pipe_tree = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", DecisionTreeRegressor(random_state=SEED, max_depth=6, min_samples_leaf=50))\n",
    "])\n",
    "pipe_tree.fit(X_train, y_train)\n",
    "pred_tree = pipe_tree.predict(X_test)\n",
    "m_tree = {\"model\": \"DecisionTree\"} | evaluate(y_test, pred_tree)\n",
    "\n",
    "results = pd.DataFrame([m_lin, m_tree]).sort_values(\"RMSE\")\n",
    "results.to_csv(TAB_DIR/\"model_baselines_loopA.csv\", index=False)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60cc96-e41f-4cf6-b089-7046de5aa346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:49.472440Z",
     "iopub.status.busy": "2026-01-11T09:24:49.472342Z",
     "iopub.status.idle": "2026-01-11T09:24:49.477279Z",
     "shell.execute_reply": "2026-01-11T09:24:49.477027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get feature names AFTER OHE fitted on TRAIN\n",
    "ohe_train = pipe_lin.named_steps[\"pre\"].named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "feat_names = num_cols + list(ohe_train.get_feature_names_out(cat_cols))\n",
    "\n",
    "# Linear coefficients correspond to transformed features\n",
    "coefs = pipe_lin.named_steps[\"model\"].coef_\n",
    "coef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coefs})\n",
    "coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
    "coef_df.sort_values(\"abs_coef\", ascending=False).to_csv(TAB_DIR/\"table_5_2_linear_coefficients.csv\", index=False)\n",
    "\n",
    "coef_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d91b7-464e-4a48-8349-450c1104d200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:49.478461Z",
     "iopub.status.busy": "2026-01-11T09:24:49.478369Z",
     "iopub.status.idle": "2026-01-11T09:24:49.739969Z",
     "shell.execute_reply": "2026-01-11T09:24:49.739491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save pipelines (artifacts)\n",
    "joblib.dump(pipe_lin, ART_DIR/\"pipe_linear_loopA.joblib\")\n",
    "joblib.dump(pipe_tree, ART_DIR/\"pipe_tree_loopA.joblib\")\n",
    "\n",
    "# Save per-row predictions for evidence\n",
    "pred_df = pd.DataFrame({\n",
    "    \"index\": X_test.index,\n",
    "    \"book_ref\": df.loc[X_test.index, \"book_ref\"].values,\n",
    "    \"y_true\": y_test.values,\n",
    "    \"y_pred_linear\": pred_lin,\n",
    "    \"y_pred_tree\": pred_tree\n",
    "})\n",
    "pred_df.to_csv(TAB_DIR/\"table_7_1_predictions_loopA.csv\", index=False)\n",
    "\n",
    "pred_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04430752-c853-4a61-8108-7ad7072d3513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:49.743041Z",
     "iopub.status.busy": "2026-01-11T09:24:49.742625Z",
     "iopub.status.idle": "2026-01-11T09:24:50.150874Z",
     "shell.execute_reply": "2026-01-11T09:24:50.150619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pick best by RMSE\n",
    "best_name = results.iloc[0][\"model\"]\n",
    "if best_name == \"Linear\":\n",
    "    best_pred = pred_lin\n",
    "    best_pipe = pipe_lin\n",
    "else:\n",
    "    best_pred = pred_tree\n",
    "    best_pipe = pipe_tree\n",
    "\n",
    "resid = y_test - best_pred\n",
    "\n",
    "# Residuals vs Fitted\n",
    "plt.figure()\n",
    "plt.scatter(best_pred, resid, s=12, alpha=0.7)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.title(f\"Residuals vs Fitted (Test) – {best_name} (Loop A)\")\n",
    "plt.xlabel(\"Fitted\"); plt.ylabel(\"Residuals\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"figure_7_1_residuals_vs_fitted_loopA.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Actual vs Predicted\n",
    "plt.figure()\n",
    "plt.scatter(y_test, best_pred, s=12, alpha=0.7)\n",
    "plt.title(f\"Actual vs Predicted (Test) – {best_name} (Loop A)\")\n",
    "plt.xlabel(\"Actual (log_total_amount)\"); plt.ylabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"figure_7_1_actual_vs_pred_loopA.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**Summary (Loop A):**\n",
    "- Trained two baselines (Linear vs Tree) on the fixed split; exported metrics & evidence tables.\n",
    "- Saved residual and predicted-vs-actual plots for Step 7.\n",
    "- Exported fitted pipelines for reproducibility.\n",
    "\n",
    "**Next:** open `05_modeling_loopB.ipynb` to create a stronger Loop B:\n",
    "- try a regularised linear model (Ridge) and a boosted tree (HistGradientBoosting/XGB),\n",
    "- tweak features (e.g., log-transform amounts, interaction terms, or better route encoding),\n",
    "- keep the same train/test split for apples-to-apples comparison (Step 8.5 iteration evidence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841968e0-a099-4a32-b06c-3bd9b3621804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:50.155725Z",
     "iopub.status.busy": "2026-01-11T09:24:50.155633Z",
     "iopub.status.idle": "2026-01-11T09:24:50.422400Z",
     "shell.execute_reply": "2026-01-11T09:24:50.410448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure log-target variables exist in this kernel\n",
    "if \"y_train_log\" not in globals():\n",
    "    y_train_log = y_train\n",
    "if \"y_test_log\" not in globals():\n",
    "    y_test_log = y_test\n",
    "\n",
    "# ---- 7.2 / Table XV: back-transform with log-normal correction to report RUB errors ----\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def rmse_score(y_true, y_pred):\n",
    "    try:\n",
    "        return float(mean_squared_error(y_true, y_pred, squared=False))\n",
    "    except TypeError:\n",
    "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def backtransform_with_lognormal_correction(pipeline, X_tr, y_tr_log, X_te):\n",
    "    yhat_tr_log = pipeline.predict(X_tr)\n",
    "    sigma2 = np.mean((y_tr_log - yhat_tr_log)**2)  # training residual variance\n",
    "    yhat_te_log = pipeline.predict(X_te)\n",
    "    return np.exp(yhat_te_log + 0.5 * sigma2)\n",
    "\n",
    "y_test_rub = np.exp(y_test_log)\n",
    "\n",
    "rub_rows = []\n",
    "for name, pipe in [(\"Linear (Loop A)\", pipe_lin), (\"Decision Tree (Loop A)\", pipe_tree)]:\n",
    "    yhat_rub = backtransform_with_lognormal_correction(pipe, X_train, y_train_log, X_test)\n",
    "    rmse = rmse_score(y_test_rub, yhat_rub)\n",
    "    mae  = mean_absolute_error(y_test_rub, yhat_rub)\n",
    "    med_ae = float(np.median(np.abs(y_test_rub - yhat_rub)))\n",
    "    rub_rows.append([name, rmse, mae, med_ae])\n",
    "\n",
    "rub_df = pd.DataFrame(rub_rows, columns=[\"Model\",\"RMSE_RUB\",\"MAE_RUB\",\"Median_AE_RUB\"])\n",
    "rub_df.to_csv(TAB_DIR/\"table_7_2_RUB_errors_loopA.csv\", index=False)\n",
    "rub_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009aa0c4-6304-44be-9b95-032eeff3544f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:50.446367Z",
     "iopub.status.busy": "2026-01-11T09:24:50.446048Z",
     "iopub.status.idle": "2026-01-11T09:24:50.499992Z",
     "shell.execute_reply": "2026-01-11T09:24:50.499353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure y_test_log exists for RUB back-transform\n",
    "if \"y_test_log\" not in globals():\n",
    "    if \"y_test\" in globals():\n",
    "        y_test_log = y_test\n",
    "    else:\n",
    "        raise RuntimeError(\"y_test_log is missing; run the split cell first.\")\n",
    "\n",
    "# ---- 7.3.3 Rule Cards from DecisionTreeRegressor (test set) ----\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.tree import _tree, DecisionTreeRegressor, export_text\n",
    "\n",
    "# 1) Define \"high revenue\" label from true RUB on TEST\n",
    "y_test_rub = np.exp(y_test_log)\n",
    "thr = np.quantile(y_test_rub, 0.75)\n",
    "y_test_high = (y_test_rub >= thr).astype(int)\n",
    "baseline = y_test_high.mean()\n",
    "\n",
    "# 2) Get preprocessor & tree from the pipeline robustly\n",
    "def _get_step(pipe, candidates):\n",
    "    for c in candidates:\n",
    "        if c in pipe.named_steps: \n",
    "            return pipe.named_steps[c]\n",
    "    raise KeyError(f\"None of {candidates} found in pipeline steps: {list(pipe.named_steps)}\")\n",
    "\n",
    "pre = _get_step(pipe_tree, (\"pre\", \"preprocessor\"))\n",
    "reg = _get_step(pipe_tree, (\"regressor\", \"model\", \"estimator\", \"tree\"))\n",
    "assert isinstance(reg, DecisionTreeRegressor), \"This block expects a DecisionTreeRegressor in pipe_tree\"\n",
    "\n",
    "Xt_test = pre.transform(X_test)\n",
    "leaf_id = reg.apply(Xt_test)\n",
    "\n",
    "# 3) Aggregate metrics by leaf\n",
    "df_leaf = pd.DataFrame({\"leaf\": leaf_id, \"high\": y_test_high})\n",
    "agg = df_leaf.groupby(\"leaf\").agg(N=(\"high\",\"size\"), hits=(\"high\",\"sum\")).reset_index()\n",
    "agg[\"coverage\"] = agg[\"N\"] / len(y_test_high)\n",
    "agg[\"confidence\"] = agg[\"hits\"] / agg[\"N\"]\n",
    "agg[\"lift\"] = agg[\"confidence\"] / baseline\n",
    "\n",
    "# 4) Extract human-readable rule text per leaf (shallow path for readability)\n",
    "feat_names = getattr(pre, \"get_feature_names_out\", lambda: np.array([]))()\n",
    "feat_names = feat_names.tolist() if isinstance(feat_names, np.ndarray) else list(feat_names)\n",
    "\n",
    "t = reg.tree_\n",
    "children_left, children_right, feature, threshold = t.children_left, t.children_right, t.feature, t.threshold\n",
    "\n",
    "def rule_for_leaf(leaf):\n",
    "    path, stack = [], [(0, [])]\n",
    "    while stack:\n",
    "        node, conds = stack.pop()\n",
    "        if node == leaf:\n",
    "            path = conds; break\n",
    "        if children_left[node] != _tree.TREE_LEAF:\n",
    "            f = feat_names[feature[node]] if feature[node] >= 0 and feature[node] < len(feat_names) else f\"f{feature[node]}\"\n",
    "            thr = threshold[node]\n",
    "            stack.append((children_left[node], conds + [f\"{f} ≤ {thr:.3f}\"]))\n",
    "            stack.append((children_right[node], conds + [f\"{f} > {thr:.3f}\"]))\n",
    "    return \"IF \" + \" AND \".join(path) if path else \"(root)\"\n",
    "\n",
    "agg[\"rule_text\"] = agg[\"leaf\"].apply(rule_for_leaf)\n",
    "\n",
    "# 5) Keep rules meeting the assignment criteria\n",
    "cards = (agg.query(\"coverage>=0.05 and confidence>=0.70 and lift>=1.20\")\n",
    "             .sort_values([\"confidence\",\"coverage\",\"N\"], ascending=False)\n",
    "             .head(5)\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "# 6) Add action & risk placeholders for the report table\n",
    "cards[\"action\"] = [\n",
    "    \"Maintain higher price ladders; bundle refundable/interline products.\",\n",
    "    \"Bundle lounge and baggage; emphasise flexibility.\",\n",
    "    \"Keep larger gauge / high frequency on trunk routes.\",\n",
    "    \"Weekend premium strategy; promo window 7–21 days.\",\n",
    "    \"Reduce surplus frequency; encourage connections.\"\n",
    "][:len(cards)]\n",
    "cards[\"risk\"] = [\n",
    "    \"Competitive response may dilute uplift.\",\n",
    "    \"Complexity can reduce conversion.\",\n",
    "    \"Price war risk.\",\n",
    "    \"Periodic spikes; monitor volatility.\",\n",
    "    \"Mis-match may reduce satisfaction.\"\n",
    "][:len(cards)]\n",
    "\n",
    "cards.to_csv(TAB_DIR/\"table_7_3_3_rule_cards_loopA.csv\", index=False)\n",
    "cards[[\"rule_text\",\"coverage\",\"confidence\",\"lift\",\"N\",\"action\",\"risk\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161567c-7f31-42b0-8d09-8713e5608987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:50.504697Z",
     "iopub.status.busy": "2026-01-11T09:24:50.504172Z",
     "iopub.status.idle": "2026-01-11T09:24:50.604500Z",
     "shell.execute_reply": "2026-01-11T09:24:50.604243Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==== BOOTSTRAP: ensure split & targets are available in this kernel ====\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROJ = PROJECT_ROOT\n",
    "OUT_DIR = OUTPUT_DIR\n",
    "TAB_DIR = OUT_DIR/\"tables\"\n",
    "\n",
    "# Where your formatted modelling artefacts live (adjust if needed)\n",
    "BK_DF = TAB_DIR/\"booking_model_df.parquet\"\n",
    "Y_TGT = OUT_DIR/\"booking_y_target.parquet\"\n",
    "SPLIT = TAB_DIR/\"split_ids.csv\"\n",
    "\n",
    "def _load_booking_df():\n",
    "    if BK_DF.exists():\n",
    "        df = pd.read_parquet(BK_DF)\n",
    "        # Expect either 'log_total_amount' precomputed or raw 'total_amount'\n",
    "        if \"log_total_amount\" not in df.columns:\n",
    "            assert \"total_amount\" in df.columns, \"booking_model_df needs total_amount\"\n",
    "            df[\"log_total_amount\"] = np.log(df[\"total_amount\"].astype(float))\n",
    "        return df\n",
    "    raise FileNotFoundError(f\"Missing {BK_DF}. Re-run 02/03 notebooks to generate it.\")\n",
    "\n",
    "def _load_target_series(df):\n",
    "    # Prefer separate y_target artefact if present; else derive from df\n",
    "    if Y_TGT.exists():\n",
    "        y = pd.read_parquet(Y_TGT).squeeze()\n",
    "        # Normalise name\n",
    "        y = y.rename(\"log_total_amount\") if y.name is None else y\n",
    "        return y\n",
    "    # Fall back: derive from df\n",
    "    return df[\"log_total_amount\"].rename(\"log_total_amount\")\n",
    "\n",
    "def _load_split_index(df):\n",
    "    assert SPLIT.exists(), f\"Missing {SPLIT}. Re-run the split notebook (03) to create it.\"\n",
    "    split = pd.read_csv(SPLIT)\n",
    "    # Align index by booking key if present; else align by positional index\n",
    "    key_col = \"book_ref\" if \"book_ref\" in split.columns and \"book_ref\" in df.columns else None\n",
    "    if key_col:\n",
    "        # Ensure uniqueness and join\n",
    "        assert df[key_col].is_unique, f\"{key_col} must be unique in booking_model_df\"\n",
    "        split = split.set_index(key_col).reindex(df.set_index(key_col).index)\n",
    "        is_train = split[\"is_train\"].fillna(0).astype(int).to_numpy().astype(bool)\n",
    "    else:\n",
    "        # Fallback by position\n",
    "        is_train = split[\"is_train\"].astype(int).to_numpy().astype(bool)\n",
    "        assert len(is_train) == len(df), \"split_ids length mismatch with booking_model_df\"\n",
    "    return is_train\n",
    "\n",
    "# Build X/y and train/test views\n",
    "booking_df = _load_booking_df()\n",
    "y_all_log = _load_target_series(booking_df)\n",
    "# Feature columns = all except target & obvious non-features; adjust to your project\n",
    "drop_cols = {\"total_amount\", \"log_total_amount\"} | {\"book_ref\"}  # keep ID out of X\n",
    "feature_cols = [c for c in booking_df.columns if c not in drop_cols]\n",
    "X_all = booking_df[feature_cols].copy()\n",
    "\n",
    "is_train = _load_split_index(booking_df)\n",
    "X_train, X_test = X_all[is_train], X_all[~is_train]\n",
    "y_train_log, y_test_log = y_all_log[is_train], y_all_log[~is_train]\n",
    "\n",
    "# Sanity checks\n",
    "assert len(X_train) == len(y_train_log) and len(X_test) == len(y_test_log)\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0462e90-8ec7-4797-b886-0e2671baeff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:50.605912Z",
     "iopub.status.busy": "2026-01-11T09:24:50.605768Z",
     "iopub.status.idle": "2026-01-11T09:24:50.608367Z",
     "shell.execute_reply": "2026-01-11T09:24:50.608057Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---- RUB metrics: be robust if y_test_log isn't in memory yet ----\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# y_test_log should exist from BOOTSTRAP; but support y_test_rub fallback just in case\n",
    "if \"y_test_log\" in globals():\n",
    "    y_test_rub = np.exp(y_test_log)\n",
    "elif \"y_test_rub\" in globals():\n",
    "    pass\n",
    "else:\n",
    "    raise RuntimeError(\"Neither y_test_log nor y_test_rub is defined. Run the BOOTSTRAP cell first.\")\n",
    "\n",
    "def backtransform_with_lognormal_correction(pipeline, X_tr, y_tr_log, X_te):\n",
    "    yhat_tr_log = pipeline.predict(X_tr)\n",
    "    sigma2 = np.mean((y_tr_log - yhat_tr_log)**2)  # training residual variance\n",
    "    yhat_te_log = pipeline.predict(X_te)\n",
    "    return np.exp(yhat_te_log + 0.5 * sigma2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d2b8d-7200-4fea-89fa-fbe35fa8f4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
