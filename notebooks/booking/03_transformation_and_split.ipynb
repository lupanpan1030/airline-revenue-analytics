{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "style"
    ]
   },
   "outputs": [],
   "source": [
    "from airline_revenue_analytics.viz.charts import apply_style, PLOT_COLORS\n",
    "apply_style()\n",
    "PASS_COLOR = \"#D9F2E6\"\n",
    "FAIL_COLOR = \"#FCE4E4\"\n",
    "NEG_BG_COLOR = FAIL_COLOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4036b2901824032a0acc32fff80c5fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:17:45.590670Z",
     "iopub.status.busy": "2026-01-11T09:17:45.590537Z",
     "iopub.status.idle": "2026-01-11T09:17:45.598720Z",
     "shell.execute_reply": "2026-01-11T09:17:45.598431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project paths (booking pipeline)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"pyproject.toml\").exists() and (p / \"src\" / \"airline_revenue_analytics\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "PROJECT_ROOT = REPO_ROOT\n",
    "SRC_ROOT = REPO_ROOT / \"src\"\n",
    "if str(SRC_ROOT) not in sys.path:\n",
    "    sys.path.append(str(SRC_ROOT))\n",
    "\n",
    "from airline_revenue_analytics.config import get_paths\n",
    "\n",
    "PATHS = get_paths(\"booking\")\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "RAW_DIR = PATHS.data_raw\n",
    "DB_PATH = PATHS.db_path\n",
    "OUTPUT_DIR = PATHS.outputs_root\n",
    "FIG_DIR = PATHS.figures\n",
    "TAB_DIR = PATHS.tables\n",
    "ART_DIR = PATHS.artifacts\n",
    "\n",
    "def _rel(p: Path) -> str:\n",
    "    try:\n",
    "        return str(Path(p).resolve().relative_to(REPO_ROOT))\n",
    "    except Exception:\n",
    "        return Path(p).name\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT.name)\n",
    "print(\"DB_PATH:\", _rel(DB_PATH))\n",
    "print(\"OUTPUT_DIR:\", _rel(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902436b-aa8a-45e7-893a-e8c3a30ff763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:17:45.600709Z",
     "iopub.status.busy": "2026-01-11T09:17:45.600588Z",
     "iopub.status.idle": "2026-01-11T09:17:45.602295Z",
     "shell.execute_reply": "2026-01-11T09:17:45.602046Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = \"\"\" \n",
    "# 03 - Transformation & Split (Steps 4.1\u20134.3)\n",
    "Goal: create a clean modeling matrix, run feature-scoring (MI/VIF), and\n",
    "prepare a reproducible train/test split with preprocessing artifacts.\n",
    "Outputs:\n",
    "- `outputs/artifacts/preprocessor.joblib`\n",
    "- `outputs/tables/table_4_3_1_train_test_counts.csv`\n",
    "- `outputs/tables/table_4_3_2_feature_names_after_ohe.csv`\n",
    "- `outputs/tables/table_4_1_1_mi.csv`  + `outputs/figures/figure_4_1_1_mi.png`\n",
    "- `outputs/tables/table_4_1_1_vif.csv` + `outputs/figures/figure_4_1_1_vif.png`\n",
    "- `outputs/tables/table_4_1_2_importances.csv`\n",
    "  + `outputs/figures/figure_4_1_2_impurity.png`\n",
    "  + `outputs/figures/figure_4_1_2_permutation.png`\n",
    "  + `outputs/figures/figure_4_1_2_random_forest_both.png`\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd401804-75e9-4459-aae7-108bf7d7edf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:17:45.603393Z",
     "iopub.status.busy": "2026-01-11T09:17:45.603320Z",
     "iopub.status.idle": "2026-01-11T09:17:47.646234Z",
     "shell.execute_reply": "2026-01-11T09:17:47.645939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports & paths\n",
    "import sys, pathlib, numpy as np, pandas as pd\n",
    "import matplotlib; import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure \"src\" is importable when running from notebooks/\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "\n",
    "from airline_revenue_analytics.features.segment import to_utc  # (not strictly needed here but available)\n",
    "# We'll build preprocessing here; modeling will be in the next notebook as well.\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import joblib\n",
    "\n",
    "# Dirs\n",
    "OUT_DIR = OUTPUT_DIR\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "TAB_DIR = OUT_DIR / \"tables\"\n",
    "ART_DIR = OUT_DIR / \"artifacts\"\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"FIG_DIR:\", _rel(FIG_DIR))\n",
    "print(\"TAB_DIR:\", _rel(TAB_DIR))\n",
    "print(\"ART_DIR:\", _rel(ART_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d4f04-7234-45d7-93f1-988d5c8eaeb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:17:47.647941Z",
     "iopub.status.busy": "2026-01-11T09:17:47.647727Z",
     "iopub.status.idle": "2026-01-11T09:17:47.814672Z",
     "shell.execute_reply": "2026-01-11T09:17:47.814423Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load booking-level dataset built in 02\n",
    "df = pd.read_parquet(OUTPUT_DIR / \"booking_model_df.parquet\")\n",
    "print(df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f58137-5f1a-402a-8ceb-400c3698b0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:17:47.815980Z",
     "iopub.status.busy": "2026-01-11T09:17:47.815881Z",
     "iopub.status.idle": "2026-01-11T09:17:48.085596Z",
     "shell.execute_reply": "2026-01-11T09:17:48.085220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define target and features (numeric + one categorical)\n",
    "target = \"log_total_amount\"\n",
    "\n",
    "# Numeric features (schedule-based, leakage-safe)\n",
    "num_cols = [\n",
    "    \"n_segments\", \"sum_sched_duration_min\", \"avg_sched_duration_min\",\n",
    "    \"max_sched_duration_min\", \"share_premium_cabin\", \"max_cabin_index\",\n",
    "    \"has_longhaul\", \"n_unique_routes\", \"avg_booking_lead_days\"\n",
    "]\n",
    "\n",
    "# Categorical feature (limit cardinality for stability)\n",
    "cat_raw = df[\"primary_route_code\"].astype(\"string\").fillna(\"Unknown\")\n",
    "top_k = 20\n",
    "top_routes = cat_raw.value_counts().index[:top_k].tolist()\n",
    "df[\"primary_route_code_top\"] = np.where(cat_raw.isin(top_routes), cat_raw, \"Other\")\n",
    "cat_cols = [\"primary_route_code_top\"]\n",
    "\n",
    "X_all = df[num_cols + cat_cols].copy()\n",
    "y_all = df[target].copy()\n",
    "\n",
    "# Preprocessor: impute+scale for numeric, impute+OHE for categorical\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"scaler\", StandardScaler())]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ]), cat_cols),\n",
    "])\n",
    "\n",
    "# Fit on the whole X to get feature names (this is *feature engineering*, not the split)\n",
    "X_all_trans = pre.fit_transform(X_all)\n",
    "\n",
    "# Compose final feature names\n",
    "ohe = pre.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "feat_names = num_cols + list(ohe.get_feature_names_out(cat_cols))\n",
    "\n",
    "# Persist artifacts & feature names\n",
    "joblib.dump(pre, ART_DIR/\"preprocessor.joblib\")\n",
    "pd.DataFrame({\"feature\": feat_names}).to_csv(TAB_DIR/\"table_4_3_2_feature_names_after_ohe.csv\", index=False)\n",
    "\n",
    "len(feat_names), feat_names[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74ce23-b47e-4c4c-98d6-d939464b9387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:17:48.086925Z",
     "iopub.status.busy": "2026-01-11T09:17:48.086803Z",
     "iopub.status.idle": "2026-01-11T09:17:49.742851Z",
     "shell.execute_reply": "2026-01-11T09:17:49.742567Z"
    }
   },
   "outputs": [],
   "source": [
    "# VIF on numeric features only (after median-impute + standardize)\n",
    "Xn = df[num_cols].copy()\n",
    "for c in num_cols:\n",
    "    Xn[c] = pd.to_numeric(Xn[c], errors=\"coerce\")\n",
    "Xn = Xn.fillna(Xn.median(numeric_only=True))\n",
    "scaler = StandardScaler()\n",
    "Xn_scaled = scaler.fit_transform(Xn)\n",
    "\n",
    "vifs = []\n",
    "for i, col in enumerate(num_cols):\n",
    "    vifs.append((col, float(variance_inflation_factor(Xn_scaled, i))))\n",
    "vif_df = pd.DataFrame(vifs, columns=[\"feature\",\"VIF\"]).sort_values(\"VIF\", ascending=False)\n",
    "vif_df.to_csv(TAB_DIR/\"table_4_1_1_vif.csv\", index=False)\n",
    "\n",
    "# Plot\n",
    "sub = vif_df.head(20).iloc[::-1]\n",
    "plt.figure()\n",
    "plt.barh(sub[\"feature\"], sub[\"VIF\"])\n",
    "plt.title(\"Table 4.1.1 \u2013 VIF for Numeric Predictors\")\n",
    "plt.xlabel(\"VIF\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"figure_4_1_1_vif.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "vif_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2a8fe-01e0-4609-b12a-ba65de6e9576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:17:49.744328Z",
     "iopub.status.busy": "2026-01-11T09:17:49.744243Z",
     "iopub.status.idle": "2026-01-11T09:17:49.905931Z",
     "shell.execute_reply": "2026-01-11T09:17:49.905576Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4.3 \u2013 Create reproducible train/test split and save evidence (FIXED to keep 'index' column)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# \u4f7f\u7528\u4e0e\u524d\u9762\u4e00\u81f4\u7684 X_all / y_all / df\uff08\u6765\u81ea\u672c\u7b14\u8bb0\u672c\u8f83\u65e9\u7684\u5355\u5143\uff09\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X_all, y_all, df.index, test_size=0.30, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "# \u4fdd\u5b58\u5207\u5206\u8ba1\u6570\uff08\u8bc1\u636e\uff09\n",
    "pd.DataFrame(\n",
    "    {\"split\": [\"train\", \"test\"], \"rows\": [len(X_train), len(X_test)]}\n",
    ").to_csv(TAB_DIR / \"table_4_3_1_train_test_counts.csv\", index=False)\n",
    "\n",
    "# \u4fdd\u5b58\u5207\u5206\u6210\u5458\uff08**\u4fdd\u7559 index \u5217**\uff0c\u5e76\u9644\u4e0a book_ref \u4fbf\u4e8e\u4eba\u8bfb\uff09\n",
    "split_ids = pd.DataFrame({\n",
    "    \"index\": list(idx_train) + list(idx_test),\n",
    "    \"split\": [\"train\"] * len(idx_train) + [\"test\"] * len(idx_test)\n",
    "})\n",
    "split_ids[\"book_ref\"] = df.loc[split_ids[\"index\"], \"book_ref\"].values\n",
    "\n",
    "split_ids.to_csv(TAB_DIR / \"table_4_3_1_split_ids.csv\", index=False)\n",
    "\n",
    "# \u8fd4\u56de\u5f62\u72b6\uff0c\u4fbf\u4e8e\u5feb\u901f\u68c0\u67e5\n",
    "(len(X_train), len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c8d97-607c-46a8-b744-a470940e7c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:17:49.907494Z",
     "iopub.status.busy": "2026-01-11T09:17:49.907366Z",
     "iopub.status.idle": "2026-01-11T09:24:41.028094Z",
     "shell.execute_reply": "2026-01-11T09:24:41.022994Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4.1.2 - Random Forest importance (impurity + permutation on transformed matrix)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2\n",
    ")\n",
    "pipe = Pipeline([(\"pre\", pre), (\"model\", rf)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Feature names after fitting on TRAIN (OHE categories come from train only)\n",
    "ohe_train = pipe.named_steps[\"pre\"].named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "feat_names_train = num_cols + list(ohe_train.get_feature_names_out(cat_cols))\n",
    "\n",
    "# (A) Impurity-based importance\n",
    "imp = pipe.named_steps[\"model\"].feature_importances_\n",
    "imp_df = (\n",
    "    pd.DataFrame({\"feature\": feat_names_train, \"impurity_importance\": imp})\n",
    "      .sort_values(\"impurity_importance\", ascending=False)\n",
    ")\n",
    "\n",
    "# (B) Permutation importance on the TRANSFORMED test matrix\n",
    "# Transform X_test with the train-fitted preprocessor INSIDE the pipeline\n",
    "X_test_trans = pipe.named_steps[\"pre\"].transform(X_test)\n",
    "\n",
    "perm = permutation_importance(\n",
    "    pipe.named_steps[\"model\"],   # evaluate the fitted model only\n",
    "    X_test_trans,                # use transformed features (same dimensionality as feat_names_train)\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=SEED,\n",
    "    n_jobs=1                     # set to 1 to avoid macOS multiprocessing warnings\n",
    ")\n",
    "\n",
    "perm_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feat_names_train,\n",
    "        \"permutation_importance\": perm.importances_mean\n",
    "    })\n",
    "    .sort_values(\"permutation_importance\", ascending=False)\n",
    ")\n",
    "\n",
    "# Merge and save\n",
    "imp_merge = pd.merge(imp_df, perm_df, on=\"feature\", how=\"outer\").fillna(0.0)\n",
    "imp_merge.sort_values(\"impurity_importance\", ascending=False).to_csv(\n",
    "    TAB_DIR/\"table_4_1_2_importances.csv\", index=False\n",
    ")\n",
    "\n",
    "# Plot: Impurity (top 15)\n",
    "sub = imp_df.head(15).iloc[::-1]\n",
    "plt.figure()\n",
    "plt.barh(sub[\"feature\"], sub[\"impurity_importance\"])\n",
    "plt.title(\"Figure 4.1.2 - Random Forest (Impurity) Importance\")\n",
    "plt.xlabel(\"importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"figure_4_1_2_impurity.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Plot: Permutation (top 15)\n",
    "subp = perm_df.head(15).iloc[::-1]\n",
    "plt.figure()\n",
    "plt.barh(subp[\"feature\"], subp[\"permutation_importance\"])\n",
    "plt.title(\"Figure 4.1.2 - Permutation Importance (Test Set)\")\n",
    "plt.xlabel(\"importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"figure_4_1_2_permutation.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Combined figure (optional)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(1,2,1); plt.barh(sub[\"feature\"], sub[\"impurity_importance\"]); plt.title(\"Impurity\")\n",
    "plt.subplot(1,2,2); plt.barh(subp[\"feature\"], subp[\"permutation_importance\"]); plt.title(\"Permutation\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/\"figure_4_1_2_random_forest_both.png\", dpi=150); plt.show()\n",
    "\n",
    "imp_merge.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccaf1be-3d8d-48fc-8b50-a1bdc9273bd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:41.048547Z",
     "iopub.status.busy": "2026-01-11T09:24:41.048320Z",
     "iopub.status.idle": "2026-01-11T09:24:41.057111Z",
     "shell.execute_reply": "2026-01-11T09:24:41.056739Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = \"\"\" \n",
    "**Summary (Step 4):**\n",
    "- Preprocessor (impute/scale/OHE) has been fitted and saved to `outputs/artifacts/preprocessor.joblib`.\n",
    "- MI, VIF and RF-based importances highlight key drivers for modeling.\n",
    "- Train/test split (70/30, random_state=42) is saved for evidence.\n",
    "\n",
    "**Next:** open `04_modeling_loopA.ipynb` (Steps 5\u20137, Loop A \u2013 baseline models: Linear vs Tree),\n",
    "then `05_modeling_loopB.ipynb` (Loop B \u2013 stronger models & feature tweaks), and finally\n",
    "`06_interpretation_compare.ipynb` (Step 8).\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d58666-ce1b-4c7e-ae48-8c883b85b30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T09:24:41.062643Z",
     "iopub.status.busy": "2026-01-11T09:24:41.062328Z",
     "iopub.status.idle": "2026-01-11T09:24:41.353886Z",
     "shell.execute_reply": "2026-01-11T09:24:41.353501Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---- 4.3 (final): stratified 70/30 split on log_total_amount deciles, random_state=42 ----\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "assert \"total_amount\" in df.columns, \"booking_model_df must contain total_amount\"\n",
    "y_log = df[\"log_total_amount\"] if \"log_total_amount\" in df.columns else np.log(df[\"total_amount\"])\n",
    "\n",
    "# Decile bins for stratification (handles ties safely)\n",
    "strata = pd.qcut(y_log, q=10, labels=False, duplicates=\"drop\")\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in (\"total_amount\", \"log_total_amount\")]\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
    "    X, y_log, test_size=0.30, random_state=42, shuffle=True, stratify=strata\n",
    ")\n",
    "\n",
    "# Persist split membership for reproducibility\n",
    "split_ids = pd.DataFrame({\n",
    "    \"book_ref\": df[\"book_ref\"] if \"book_ref\" in df.columns else X.index,\n",
    "    \"is_train\": X.index.isin(X_train.index).astype(int)\n",
    "})\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "split_ids.to_csv(TAB_DIR/\"split_ids.csv\", index=False)\n",
    "\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae78a38-f3f3-4373-8573-2c546667a411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
